use cpal::SampleRate;
use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use std::sync::{Arc, Mutex, mpsc};
use std::thread;
use std::time::{Duration, Instant};
use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Emitter, State, Manager};
use std::sync::Mutex as StdMutex;
// use webrtc_vad::Vad; // æš‚æ—¶æœªä½¿ç”¨
use std::ffi::{CStr, CString};
use std::os::raw::c_char;

// å¯¼å…¥whisperç›¸å…³å‡½æ•°
use crate::{
    whisper_full, whisper_full_default_params, whisper_full_get_segment_text, 
    whisper_full_n_segments, whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH,
    WhisperContextState, post_process_text
};
use crate::realtime_speaker_diarization::RealtimeSpeakerDiarization;
use crate::audio_devices;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RealtimeConfig {
    pub language: String,
    pub mode: String, // "streaming", "buffered", "hybrid"
    pub speaker_diarization: bool,
    pub noise_reduction: bool,
    pub auto_save: bool,
    pub save_interval: u32, // minutes
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AudioLevelUpdate {
    pub level: f32,
    pub timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecognitionResult {
    pub text: String,
    pub confidence: f32,
    pub is_temporary: bool,
    pub speaker: Option<String>,
    pub timestamp: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RecordingStats {
    pub duration: u64, // seconds
    pub segments_count: u32,
    pub speaker_count: u32,
    pub average_confidence: f32,
}

// éŸ³é¢‘å¤„ç†çŠ¶æ€
struct AudioProcessor {
    audio_buffer: Vec<f32>,
    continuous_buffer: Vec<f32>, // è¿ç»­çš„éŸ³é¢‘ç¼“å†²åŒº
    last_recognition_time: Instant,
    recognition_interval: Duration, // è¯†åˆ«é—´éš”
    min_audio_length: usize, // æœ€å°éŸ³é¢‘é•¿åº¦(æ ·æœ¬æ•°)
    max_audio_length: usize, // æœ€å¤§éŸ³é¢‘é•¿åº¦(æ ·æœ¬æ•°)
    activity_threshold: f32, // æ´»åŠ¨æ£€æµ‹é˜ˆå€¼
    speaker_diarization: RealtimeSpeakerDiarization,
}

impl AudioProcessor {
    fn new() -> Result<Self, String> {
        Ok(Self {
            audio_buffer: Vec::new(),
            continuous_buffer: Vec::new(),
            last_recognition_time: Instant::now(),
            recognition_interval: Duration::from_millis(2000), // æ¯2ç§’è¯†åˆ«ä¸€æ¬¡
            min_audio_length: 16000, // 1ç§’çš„éŸ³é¢‘ (16kHz)
            max_audio_length: 16000 * 10, // 10ç§’çš„éŸ³é¢‘
            activity_threshold: 0.005, // æ´»åŠ¨æ£€æµ‹é˜ˆå€¼
            speaker_diarization: RealtimeSpeakerDiarization::new(),
        })
    }
    
    fn process_audio_chunk(&mut self, audio: &[f32]) -> Option<(Vec<f32>, Option<String>)> {
        // æ·»åŠ éŸ³é¢‘åˆ°è¿ç»­ç¼“å†²åŒº
        self.continuous_buffer.extend_from_slice(audio);
        
        // è®¡ç®—å½“å‰éŸ³é¢‘å—çš„å¹³å‡éŸ³é‡
        let current_level = audio.iter().map(|&x| x.abs()).sum::<f32>() / audio.len() as f32;
        
        // æ£€æŸ¥æ˜¯å¦æœ‰æ´»åŠ¨éŸ³é¢‘
        let has_activity = current_level > self.activity_threshold;
        
        if has_activity {
            println!("ğŸµ Audio activity detected: level={:.6}", current_level);
        }
        
        // é™åˆ¶ç¼“å†²åŒºå¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
        if self.continuous_buffer.len() > self.max_audio_length {
            let excess = self.continuous_buffer.len() - self.max_audio_length;
            self.continuous_buffer.drain(..excess);
            println!("ğŸ”„ Buffer trimmed, removed {} samples", excess);
        }
        
        // å®šæœŸæˆ–æ£€æµ‹åˆ°æ´»åŠ¨æ—¶è¿›è¡Œè¯†åˆ«
        let should_recognize = {
            let time_since_last = self.last_recognition_time.elapsed();
            let has_enough_audio = self.continuous_buffer.len() >= self.min_audio_length;
            
            // å¦‚æœæœ‰æ´»åŠ¨ä¸”è·ç¦»ä¸Šæ¬¡è¯†åˆ«è¶…è¿‡é—´éš”æ—¶é—´ï¼Œæˆ–è€…ç¼“å†²åŒºå¿«æ»¡äº†
            (has_activity && time_since_last >= self.recognition_interval && has_enough_audio) ||
            (self.continuous_buffer.len() >= self.max_audio_length * 8 / 10) // 80%æ»¡æ—¶å¼ºåˆ¶è¯†åˆ«
        };
        
        if should_recognize {
            println!("ğŸ” Triggering recognition: buffer_size={} samples ({:.1}s), activity={}, time_elapsed={:.1}s", 
                self.continuous_buffer.len(), 
                self.continuous_buffer.len() as f32 / 16000.0,
                has_activity, 
                self.last_recognition_time.elapsed().as_secs_f32());
            
            // å–æœ€è¿‘çš„éŸ³é¢‘æ•°æ®è¿›è¡Œè¯†åˆ«
            let recognition_length = self.continuous_buffer.len().min(self.max_audio_length);
            let start_pos = self.continuous_buffer.len() - recognition_length;
            let audio_for_recognition = self.continuous_buffer[start_pos..].to_vec();
            
            // æ›´æ–°æœ€åè¯†åˆ«æ—¶é—´
            self.last_recognition_time = Instant::now();
            
            // è¿›è¡Œè¯´è¯äººè¯†åˆ«
            let speaker = if has_activity {
                self.speaker_diarization.identify_speaker(&audio_for_recognition)
            } else {
                None
            };
            
            return Some((audio_for_recognition, speaker));
        }
        
        None
    }
}

// æ§åˆ¶å‘½ä»¤æšä¸¾
#[derive(Debug)]
enum AudioCommand {
    Start,
    Pause,
    Resume,
    Stop,
}

// çº¿ç¨‹å®‰å…¨çš„éŸ³é¢‘ç®¡ç†å™¨
pub struct RealtimeAudioCapture {
    command_tx: Option<mpsc::Sender<AudioCommand>>,
    is_recording: Arc<Mutex<bool>>,
    is_paused: Arc<Mutex<bool>>,
    start_time: Option<Instant>,
    recognition_config: RealtimeConfig,
    app_handle: AppHandle,
    audio_data: Arc<Mutex<Vec<f32>>>, // ä¿å­˜å½•éŸ³æ•°æ®
    recording_id: String, // å½•éŸ³ID
}

impl RealtimeAudioCapture {
    pub fn new(app_handle: AppHandle, config: RealtimeConfig) -> Result<Self, Box<dyn std::error::Error>> {
        println!("ğŸ”§ åˆ›å»º RealtimeAudioCapture å®ä¾‹...");
        
        // æ£€æŸ¥éŸ³é¢‘è®¾å¤‡å¯ç”¨æ€§
        let host = cpal::default_host();
        println!("éŸ³é¢‘ä¸»æœº: {:?}", host.id());
        
        // æ£€æŸ¥é»˜è®¤è¾“å…¥è®¾å¤‡
        match host.default_input_device() {
            Some(device) => {
                if let Ok(name) = device.name() {
                    println!("âœ… æ‰¾åˆ°é»˜è®¤è¾“å…¥è®¾å¤‡: {}", name);
                } else {
                    println!("âš ï¸ é»˜è®¤è¾“å…¥è®¾å¤‡æ— æ³•è·å–åç§°");
                }
            }
            None => {
                return Err("âŒ æœªæ‰¾åˆ°ä»»ä½•éŸ³é¢‘è¾“å…¥è®¾å¤‡ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£è¿æ¥".into());
            }
        }
        
        println!("âœ… RealtimeAudioCapture å®ä¾‹åˆ›å»ºæˆåŠŸ");
        
        // ç”Ÿæˆå”¯ä¸€çš„å½•éŸ³ID
        let recording_id = format!("recording_{}", std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis());
        
        Ok(Self {
            command_tx: None,
            is_recording: Arc::new(Mutex::new(false)),
            is_paused: Arc::new(Mutex::new(false)),
            start_time: None,
            recognition_config: config,
            app_handle,
            audio_data: Arc::new(Mutex::new(Vec::new())),
            recording_id,
        })
    }

    pub fn start_recording(&mut self, whisper_state: Arc<WhisperContextState>) -> Result<(), Box<dyn std::error::Error>> {
        *self.is_recording.lock().unwrap() = true;
        *self.is_paused.lock().unwrap() = false;
        self.start_time = Some(Instant::now());

        let (command_tx, command_rx) = mpsc::channel::<AudioCommand>();
        self.command_tx = Some(command_tx);

        let is_recording = self.is_recording.clone();
        let is_paused = self.is_paused.clone();
        let app_handle = self.app_handle.clone();
        let config = self.recognition_config.clone();
        let audio_data = self.audio_data.clone();

        // å¯åŠ¨ç‹¬ç«‹çš„éŸ³é¢‘å¤„ç†çº¿ç¨‹
        thread::spawn(move || {
            Self::audio_thread(
                command_rx,
                is_recording,
                is_paused,
                app_handle,
                config,
                whisper_state,
                audio_data,
            );
        });

        Ok(())
    }

    pub fn pause_recording(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        *self.is_paused.lock().unwrap() = true;
        if let Some(ref tx) = self.command_tx {
            let _ = tx.send(AudioCommand::Pause);
        }
        println!("Recording paused");
        Ok(())
    }

    pub fn resume_recording(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        *self.is_paused.lock().unwrap() = false;
        if let Some(ref tx) = self.command_tx {
            let _ = tx.send(AudioCommand::Resume);
        }
        println!("Recording resumed");
        Ok(())
    }

    pub fn stop_recording(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("Stopping recording...");
        
        // ç«‹å³è®¾ç½®åœæ­¢æ ‡å¿—
        *self.is_recording.lock().unwrap() = false;
        *self.is_paused.lock().unwrap() = false;
        
        // å‘é€åœæ­¢å‘½ä»¤
        if let Some(ref tx) = self.command_tx {
            let _ = tx.send(AudioCommand::Stop);
            println!("Stop command sent to audio thread");
        }
        
        // æ¸…ç†å‘½ä»¤é€šé“
        self.command_tx = None;
        
        // ä¿å­˜å½•éŸ³æ–‡ä»¶ - ç§»é™¤ await è°ƒç”¨
        if let Err(e) = self.save_audio_file() {
            eprintln!("ä¿å­˜å½•éŸ³æ–‡ä»¶å¤±è´¥: {}", e);
        }
        
        // å‘é€åœæ­¢å®Œæˆäº‹ä»¶
        let _ = self.app_handle.emit("recording_stopped", ());
        let _ = self.app_handle.emit("recording_completed", ());
        
        println!("Recording stopped successfully");
        Ok(())
    }

    fn save_audio_file(&self) -> Result<(), Box<dyn std::error::Error>> {
        use std::fs::File;
        use std::io::BufWriter;
        
        // è·å–éŸ³é¢‘æ•°æ®
        let audio_data = self.audio_data.lock().unwrap().clone();
        if audio_data.is_empty() {
            println!("æ²¡æœ‰éŸ³é¢‘æ•°æ®å¯ä¿å­˜");
            return Ok(());
        }
        
        // è·å–åº”ç”¨æ•°æ®ç›®å½•
        let app_data_dir = self.app_handle.path().app_data_dir()?;
        let recordings_dir = app_data_dir.join("recordings");
        
        // åˆ›å»ºå½•éŸ³ç›®å½•
        std::fs::create_dir_all(&recordings_dir)?;
        
        // ç”Ÿæˆæ–‡ä»¶å
        let filename = format!("{}.wav", self.recording_id);
        let file_path = recordings_dir.join(&filename);
        
        // åˆ›å»ºWAVæ–‡ä»¶
        let spec = hound::WavSpec {
            channels: 1,
            sample_rate: 16000,
            bits_per_sample: 32,
            sample_format: hound::SampleFormat::Float,
        };
        
        let file = File::create(&file_path)?;
        let mut writer = hound::WavWriter::new(BufWriter::new(file), spec)?;
        
        // å†™å…¥éŸ³é¢‘æ•°æ®
        for &sample in &audio_data {
            writer.write_sample(sample)?;
        }
        
        writer.finalize()?;
        
        println!("å½•éŸ³æ–‡ä»¶å·²ä¿å­˜: {:?}", file_path);
        
        // å‘é€å½•éŸ³æ–‡ä»¶è·¯å¾„äº‹ä»¶ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¾¿äºå‰ç«¯è®¿é—®
        let relative_path = format!("recordings/{}", filename);
        let _ = self.app_handle.emit("recording_file_saved", relative_path);
        
        Ok(())
    }

    // è·å–å½•éŸ³æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    pub fn get_audio_file_path(&self) -> Result<String, Box<dyn std::error::Error>> {
        let app_data_dir = self.app_handle.path().app_data_dir()?;
        let recordings_dir = app_data_dir.join("recordings");
        let filename = format!("{}.wav", self.recording_id);
        let file_path = recordings_dir.join(&filename);
        Ok(file_path.to_string_lossy().to_string())
    }

    fn audio_thread(
        command_rx: mpsc::Receiver<AudioCommand>,
        is_recording: Arc<Mutex<bool>>,
        is_paused: Arc<Mutex<bool>>,
        app_handle: AppHandle,
        config: RealtimeConfig,
        whisper_state: Arc<WhisperContextState>,
        audio_data: Arc<Mutex<Vec<f32>>>,
    ) {
        println!("Starting audio thread");
        
        // è·å–éŸ³é¢‘ä¸»æœº
        let host = cpal::default_host();
        println!("Audio host: {:?}", host.id());
        
        // åˆ—å‡ºæ‰€æœ‰è¾“å…¥è®¾å¤‡
        match host.input_devices() {
            Ok(devices) => {
                println!("Available input devices:");
                for (i, device) in devices.enumerate() {
                    if let Ok(name) = device.name() {
                        println!("  {}: {}", i, name);
                    }
                }
            }
            Err(e) => {
                eprintln!("Failed to enumerate input devices: {}", e);
            }
        }
        
        // è·å–é€‰å®šçš„è¾“å…¥è®¾å¤‡ï¼ˆå¦‚æœæ²¡æœ‰é€‰æ‹©åˆ™ä½¿ç”¨é»˜è®¤è®¾å¤‡ï¼‰
        let device = match get_selected_input_device_sync(&host) {
            Ok(device) => {
                if let Ok(name) = device.name() {
                    println!("Using selected input device: {}", name);
                } else {
                    println!("Using selected input device (name unavailable)");
                }
                device
            }
            Err(e) => {
                eprintln!("Failed to get selected device, falling back to default: {}", e);
                match host.default_input_device() {
                    Some(device) => {
                        if let Ok(name) = device.name() {
                            println!("Using default input device: {}", name);
                        }
                        device
                    }
                    None => {
                        eprintln!("No input device available");
                        let _ = app_handle.emit("recording_error", "No input device available");
                        return;
                    }
                }
            }
        };
        
        // æ£€æŸ¥è®¾å¤‡æ”¯æŒçš„é…ç½®
        let supported_configs = match device.supported_input_configs() {
            Ok(configs) => configs.collect::<Vec<_>>(),
            Err(e) => {
                eprintln!("Failed to get supported configs: {}", e);
                let _ = app_handle.emit("recording_error", format!("Failed to get supported configs: {}", e));
                return;
            }
        };
        
        println!("Supported input configurations:");
        for (i, config) in supported_configs.iter().enumerate() {
            println!("  {}: {:?}", i, config);
        }
        
        // æŸ¥æ‰¾æœ€ä½³é…ç½® - ä½¿ç”¨è®¾å¤‡æ”¯æŒçš„æœ€ä½é‡‡æ ·ç‡
        let (stream_config, sample_format, need_resample, original_sample_rate) = if let Some(config) = supported_configs.iter()
            .find(|c| c.channels() >= 1 && c.min_sample_rate() <= SampleRate(16000) && c.max_sample_rate() >= SampleRate(16000)) {
            // ä½¿ç”¨æ”¯æŒçš„é…ç½®
            let config_range = config.clone();
            let stream_config = config_range.with_sample_rate(SampleRate(16000)).config();
            (stream_config, config_range.sample_format(), false, 16000)
        } else if let Some(config) = supported_configs.first() {
            // ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é…ç½®ï¼Œç¨åé‡é‡‡æ ·
            let original_rate = config.min_sample_rate().0;
            println!("Using config with resampling from {}Hz to 16kHz", original_rate);
            let config_range = config.clone();
            let stream_config = config_range.with_sample_rate(config.min_sample_rate()).config();
            (stream_config, config_range.sample_format(), true, original_rate)
        } else {
            eprintln!("No supported input configurations found");
            let _ = app_handle.emit("recording_error", "No supported input configurations found");
            return;
        };
        
        println!("Selected config: channels={}, sample_rate={:?}, sample_format={:?}, need_resample={}", 
                stream_config.channels, stream_config.sample_rate, sample_format, need_resample);
        
        let (audio_tx, audio_rx) = mpsc::channel::<Vec<f32>>();
        let (level_tx, level_rx) = mpsc::channel::<f32>();
        
        let is_recording_stream = is_recording.clone();
        let is_paused_stream = is_paused.clone();
        let audio_data_storage = audio_data.clone();
        
        // åˆ›å»ºéŸ³é¢‘æµå›è°ƒ
        let stream = match sample_format {
            cpal::SampleFormat::I8 => {
                device.build_input_stream(
                    &stream_config,
                    move |data: &[i8], _: &cpal::InputCallbackInfo| {
                        let recording = *is_recording_stream.lock().unwrap();
                        let paused = *is_paused_stream.lock().unwrap();
                        if recording && !paused {
                            let float_data: Vec<f32> = data.iter().map(|&x| x as f32 / 128.0).collect();
                            let level = float_data.iter().map(|&sample| sample.abs()).sum::<f32>() / float_data.len() as f32;
                            let _ = level_tx.send(level);
                            
                            // ä¿å­˜åŸå§‹éŸ³é¢‘æ•°æ®
                            if let Ok(mut storage) = audio_data_storage.lock() {
                                storage.extend_from_slice(&float_data);
                            }
                            
                            let _ = audio_tx.send(float_data);
                        }
                    },
                    |err| eprintln!("Audio stream error: {}", err),
                    None,
                )
            }
            cpal::SampleFormat::I16 => {
                device.build_input_stream(
                    &stream_config,
                    move |data: &[i16], _: &cpal::InputCallbackInfo| {
                        let recording = *is_recording_stream.lock().unwrap();
                        let paused = *is_paused_stream.lock().unwrap();
                        if recording && !paused {
                            let float_data: Vec<f32> = data.iter().map(|&x| x as f32 / 32768.0).collect();
                            let level = float_data.iter().map(|&sample| sample.abs()).sum::<f32>() / float_data.len() as f32;
                            let _ = level_tx.send(level);
                            
                            // ä¿å­˜åŸå§‹éŸ³é¢‘æ•°æ®
                            if let Ok(mut storage) = audio_data_storage.lock() {
                                storage.extend_from_slice(&float_data);
                            }
                            
                            let _ = audio_tx.send(float_data);
                        }
                    },
                    |err| eprintln!("Audio stream error: {}", err),
                    None,
                )
            }
            cpal::SampleFormat::I32 => {
                device.build_input_stream(
                    &stream_config,
                    move |data: &[i32], _: &cpal::InputCallbackInfo| {
                        let recording = *is_recording_stream.lock().unwrap();
                        let paused = *is_paused_stream.lock().unwrap();
                        if recording && !paused {
                            let float_data: Vec<f32> = data.iter().map(|&x| x as f32 / 2147483648.0).collect();
                            let level = float_data.iter().map(|&sample| sample.abs()).sum::<f32>() / float_data.len() as f32;
                            let _ = level_tx.send(level);
                            
                            // ä¿å­˜åŸå§‹éŸ³é¢‘æ•°æ®
                            if let Ok(mut storage) = audio_data_storage.lock() {
                                storage.extend_from_slice(&float_data);
                            }
                            
                            let _ = audio_tx.send(float_data);
                        }
                    },
                    |err| eprintln!("Audio stream error: {}", err),
                    None,
                )
            }
            cpal::SampleFormat::F32 => {
                device.build_input_stream(
                    &stream_config,
                    move |data: &[f32], _: &cpal::InputCallbackInfo| {
                        let recording = *is_recording_stream.lock().unwrap();
                        let paused = *is_paused_stream.lock().unwrap();
                        if recording && !paused {
                            let level = data.iter().map(|&sample| sample.abs()).sum::<f32>() / data.len() as f32;
                            let _ = level_tx.send(level);
                            // é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
                            let float_data = if need_resample {
                                let ratio = original_sample_rate as f64 / 16000.0;
                                let output_len = (data.len() as f64 / ratio) as usize;
                                let mut resampled = Vec::with_capacity(output_len);
                                
                                for i in 0..output_len {
                                    let src_index = (i as f64 * ratio) as usize;
                                    if src_index < data.len() {
                                        resampled.push(data[src_index]);
                                    } else {
                                        resampled.push(0.0);
                                    }
                                }
                                println!("Resampled audio: {} -> {} samples", data.len(), resampled.len());
                                resampled
                            } else {
                                data.to_vec()
                            };
                            
                            // ä¿å­˜åŸå§‹éŸ³é¢‘æ•°æ®
                            if let Ok(mut storage) = audio_data_storage.lock() {
                                storage.extend_from_slice(&float_data);
                            }
                            
                            // å‘é€éŸ³é¢‘æ•°æ®åˆ°å¤„ç†çº¿ç¨‹
                            if let Err(_) = audio_tx.send(float_data.clone()) {
                                println!("Failed to send audio data to processing thread");
                            } else {
                                println!("Sent {} audio samples to processing thread", float_data.len());
                            }
                        }
                    },
                    |err| eprintln!("Audio stream error: {}", err),
                    None,
                )
            }
            _ => {
                eprintln!("Unsupported sample format: {:?}", sample_format);
                let _ = app_handle.emit("recording_error", format!("Unsupported sample format: {:?}", sample_format));
                return;
            }
        };
        
        let stream = match stream {
            Ok(stream) => {
                println!("Audio stream created successfully");
                stream
            }
            Err(e) => {
                eprintln!("Failed to build audio stream: {}", e);
                let _ = app_handle.emit("recording_error", format!("Failed to build audio stream: {}", e));
                return;
            }
        };
        
        if let Err(e) = stream.play() {
            eprintln!("Failed to start audio stream: {}", e);
            let _ = app_handle.emit("recording_error", format!("Failed to start audio stream: {}", e));
            return;
        } else {
            println!("Audio stream started successfully");
            let _ = app_handle.emit("recording_started", ());
        }
        
        // å¯åŠ¨éŸ³é¢‘çº§åˆ«ç›‘æ§çº¿ç¨‹
        let app_handle_level = app_handle.clone();
        thread::spawn(move || {
            while let Ok(level) = level_rx.recv() {
                let level_update = AudioLevelUpdate {
                    level: level * 5.0,
                    timestamp: std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_millis() as u64,
                };
                let _ = app_handle_level.emit("audio_level_update", level_update);
            }
        });
        
        // å¯åŠ¨éŸ³é¢‘å¤„ç†å’Œè¯†åˆ«çº¿ç¨‹
        let app_handle_processing = app_handle.clone();
        let is_recording_processing = is_recording.clone();
        thread::spawn(move || {
            Self::audio_processing_thread(
                audio_rx,
                app_handle_processing,
                config,
                is_recording_processing,
                whisper_state,
            );
        });
        
        // å‘½ä»¤å¤„ç†å¾ªç¯
        while let Ok(command) = command_rx.recv() {
            match command {
                AudioCommand::Start => {
                    println!("Audio thread: Start command received");
                }
                AudioCommand::Pause => {
                    println!("Audio thread: Pause command received");
                }
                AudioCommand::Resume => {
                    println!("Audio thread: Resume command received");
                }
                AudioCommand::Stop => {
                    println!("Audio thread: Stop command received");
                    break;
                }
            }
        }
        
        drop(stream);
        println!("Audio thread ended");
    }
    
    fn audio_processing_thread(
        audio_rx: mpsc::Receiver<Vec<f32>>,
        app_handle: AppHandle,
        config: RealtimeConfig,
        is_recording: Arc<Mutex<bool>>,
        whisper_state: Arc<WhisperContextState>,
    ) {
        println!("ğŸš€ Audio processing thread starting...");
        
        let mut processor = match AudioProcessor::new() {
            Ok(p) => {
                println!("âœ… Audio processor created successfully");
                p
            },
            Err(e) => {
                eprintln!("âŒ Failed to create audio processor: {}", e);
                return;
            }
        };

        let mut segment_id = 0u32;
        let mut total_segments = 0u32;
        let mut confidence_sum = 0.0f32;

        println!("ğŸµ Audio processing thread ready, waiting for audio data...");

        // ä½¿ç”¨æ›´å®‰å…¨çš„å¾ªç¯æ£€æŸ¥
        loop {
            // æ£€æŸ¥å½•éŸ³çŠ¶æ€
            let recording = match is_recording.lock() {
                Ok(guard) => *guard,
                Err(e) => {
                    eprintln!("âŒ Failed to lock recording state: {}", e);
                    break;
                }
            };
            
            if !recording {
                println!("ğŸ›‘ Recording stopped, exiting audio processing thread");
                break;
            }
            match audio_rx.recv_timeout(Duration::from_millis(100)) {
                Ok(audio_chunk) => {
                    println!("ğŸ“Š Processing audio chunk with {} samples", audio_chunk.len());
                    
                    // å®‰å…¨åœ°å¤„ç†éŸ³é¢‘å—
                    match std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                        processor.process_audio_chunk(&audio_chunk)
                    })) {
                        Ok(result) => {
                            if let Some((speech_audio, speaker)) = result {
                                println!("ğŸ¯ Processing speech segment of {} samples", speech_audio.len());
                                
                                // å®‰å…¨åœ°ä½¿ç”¨Whisperè¿›è¡Œè¯†åˆ«
                                match std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                                    Self::recognize_speech_segment_optimized(&speech_audio, &config, &whisper_state)
                                })) {
                                    Ok(recognition_result) => match recognition_result {
                                        Ok(text) => {
                                            if !text.trim().is_empty() {
                                                let confidence = 0.85 + (speech_audio.len() as f32 / 32000.0 * 0.1).min(0.15);
                                                confidence_sum += confidence;
                                                total_segments += 1;

                                                let result = RecognitionResult {
                                                    text: text.clone(),
                                                    confidence,
                                                    is_temporary: false,
                                                    speaker: if config.speaker_diarization {
                                                        speaker.clone()
                                                    } else {
                                                        None
                                                    },
                                                    timestamp: std::time::SystemTime::now()
                                                        .duration_since(std::time::UNIX_EPOCH)
                                                        .unwrap()
                                                        .as_millis() as u64,
                                                };

                                                println!("âœ… Recognition result: {}", text);
                                                let _ = app_handle.emit("recognition_result", result);

                                                segment_id += 1;

                                                // å‘é€ç»Ÿè®¡ä¿¡æ¯
                                                let stats = RecordingStats {
                                                    duration: segment_id as u64 * 2, // ä¼°ç®—æ—¶é•¿
                                                    segments_count: total_segments,
                                                    speaker_count: if config.speaker_diarization { 2 } else { 1 },
                                                    average_confidence: if total_segments > 0 { confidence_sum / total_segments as f32 } else { 0.0 },
                                                };
                                                let _ = app_handle.emit("recording_stats", stats);
                                            }
                                        }
                                        Err(e) => {
                                            eprintln!("âŒ Recognition failed: {}", e);
                                        }
                                    },
                                    Err(_) => {
                                        eprintln!("âš ï¸ Recognition panicked, skipping this segment");
                                    }
                                }
                            }
                        },
                        Err(_) => {
                            eprintln!("âš ï¸ Audio processing panicked, skipping this chunk");
                        }
                    }
                },
                Err(mpsc::RecvTimeoutError::Timeout) => {
                    // å®šæœŸå‘é€å¿ƒè·³ç»Ÿè®¡
                    if total_segments > 0 {
                        let stats = RecordingStats {
                            duration: segment_id as u64 * 2,
                            segments_count: total_segments,
                            speaker_count: if config.speaker_diarization { 2 } else { 1 },
                            average_confidence: confidence_sum / total_segments as f32,
                        };
                        let _ = app_handle.emit("recording_stats", stats);
                    }
                },
                Err(mpsc::RecvTimeoutError::Disconnected) => {
                    println!("Audio processing thread: channel disconnected");
                    break;
                }
            }
        }

        println!("Audio processing thread ended");
    }

    fn recognize_speech_segment_optimized(
        audio: &[f32],
        config: &RealtimeConfig,
        whisper_state: &WhisperContextState,
    ) -> Result<String, String> {
        println!("ğŸ¯ Starting Whisper recognition for {} samples ({:.2}s)", 
            audio.len(), audio.len() as f32 / 16000.0);
        
        // æ£€æŸ¥éŸ³é¢‘é•¿åº¦
        if audio.len() < 1600 { // å°‘äº0.1ç§’çš„éŸ³é¢‘è·³è¿‡
            println!("âš ï¸ Audio too short for recognition: {} samples", audio.len());
            return Ok(String::new());
        }
        
        // é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–éŸ³é¢‘
        let normalized_audio = Self::normalize_audio(audio);
        
        Self::recognize_speech_segment(&normalized_audio, config, whisper_state)
    }
    
    fn normalize_audio(audio: &[f32]) -> Vec<f32> {
        // è®¡ç®—RMS
        let rms = (audio.iter().map(|&x| x * x).sum::<f32>() / audio.len() as f32).sqrt();
        
        if rms < 0.001 {
            // éŸ³é¢‘å¤ªå®‰é™ï¼Œç›´æ¥è¿”å›
            return audio.to_vec();
        }
        
        // æ ‡å‡†åŒ–åˆ°é€‚å½“çš„éŸ³é‡
        let target_rms = 0.1;
        let gain = target_rms / rms;
        let max_gain = 10.0; // é™åˆ¶æœ€å¤§å¢ç›Š
        let actual_gain = gain.min(max_gain);
        
        println!("ğŸ”Š Audio normalization: RMS={:.6} -> gain={:.2}", rms, actual_gain);
        
        audio.iter().map(|&x| (x * actual_gain).clamp(-1.0, 1.0)).collect()
    }

    fn recognize_speech_segment(
        audio: &[f32],
        config: &RealtimeConfig,
        whisper_state: &WhisperContextState,
    ) -> Result<String, String> {
        println!("ğŸ”’ Attempting to acquire Whisper context lock...");
        
        let ctx = match whisper_state.ctx.lock() {
            Ok(ctx) => {
                println!("âœ… Whisper context lock acquired");
                ctx
            },
            Err(e) => {
                println!("âŒ Failed to acquire Whisper context lock: {}", e);
                return Err("Failed to acquire Whisper context lock".to_string());
            }
        };
        
        // éªŒè¯Whisperä¸Šä¸‹æ–‡
        if ctx.is_null() {
            println!("âŒ Whisper context is null");
            return Err("Whisper context is null".to_string());
        }
        
        println!("ğŸ”§ Setting up Whisper parameters...");
        
        // ä½¿ç”¨è´ªå©ªæœç´¢è€Œä¸æ˜¯beam searchï¼Œæ›´å¿«æ›´ç¨³å®š
        let mut params = unsafe { 
            whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH) 
        };
        
        // é’ˆå¯¹å®æ—¶è¯†åˆ«çš„ä¿å®ˆå‚æ•°è®¾ç½®
        params.temperature = 0.0;
        params.suppress_blank = true;
        params.token_timestamps = false;
        params.max_len = 1;
        params.n_threads = 1; // ä½¿ç”¨å•çº¿ç¨‹é¿å…ç«äº‰
        params.beam_search.beam_size = 1; // æœ€å°beam size
        params.greedy.best_of = 1;
        params.translate = false; // ç¦ç”¨ç¿»è¯‘
        params.no_context = true; // ç¦ç”¨ä¸Šä¸‹æ–‡ï¼Œæé«˜ç¨³å®šæ€§
        
        // è¯­è¨€è®¾ç½®
        let lang_cstring = match config.language.as_str() {
            "zh" => Some(CString::new("zh").unwrap()),
            "en" => Some(CString::new("en").unwrap()),
            _ => None,
        };
        
        if let Some(ref lang_str) = lang_cstring {
            params.language = lang_str.as_ptr();
        } else {
            params.language = std::ptr::null();
        }
        
        // éªŒè¯éŸ³é¢‘æ•°æ®
        if audio.is_empty() {
            println!("âš ï¸ Audio data is empty");
            return Ok(String::new());
        }
        
        println!("ğŸ“Š Audio data: {} samples, range: [{:.6}, {:.6}]", 
            audio.len(), 
            audio.iter().min_by(|a, b| a.partial_cmp(b).unwrap()).unwrap_or(&0.0),
            audio.iter().max_by(|a, b| a.partial_cmp(b).unwrap()).unwrap_or(&0.0)
        );
        
        // åˆ›å»ºéŸ³é¢‘æ•°æ®å‰¯æœ¬å¹¶ç¡®ä¿æ•°æ®æœ‰æ•ˆ
        let mut audio_copy: Vec<f32> = audio.iter()
            .map(|&x| x.clamp(-1.0, 1.0)) // ç¡®ä¿éŸ³é¢‘åœ¨æœ‰æ•ˆèŒƒå›´å†…
            .collect();
        
        println!("ğŸš€ Starting Whisper recognition...");
        
        // å®‰å…¨åœ°æ‰§è¡Œè¯†åˆ«
        let result = unsafe {
            whisper_full(
                *ctx,
                params,
                audio_copy.as_mut_ptr(),
                audio_copy.len() as i32,
            )
        };
        
        println!("ğŸ“ Whisper recognition result: {}", result);
        
        if result != 0 {
            println!("âŒ Whisper recognition failed with code: {}", result);
            return Err(format!("Whisper recognition failed with code: {}", result));
        }
        
        // å®‰å…¨åœ°æå–æ–‡æœ¬
        let num_segments = unsafe { whisper_full_n_segments(*ctx) };
        println!("ğŸ“‹ Number of segments: {}", num_segments);
        
        if num_segments == 0 {
            println!("âš ï¸ No segments recognized");
            return Ok(String::new());
        }
        
        let mut text = String::new();
        
        for i in 0..num_segments {
            let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
            if !segment_ptr.is_null() {
                let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
                match c_str.to_str() {
                    Ok(segment_text) => {
                        println!("ğŸ“ Segment {}: '{}'", i, segment_text);
                        text.push_str(segment_text);
                    },
                    Err(e) => {
                        println!("âš ï¸ Failed to convert segment {} to string: {}", i, e);
                    }
                }
            } else {
                println!("âš ï¸ Segment {} pointer is null", i);
            }
        }
        
        println!("ğŸ“œ Raw recognized text: '{}'", text);
        
        // å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•æ–‡æœ¬ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if text.trim().is_empty() {
            println!("â„¹ï¸ No text recognized");
            return Ok(String::new());
        }
        
        // æ–‡æœ¬åå¤„ç†
        let processed_text = post_process_text(&text, &config.language);
        println!("âœ¨ Processed text: '{}'", processed_text);
        
        Ok(processed_text)
    }

    pub fn get_recording_duration(&self) -> u64 {
        if let Some(start_time) = self.start_time {
            start_time.elapsed().as_secs()
        } else {
            0
        }
    }
}

// çº¿ç¨‹å®‰å…¨çš„çŠ¶æ€åŒ…è£…å™¨  
pub struct AudioCaptureState {
    inner: StdMutex<Option<RealtimeAudioCapture>>,
}

impl AudioCaptureState {
    pub fn new() -> Self {
        AudioCaptureState {
            inner: StdMutex::new(None),
        }
    }
    
    pub fn lock(&self) -> Result<std::sync::MutexGuard<Option<RealtimeAudioCapture>>, std::sync::PoisonError<std::sync::MutexGuard<Option<RealtimeAudioCapture>>>> {
        self.inner.lock()
    }
}

unsafe impl Send for AudioCaptureState {}
unsafe impl Sync for AudioCaptureState {}

impl Default for AudioCaptureState {
    fn default() -> Self {
        Self::new()
    }
}

// Tauriå‘½ä»¤å®ç°
#[tauri::command]
pub async fn start_realtime_recording(
    app_handle: AppHandle,
    config: RealtimeConfig,
    state: State<'_, AudioCaptureState>,
    whisper_state: State<'_, WhisperContextState>,
) -> Result<(), String> {
    println!("ğŸ¤ å¼€å§‹åˆå§‹åŒ–å®æ—¶å½•éŸ³...");
    println!("é…ç½®: {:?}", config);
    
    let mut capture_state = state.lock().map_err(|e| {
        let error_msg = format!("æ— æ³•è·å–å½•éŸ³çŠ¶æ€é”: {}", e);
        eprintln!("{}", error_msg);
        error_msg
    })?;
    
    // åˆ›å»ºæ–°çš„éŸ³é¢‘æ•è·å®ä¾‹
    let mut capture = RealtimeAudioCapture::new(app_handle, config)
        .map_err(|e| format!("Failed to create audio capture: {}", e))?;
    
    // å¯åŠ¨å½•éŸ³
    let whisper_state_arc = Arc::new(WhisperContextState {
        ctx: Mutex::new(*whisper_state.ctx.lock().unwrap()),
    });
    
    capture.start_recording(whisper_state_arc)
        .map_err(|e| format!("Failed to start recording: {}", e))?;
    
    *capture_state = Some(capture);
    println!("Realtime recording started");
    Ok(())
}

#[tauri::command]
pub async fn pause_realtime_recording(
    state: State<'_, AudioCaptureState>,
) -> Result<(), String> {
    let mut capture_state = state.lock().map_err(|e| e.to_string())?;
    
    if let Some(ref mut capture) = capture_state.as_mut() {
        capture.pause_recording().map_err(|e| e.to_string())?;
    }
    
    Ok(())
}

#[tauri::command]
pub async fn resume_realtime_recording(
    state: State<'_, AudioCaptureState>,
) -> Result<(), String> {
    let mut capture_state = state.lock().map_err(|e| e.to_string())?;
    
    if let Some(ref mut capture) = capture_state.as_mut() {
        capture.resume_recording().map_err(|e| e.to_string())?;
    }
    
    Ok(())
}

#[tauri::command]
pub async fn stop_realtime_recording(
    state: State<'_, AudioCaptureState>,
) -> Result<(), String> {
    let mut capture_state = state.lock().map_err(|e| e.to_string())?;
    
    if let Some(mut capture) = capture_state.take() {
        capture.stop_recording().map_err(|e| e.to_string())?;
    }
    
    Ok(())
}

#[tauri::command]
pub async fn get_recording_duration(
    state: State<'_, AudioCaptureState>,
) -> Result<u64, String> {
    let capture_state = state.lock().map_err(|e| e.to_string())?;
    
    if let Some(ref capture) = capture_state.as_ref() {
        Ok(capture.get_recording_duration())
    } else {
        Ok(0)
    }
}

// è·å–ç”¨æˆ·é€‰å®šçš„è¾“å…¥è®¾å¤‡ (å¼‚æ­¥ç‰ˆæœ¬)
async fn get_selected_input_device(host: &cpal::Host) -> Result<cpal::Device, String> {
    // å°è¯•è·å–å…¨å±€é€‰å®šçš„è®¾å¤‡ID
    let selected_device_id = audio_devices::get_global_audio_device("input".to_string()).await?;
    
    if let Some(device_id) = selected_device_id {
        // è§£æè®¾å¤‡ID
        let device_index: usize = device_id
            .strip_prefix("input_")
            .and_then(|s| s.parse().ok())
            .ok_or("Invalid device ID format")?;
        
        // è·å–è®¾å¤‡åˆ—è¡¨
        let devices: Vec<_> = host.input_devices()
            .map_err(|e| format!("Failed to enumerate devices: {}", e))?
            .collect();
        
        // è·å–æŒ‡å®šçš„è®¾å¤‡
        devices.get(device_index)
            .cloned()
            .ok_or("Selected device not found".to_string())
    } else {
        // å¦‚æœæ²¡æœ‰é€‰æ‹©è®¾å¤‡ï¼Œè¿”å›é»˜è®¤è®¾å¤‡
        host.default_input_device()
            .ok_or("No default input device available".to_string())
    }
}

// è·å–ç”¨æˆ·é€‰å®šçš„è¾“å…¥è®¾å¤‡ (åŒæ­¥ç‰ˆæœ¬)
fn get_selected_input_device_sync(host: &cpal::Host) -> Result<cpal::Device, String> {
    // ä½¿ç”¨tokioçš„é˜»å¡è°ƒç”¨æ¥æ‰§è¡Œå¼‚æ­¥å‡½æ•°
    let rt = tokio::runtime::Runtime::new()
        .map_err(|e| format!("Failed to create tokio runtime: {}", e))?;
    
    rt.block_on(async {
        // å°è¯•è·å–å…¨å±€é€‰å®šçš„è®¾å¤‡ID
        let selected_device_id = audio_devices::get_global_audio_device("input".to_string()).await?;
        
        if let Some(device_id) = selected_device_id {
            // è§£æè®¾å¤‡ID
            let device_index: usize = device_id
                .strip_prefix("input_")
                .and_then(|s| s.parse().ok())
                .ok_or("Invalid device ID format")?;
            
            // è·å–è®¾å¤‡åˆ—è¡¨
            let devices: Vec<_> = host.input_devices()
                .map_err(|e| format!("Failed to enumerate devices: {}", e))?
                .collect();
            
            // è·å–æŒ‡å®šçš„è®¾å¤‡
            devices.get(device_index)
                .cloned()
                .ok_or("Selected device not found".to_string())
        } else {
            // å¦‚æœæ²¡æœ‰é€‰æ‹©è®¾å¤‡ï¼Œè¿”å›é»˜è®¤è®¾å¤‡
            host.default_input_device()
                .ok_or("No default input device available".to_string())
        }
    })
}