// These are required for bindgen generated code
#![allow(non_upper_case_globals)]
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]
#![allow(dead_code)]

// Include FFI bindings generated by build.rs
include!(concat!(env!("OUT_DIR"), "/bindings.rs"));

// æ¨¡å—å¯¼å…¥
mod logging;
mod path_test;
mod storage;
mod storage_commands;
mod database_manager;
mod database_commands;
mod app_lifecycle;
mod long_audio;
mod long_audio_commands;
mod realtime_audio_full;
mod realtime_speaker_diarization;
mod audio_devices;
mod realtime_whisper;
// æ–°çš„ä¼˜åŒ–æ¨¡å—
mod audio_processing;
mod layered_processor;
mod context_processor;
mod result_manager;
mod optimal_realtime_processor;
mod model_management;

use std::ffi::{CStr, CString};
use std::os::raw::c_char;
use std::sync::{Arc, Mutex};
use tauri::{Emitter, Manager, WebviewWindow};
use serde::Serialize;
use regex::Regex;
use webrtc_vad::Vad;
use rustfft::{FftPlanner, num_complex::Complex32};
use rayon::prelude::*;

// å­˜å‚¨ç›¸å…³å¯¼å…¥
use storage_commands::StorageState;

// éŸ³é¢‘è½¬æ¢ç›¸å…³å¯¼å…¥
use symphonia::core::audio::SampleBuffer;
use symphonia::core::codecs::DecoderOptions;
use symphonia::core::formats::FormatOptions;
use symphonia::core::io::MediaSourceStream;
use symphonia::core::meta::MetadataOptions;
use symphonia::core::probe::Hint;
// use rubato::{FftFixedInOut, Resampler}; // æš‚æ—¶ä¸ä½¿ç”¨å¤æ‚çš„é‡é‡‡æ ·

struct WhisperContextState {
    ctx: Mutex<*mut whisper_context>,
}

// è¿›åº¦å›è°ƒæ•°æ®ç»“æ„
#[derive(Clone, Serialize)]
pub struct RecognitionProgress {
    pub stage: String,
    pub progress: f32,
    pub message: String,
}

// è¯†åˆ«ç»“æœæ•°æ®ç»“æ„
#[derive(Clone, Serialize)]
pub struct RecognitionResult {
    pub success: bool,
    pub text: Option<String>,
    pub error: Option<String>,
    pub processing_time: f64,
}

// å…¨å±€çŠ¶æ€ç®¡ç†å™¨
pub struct RecognitionState {
    pub is_processing: Arc<Mutex<bool>>,
    pub should_cancel: Arc<Mutex<bool>>,
}

unsafe impl Send for WhisperContextState {}
unsafe impl Sync for WhisperContextState {}

impl RecognitionState {
    fn new() -> Self {
        Self {
            is_processing: Arc::new(Mutex::new(false)),
            should_cancel: Arc::new(Mutex::new(false)),
        }
    }

    fn start_processing(&self) {
        *self.is_processing.lock().unwrap() = true;
        *self.should_cancel.lock().unwrap() = false;
    }

    fn stop_processing(&self) {
        *self.is_processing.lock().unwrap() = false;
        *self.should_cancel.lock().unwrap() = false;
    }

    fn request_cancel(&self) {
        *self.should_cancel.lock().unwrap() = true;
    }

    fn should_cancel(&self) -> bool {
        *self.should_cancel.lock().unwrap()
    }

    fn is_processing(&self) -> bool {
        *self.is_processing.lock().unwrap()
    }
}

impl WhisperContextState {
    fn new(model_path: &str) -> Result<Self, String> {
        let c_model_path = CString::new(model_path).map_err(|e| e.to_string())?;
        
        // Use new recommended API
        // 1. Get default context parameters
        let mut cparams = unsafe { whisper_context_default_params() };
        // 2. Enable GPU
        cparams.use_gpu = true;

        unsafe {
            // 3. Use initialization function with parameters
            let ctx = whisper_init_from_file_with_params(c_model_path.as_ptr(), cparams);
            if ctx.is_null() {
                Err("Failed to initialize whisper context".to_string())
            } else {
                Ok(Self {
                    ctx: Mutex::new(ctx),
                })
            }
        }
    }

    pub fn get_context_ptr(&self) -> *mut whisper_context {
        *self.ctx.lock().unwrap()
    }

    pub fn reinitialize(&self, model_path: &str) -> Result<(), String> {
        let c_model_path = CString::new(model_path).map_err(|e| e.to_string())?;
        
        let mut cparams = unsafe { whisper_context_default_params() };
        cparams.use_gpu = true;

        unsafe {
            let new_ctx = whisper_init_from_file_with_params(c_model_path.as_ptr(), cparams);
            if new_ctx.is_null() {
                return Err("Failed to initialize new whisper context".to_string());
            }

            let mut ctx_lock = self.ctx.lock().unwrap();
            let old_ctx = *ctx_lock;
            *ctx_lock = new_ctx;
            
            // é‡Šæ”¾æ—§çš„ä¸Šä¸‹æ–‡
            if !old_ctx.is_null() {
                whisper_free(old_ctx);
            }
        }

        Ok(())
    }

    // åˆ›å»ºç©ºçš„ä¸Šä¸‹æ–‡ï¼Œç”¨äºæ¨¡å‹ä¸å­˜åœ¨çš„æƒ…å†µ
    fn new_empty() -> Self {
        Self {
            ctx: Mutex::new(std::ptr::null_mut()),
        }
    }
}

impl Drop for WhisperContextState {
    fn drop(&mut self) {
        unsafe {
            // å®‰å…¨åœ°è·å–é”ï¼Œé¿å…åœ¨æ¸…ç†æ—¶panic
            if let Ok(ctx) = self.ctx.lock() {
                whisper_free(*ctx);
            }
        }
    }
}

// éŸ³é¢‘æ ¼å¼è½¬æ¢å‡½æ•° - æ”¯æŒå¤šç§æ ¼å¼åŒ…æ‹¬MP3, M4A, AACç­‰
pub fn load_and_convert_audio(file_path: &str) -> Result<(Vec<f32>, u32, f64), String> {
    println!("å¼€å§‹å¤„ç†éŸ³é¢‘æ–‡ä»¶: {}", file_path);
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶
    let file = std::fs::File::open(file_path).map_err(|e| format!("æ— æ³•æ‰“å¼€æ–‡ä»¶: {}", e))?;
    let mss = MediaSourceStream::new(Box::new(file), Default::default());

    // æ¢æµ‹éŸ³é¢‘æ ¼å¼
    let mut hint = Hint::new();
    if let Some(extension) = std::path::Path::new(file_path).extension() {
        if let Some(ext_str) = extension.to_str() {
            hint.with_extension(ext_str);
            println!("æ£€æµ‹åˆ°æ–‡ä»¶æ‰©å±•å: {}", ext_str);
        }
    }

    let meta_opts: MetadataOptions = Default::default();
    let fmt_opts: FormatOptions = Default::default();

    let probed = symphonia::default::get_probe()
        .format(&hint, mss, &fmt_opts, &meta_opts)
        .map_err(|e| format!("æ— æ³•æ¢æµ‹éŸ³é¢‘æ ¼å¼ (æ”¯æŒMP3, M4A, AAC, FLAC, OGG, WAVç­‰): {}", e))?;

    let mut format = probed.format;
    println!("æˆåŠŸæ¢æµ‹éŸ³é¢‘æ ¼å¼");
    
    let track = format
        .tracks()
        .iter()
        .find(|t| t.codec_params.codec != symphonia::core::codecs::CODEC_TYPE_NULL)
        .ok_or("æ‰¾ä¸åˆ°éŸ³é¢‘è½¨é“")?;
    
    println!("æ‰¾åˆ°éŸ³é¢‘è½¨é“ï¼Œç¼–è§£ç å™¨: {:?}", track.codec_params.codec);

    let track_id = track.id;
    let mut decoder = symphonia::default::get_codecs()
        .make(&track.codec_params, &DecoderOptions { verify: false })
        .map_err(|e| format!("æ— æ³•åˆ›å»ºè§£ç å™¨: {}", e))?;

    let codec_params = &track.codec_params;
    let sample_rate = codec_params.sample_rate.ok_or("æ— æ³•è·å–é‡‡æ ·ç‡")?;
    let channels = if let Some(channel_layout) = codec_params.channels {
        channel_layout.count()
    } else {
        // å¯¹äºæŸäº›M4A/AACæ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦ä»ç¬¬ä¸€ä¸ªåŒ…ä¸­æ¨æ–­å£°é“æ•°
        println!("æ— æ³•ç›´æ¥è·å–å£°é“æ•°ï¼Œå°†åœ¨è§£ç è¿‡ç¨‹ä¸­ç¡®å®š");
        1 // å…ˆå‡è®¾å•å£°é“ï¼Œç¨åä»å®é™…è§£ç æ•°æ®ä¸­è·å–
    };
    
    println!("éŸ³é¢‘ä¿¡æ¯: {}Hz, {}å£°é“(é¢„ä¼°)", sample_rate, channels);

    // è§£ç éŸ³é¢‘æ•°æ®
    let mut audio_samples = Vec::new();
    let mut sample_buf = None;
    let mut actual_channels = channels; // ä»éŸ³é¢‘æ•°æ®ä¸­è·å–çš„å®é™…å£°é“æ•°

    loop {
        let packet = match format.next_packet() {
            Ok(packet) => packet,
            Err(_) => break,
        };

        if packet.track_id() != track_id {
            continue;
        }

        match decoder.decode(&packet) {
            Ok(decoded) => {
                if sample_buf.is_none() {
                    let spec = *decoded.spec();
                    actual_channels = spec.channels.count(); // ä»å®é™…è§£ç æ•°æ®è·å–å£°é“æ•°
                    println!("ä»è§£ç æ•°æ®ç¡®å®šå®é™…å£°é“æ•°: {}", actual_channels);
                    let duration = decoded.capacity() as u64;
                    sample_buf = Some(SampleBuffer::<f32>::new(duration, spec));
                }

                if let Some(ref mut buf) = sample_buf {
                    buf.copy_interleaved_ref(decoded);
                    audio_samples.extend_from_slice(buf.samples());
                }
            }
            Err(e) => {
                println!("è§£ç åŒ…æ—¶å‡ºé”™: {}, è·³è¿‡", e);
                continue;
            }
        }
    }

    if audio_samples.is_empty() {
        return Err("æ— æ³•è§£ç éŸ³é¢‘æ•°æ®".to_string());
    }

    // è½¬æ¢ä¸ºå•å£°é“ (å¦‚æœæ˜¯å¤šå£°é“)
    let mono_samples = if actual_channels > 1 {
        println!("è½¬æ¢{}å£°é“éŸ³é¢‘ä¸ºå•å£°é“", actual_channels);
        audio_samples
            .chunks(actual_channels)
            .map(|chunk| chunk.iter().sum::<f32>() / actual_channels as f32)
            .collect()
    } else {
        println!("éŸ³é¢‘å·²ç»æ˜¯å•å£°é“");
        audio_samples
    };

    // é‡é‡‡æ ·åˆ° 16kHz (å¦‚æœéœ€è¦)
    let final_samples = if sample_rate != 16000 {
        println!("éœ€è¦é‡é‡‡æ ·: {}Hz -> 16000Hz, æ ·æœ¬æ•°: {}", sample_rate, mono_samples.len());
        
        // ä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·
        match high_quality_resample(&mono_samples, sample_rate, 16000) {
            Ok(resampled) => {
                println!("é«˜è´¨é‡é‡é‡‡æ ·å®Œæˆ: {} -> {} ä¸ªé‡‡æ ·ç‚¹", mono_samples.len(), resampled.len());
                resampled
            }
            Err(e) => {
                println!("é«˜è´¨é‡é‡é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {}", e);
                // å›é€€åˆ°ç®€å•é‡é‡‡æ ·
                fallback_resample(&mono_samples, sample_rate, 16000)
            }
        }
    } else {
        mono_samples
    };

    // åº”ç”¨å®Œæ•´éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿
    let config = AudioProcessingConfig::default();
    let optimized_samples = match advanced_audio_preprocessing_pipeline(final_samples.clone(), &config) {
        Ok(processed) => {
            println!("å®Œæ•´éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿æˆåŠŸ");
            processed
        }
        Err(e) => {
            println!("å®Œæ•´é¢„å¤„ç†å¤±è´¥ï¼Œå°è¯•ç®€å•ä¼˜åŒ–: {}", e);
            // å›é€€åˆ°ç®€å•å¤„ç†
            match preprocess_audio_with_vad(final_samples.clone()) {
                Ok(processed) => processed,
                Err(_) => {
                    println!("ç®€å•å¤„ç†ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘");
                    final_samples
                }
            }
        }
    };

    let duration = optimized_samples.len() as f64 / 16000.0;
    Ok((optimized_samples, 16000, duration))
}

// Learn more about Tauri commands at https://tauri.app/develop/calling-rust/
#[tauri::command]
fn greet(name: &str) -> String {
    format!("Hello, {}! You've been greeted from Rust!", name)
}

// æ”¹è¿›çš„è¯­éŸ³è¯†åˆ«å‘½ä»¤ï¼Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œé¿å…é˜»å¡å‰ç«¯
#[tauri::command]
fn recognize_file_async(
    path: String,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    app_handle: tauri::AppHandle,
) -> Result<String, String> {
    // è·å–çŠ¶æ€ç®¡ç†å™¨
    let recognition_state = app_handle.state::<RecognitionState>();
    
    // æ£€æŸ¥æ˜¯å¦å·²åœ¨å¤„ç†ä¸­
    if recognition_state.is_processing() {
        return Err("å·²æœ‰è¯†åˆ«ä»»åŠ¡åœ¨è¿›è¡Œä¸­".to_string());
    }
    
    // å¯åŠ¨å¤„ç†çŠ¶æ€
    recognition_state.start_processing();
    
    // è·å–ä¸»çª—å£
    let window = app_handle.get_webview_window("main").unwrap();
    
    // ç«‹å³å‘é€å¼€å§‹äº‹ä»¶
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "initializing".to_string(),
        progress: 0.0,
        message: "åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å¼•æ“...".to_string(),
    });
    
    // åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œè¯†åˆ«ï¼Œé¿å…é˜»å¡å‰ç«¯
    let path_clone = path.clone();
    let language_clone = language.clone();
    let mode_clone = mode.clone();
    let initial_prompt_clone = initial_prompt.clone();
    let app_handle_clone = app_handle.clone();
    
    std::thread::spawn(move || {
        let whisper_state = app_handle_clone.state::<WhisperContextState>();
        let recognition_state = app_handle_clone.state::<RecognitionState>();
        let window = app_handle_clone.get_webview_window("main").unwrap();
        
        let result = recognize_file_blocking_inner(
            path_clone, 
            language_clone,
            mode_clone,
            initial_prompt_clone,
            window, 
            &*whisper_state, 
            &*recognition_state
        );
        
        // æ— è®ºæˆåŠŸå¤±è´¥éƒ½åœæ­¢å¤„ç†çŠ¶æ€
        recognition_state.stop_processing();
        
        match result {
            Ok(text) => println!("è¯†åˆ«æˆåŠŸå®Œæˆ: {} å­—ç¬¦", text.len()),
            Err(e) => println!("è¯†åˆ«å¤±è´¥: {}", e),
        }
    });
    
    Ok("è¯†åˆ«ä»»åŠ¡å·²å¯åŠ¨".to_string())
}

#[tauri::command]
fn cancel_file_transcription(app_handle: tauri::AppHandle) -> Result<String, String> {
    let recognition_state = app_handle.state::<RecognitionState>();
    
    // æ€»æ˜¯è®¾ç½®å–æ¶ˆæ ‡å¿—ï¼Œå³ä½¿å½“å‰çŠ¶æ€æ˜¾ç¤ºæ²¡æœ‰å¤„ç†ä¸­çš„ä»»åŠ¡
    // è¿™å¯ä»¥å¤„ç†ç«æ€æ¡ä»¶é—®é¢˜ï¼Œç¡®ä¿å–æ¶ˆè¯·æ±‚ä¸ä¼šè¢«æ‹’ç»
    recognition_state.request_cancel();
    
    // å‘é€å–æ¶ˆäº‹ä»¶åˆ°å‰ç«¯
    if let Some(window) = app_handle.get_webview_window("main") {
        let _ = window.emit("recognition_progress", RecognitionProgress {
            stage: "cancelling".to_string(),
            progress: 0.0,
            message: "æ­£åœ¨å–æ¶ˆè½¬å½•...".to_string(),
        });
    }
    
    // æ ¹æ®å¤„ç†çŠ¶æ€è¿”å›ç›¸åº”çš„æ¶ˆæ¯
    if recognition_state.is_processing() {
        Ok("è½¬å½•å–æ¶ˆè¯·æ±‚å·²å‘é€".to_string())
    } else {
        Ok("å–æ¶ˆè¯·æ±‚å·²å¤„ç†".to_string())
    }
}

// å®é™…çš„é˜»å¡å¼è¯†åˆ«å‡½æ•°
fn recognize_file_blocking_inner(
    path: String,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    window: WebviewWindow,
    whisper_state: &WhisperContextState,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    let start_time = std::time::Instant::now();
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }

    // æ­¥éª¤1: éŸ³é¢‘è½¬æ¢
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "converting".to_string(),
        progress: 20.0,
        message: "æ­£åœ¨è½¬æ¢éŸ³é¢‘æ ¼å¼...".to_string(),
    });

    let (audio_data, _, _) = match load_and_convert_audio(&path) {
        Ok(data) => {
            println!("éŸ³é¢‘è½¬æ¢æˆåŠŸ: {} ä¸ªé‡‡æ ·ç‚¹", data.0.len());
            data
        }
        Err(e) => {
            // å¦‚æœé€šç”¨è½¬æ¢å¤±è´¥ï¼Œå°è¯•ä½œä¸º WAV æ–‡ä»¶å¤„ç†
            println!("é€šç”¨éŸ³é¢‘è½¬æ¢å¤±è´¥: {}, å°è¯• WAV æ ¼å¼", e);
            
            let _ = window.emit("recognition_progress", RecognitionProgress {
                stage: "converting_wav".to_string(),
                progress: 25.0,
                message: "å°è¯•WAVæ ¼å¼è½¬æ¢...".to_string(),
            });
            
            let mut reader = hound::WavReader::open(&path).map_err(|e| {
                let error_msg = format!("æ— æ³•æ‰“å¼€æ–‡ä»¶ (å°è¯•äº†é€šç”¨æ ¼å¼å’Œ WAV æ ¼å¼): {}", e);
                // åœæ­¢å¤„ç†çŠ¶æ€å¹¶å‘é€é”™è¯¯äº‹ä»¶
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                error_msg
            })?;
            let spec = reader.spec();
            
            if spec.channels != 1 || spec.sample_rate != 16000 {
                let error_msg = format!(
                    "æ£€æµ‹åˆ° WAV æ–‡ä»¶ï¼Œä½†æ ¼å¼ä¸ç¬¦åˆè¦æ±‚:\\nå½“å‰: {}å£°é“, {}Hz\\néœ€è¦: 1å£°é“, 16000Hz\\n\\nå»ºè®®ä½¿ç”¨ FFmpeg è½¬æ¢:\\nffmpeg -i \\\"{}\\\" -ar 16000 -ac 1 output.wav", 
                    spec.channels, spec.sample_rate, path
                );
                // åœæ­¢å¤„ç†çŠ¶æ€å¹¶å‘é€é”™è¯¯äº‹ä»¶
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                return Err(error_msg);
            }
            
            let samples: Result<Vec<i16>, _> = reader.samples().collect();
            let samples = samples.map_err(|e| {
                let error_msg = e.to_string();
                // åœæ­¢å¤„ç†çŠ¶æ€å¹¶å‘é€é”™è¯¯äº‹ä»¶
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                error_msg
            })?;
            
            (samples.iter().map(|&s| s as f32 / 32768.0).collect(), 16000, samples.len() as f64 / 16000.0)
        }
    };

    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }

    // æ­¥éª¤2: å‡†å¤‡è¯†åˆ«
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "preparing".to_string(),
        progress: 40.0,
        message: "å‡†å¤‡è¯­éŸ³è¯†åˆ«...".to_string(),
    });

    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }

    // æ­¥éª¤3: ä½¿ç”¨é«˜çº§éŸ³é¢‘è¯†åˆ«æµç¨‹
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "processing".to_string(),
        progress: 50.0,
        message: "å¼€å§‹é«˜çº§éŸ³é¢‘é¢„å¤„ç†...".to_string(),
    });

    // ä½¿ç”¨é«˜çº§è¯†åˆ«æµæ°´çº¿ï¼ŒåŒ…å«éŸ³é¢‘é¢„å¤„ç†ã€VADåˆ†æ®µã€å¢å¼ºç­‰åŠŸèƒ½
    let full_text = match advanced_recognition_pipeline(audio_data, language.clone(), mode.clone(), initial_prompt.clone(), whisper_state, &window, recognition_state) {
        Ok(text) => text,
        Err(e) => {
            let error_msg = format!("é«˜çº§è¯†åˆ«æµç¨‹å¤±è´¥: {}", e);
            // åœæ­¢å¤„ç†çŠ¶æ€å¹¶å‘é€é”™è¯¯äº‹ä»¶
            recognition_state.stop_processing();
            let _ = window.emit("recognition_complete", RecognitionResult {
                success: false,
                text: None,
                error: Some(error_msg.clone()),
                processing_time: start_time.elapsed().as_secs_f64(),
            });
            return Err(error_msg);
        }
    };

    // æ­¥éª¤4: æ–‡æœ¬åå¤„ç†
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "post_processing".to_string(),
        progress: 95.0,
        message: "åå¤„ç†è¯†åˆ«ç»“æœ...".to_string(),
    });

    let processed_text = post_process_text(&full_text, &language);

    let processing_time = start_time.elapsed().as_secs_f64();

    // æ­¥éª¤5: å®Œæˆ
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "completing".to_string(),
        progress: 100.0,
        message: "è¯†åˆ«å®Œæˆ!".to_string(),
    });

    // å‘é€ç»“æœäº‹ä»¶
    let final_result = RecognitionResult {
        success: true,
        text: Some(processed_text.clone()),
        error: None,
        processing_time,
    };

    let _ = window.emit("recognition_complete", final_result);
    
    // åœæ­¢å¤„ç†çŠ¶æ€
    recognition_state.stop_processing();

    Ok(processed_text)
}


// #[tauri::command]
// fn recognize_file(
//     path: String,
//     whisper_state: State<'_, WhisperContextState>,
// ) -> Result<String, String> {
//     let mut reader = hound::WavReader::open(&path).map_err(|e| e.to_string())?;
//     let samples: Vec<i16> = reader.samples().map(|s| s.unwrap()).collect();

//     // whisper.cpp needs f32 format sample data
//     let mut audio_data: Vec<f32> = samples
//         .iter()
//         .map(|&s| s as f32 / 32768.0)
//         .collect();

//     let ctx = whisper_state.ctx.lock().unwrap();

//     let mut params = unsafe { whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_GREEDY) };
//     params.n_threads = 8; // M1 has 8 performance cores

//     let result = unsafe {
//         whisper_full(
//             *ctx,
//             params,
//             audio_data.as_mut_ptr(),
//             audio_data.len() as i32,
//         )
//     };

//     if result != 0 {
//         return Err("Whisper recognition failed".to_string());
//     }

//     let num_segments = unsafe { whisper_full_n_segments(*ctx) };
//     let mut full_text = String::new();

//     for i in 0..num_segments {
//         let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
//         if !segment_ptr.is_null() {
//             let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
//             full_text.push_str(c_str.to_str().unwrap_or(""));
//         }
//     }

//     Ok(full_text)
// }

#[cfg_attr(mobile, tauri::mobile_entry_point)]
// çª—å£æ§åˆ¶å‘½ä»¤
#[tauri::command]
async fn minimize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.minimize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn maximize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.maximize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn unmaximize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.unmaximize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn close_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.close().map_err(|e| e.to_string())
}

#[tauri::command]
async fn is_maximized(window: tauri::WebviewWindow) -> Result<bool, String> {
    window.is_maximized().map_err(|e| e.to_string())
}

pub fn run() {
    // å…ˆåˆ›å»ºModelManageræ¥è¯»å–æŒä¹…åŒ–é…ç½®
    let model_manager = Arc::new(Mutex::new(model_management::ModelManager::new()));
    
    // ä»æŒä¹…åŒ–é…ç½®è·å–å½“å‰æ¨¡å‹è·¯å¾„
    let model_path = {
        let manager = model_manager.lock().unwrap();
        manager.get_current_model_path()
    };
    
    // æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
    let final_model_path = if model_path.exists() {
        model_path
    } else {
        // å¦‚æœé…ç½®çš„æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        let default_path = std::env::current_dir()
            .unwrap()
            .parent() // ä» src-tauri ç›®å½•å›åˆ°é¡¹ç›®æ ¹ç›®å½•
            .unwrap()
            .join("models")
            .join("ggml-large-v3.bin");
        
        // å¦‚æœé»˜è®¤æ¨¡å‹ä¹Ÿä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç©ºä¸Šä¸‹æ–‡
        if !default_path.exists() {
            eprintln!("Warning: No model file found. Please download a model first.");
            // åˆ›å»ºç©ºçš„æ¨¡å‹è·¯å¾„ï¼Œç¨åéœ€è¦ç”¨æˆ·æ‰‹åŠ¨åˆ‡æ¢
            std::env::current_dir()
                .unwrap()
                .parent()
                .unwrap()
                .join("models")
                .join("dummy.bin")
        } else {
            default_path
        }
    };
    
    let whisper_context = if final_model_path.exists() {
        WhisperContextState::new(&final_model_path.to_string_lossy())
            .expect("Failed to create whisper context")
    } else {
        // åˆ›å»ºä¸€ä¸ªå ä½ç¬¦ä¸Šä¸‹æ–‡ï¼Œç¨åéœ€è¦é‡æ–°åˆå§‹åŒ–
        WhisperContextState::new_empty()
    };
    
    let recognition_state = RecognitionState::new();

    tauri::Builder::default()
        .manage(whisper_context)
        .manage(recognition_state)
        .manage(StorageState::new())
        .manage(realtime_audio_full::AudioCaptureState::default())
        // æ–°çš„ä¼˜åŒ–å¤„ç†å™¨çŠ¶æ€
        .manage(optimal_realtime_processor::OptimalRealtimeState::default())
        // æ¨¡å‹ç®¡ç†çŠ¶æ€
        .manage(model_manager)
        .plugin(tauri_plugin_opener::init())
        .plugin(tauri_plugin_dialog::init())
        .plugin(tauri_plugin_fs::init())
        .plugin(tauri_plugin_window_state::Builder::default().build())
        .plugin(tauri_plugin_os::init())
        .invoke_handler(tauri::generate_handler![
            greet, 
            recognize_file_async,
            cancel_file_transcription,
            storage_commands::init_storage,
            storage_commands::save_transcription_record,
            storage_commands::get_transcription_record,
            storage_commands::get_all_transcription_records,
            storage_commands::update_transcription_status,
            storage_commands::update_transcription_result,
            storage_commands::delete_transcription_record,
            storage_commands::toggle_transcription_star,
            storage_commands::update_transcription_name,
            storage_commands::search_transcription_records,
            // æç¤ºè¯ç®¡ç†ç›¸å…³å‘½ä»¤
            storage_commands::get_prompt_templates,
            storage_commands::get_prompts_by_filter,
            storage_commands::save_prompt_template,
            storage_commands::get_prompt_template,
            storage_commands::delete_prompt_template,
            storage_commands::search_prompt_templates,
            storage_commands::increment_prompt_usage,
            // æ•°æ®åº“ç®¡ç†å‘½ä»¤
            database_commands::get_database_info,
            database_commands::create_database_backup,
            database_commands::list_database_backups,
            database_commands::restore_database_backup,
            database_commands::vacuum_database,
            database_commands::check_database_integrity,
            database_commands::delete_database_backup,
            long_audio_commands::create_long_audio_task,
            long_audio_commands::start_long_audio_task,
            long_audio_commands::pause_long_audio_task,
            long_audio_commands::resume_long_audio_task,
            long_audio_commands::cancel_long_audio_task,
            long_audio_commands::get_long_audio_task,
            long_audio_commands::get_all_long_audio_tasks,
            realtime_audio_full::start_realtime_recording,
            realtime_audio_full::pause_realtime_recording,
            realtime_audio_full::resume_realtime_recording,
            realtime_audio_full::stop_realtime_recording,
            realtime_audio_full::get_recording_duration,
            audio_devices::get_audio_devices,
            audio_devices::test_audio_device,
            audio_devices::stop_audio_test,
            audio_devices::start_mic_test,
            audio_devices::get_mic_test_state,
            audio_devices::play_recorded_audio,
            audio_devices::set_global_audio_device,
            audio_devices::get_global_audio_device,
            // æ–°çš„ä¼˜åŒ–å®æ—¶å¤„ç†å‘½ä»¤
            optimal_realtime_processor::start_optimal_realtime_recording,
            optimal_realtime_processor::pause_optimal_realtime_recording,
            optimal_realtime_processor::resume_optimal_realtime_recording,
            optimal_realtime_processor::stop_optimal_realtime_recording,
            optimal_realtime_processor::get_optimal_current_transcript,
            optimal_realtime_processor::get_optimal_segments,
            optimal_realtime_processor::update_optimal_segment,
            optimal_realtime_processor::get_optimal_recording_duration,
            // çª—å£æ§åˆ¶å‘½ä»¤
            minimize_window,
            maximize_window,
            unmaximize_window,
            close_window,
            is_maximized,
            // æ¨¡å‹ç®¡ç†å‘½ä»¤
            model_management::list_installed_models,
            model_management::get_storage_info,
            model_management::download_model,
            model_management::switch_model,
            model_management::delete_model,
            model_management::scan_local_models,
            model_management::import_local_model,
            model_management::get_current_model
        ])
        .setup(|app| {
            let app_handle = app.handle().clone();
            
            // é¦–å…ˆåˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
            if let Err(e) = logging::init_logging(&app_handle) {
                eprintln!("âš ï¸ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {}", e);
                // æ—¥å¿—åˆå§‹åŒ–å¤±è´¥ä¸åº”é˜»æ­¢åº”ç”¨å¯åŠ¨
            }
            
            log::info!("ğŸš€ å¼€å§‹ Tauri åº”ç”¨è®¾ç½®...");
            println!("ğŸš€ å¼€å§‹ Tauri åº”ç”¨è®¾ç½®...");
            
            // ã€ç»Ÿä¸€ä¼˜åŒ–ã€‘åŒæ­¥åˆå§‹åŒ–å…³é”®å­˜å‚¨ç»„ä»¶ - ç¡®ä¿åœ¨å‰ç«¯è®¿é—®å‰å®Œæˆ
            log::info!("ğŸ”§ å¼€å§‹åŒæ­¥åˆå§‹åŒ–å…³é”®å­˜å‚¨ç»„ä»¶...");
            println!("ğŸ”§ å¼€å§‹åŒæ­¥åˆå§‹åŒ–å…³é”®å­˜å‚¨ç»„ä»¶...");
            
            // 1. ä¼˜å…ˆåˆå§‹åŒ–å­˜å‚¨æœåŠ¡ï¼ˆåŒæ­¥ï¼Œå¿«é€Ÿå®Œæˆï¼‰
            if let Some(storage_state) = app.try_state::<storage_commands::StorageState>() {
                match storage_state.init(&app_handle) {
                    Ok(_) => {
                        log::info!("âœ… å­˜å‚¨æœåŠ¡åŒæ­¥åˆå§‹åŒ–æˆåŠŸ");
                        println!("âœ… å­˜å‚¨æœåŠ¡åŒæ­¥åˆå§‹åŒ–æˆåŠŸ");
                    },
                    Err(e) => {
                        log::error!("âŒ å­˜å‚¨æœåŠ¡åŒæ­¥åˆå§‹åŒ–å¤±è´¥: {}", e);
                        eprintln!("âŒ å­˜å‚¨æœåŠ¡åŒæ­¥åˆå§‹åŒ–å¤±è´¥: {}", e);
                        // å­˜å‚¨åˆå§‹åŒ–å¤±è´¥æ˜¯ä¸¥é‡é—®é¢˜ï¼Œä½†ä¸åº”é˜»æ­¢åº”ç”¨å¯åŠ¨
                        // ä¾èµ–è‡ªåŠ¨é‡è¯•æœºåˆ¶æ¥æ¢å¤
                    }
                }
            } else {
                log::warn!("âš ï¸ æœªæ‰¾åˆ°å­˜å‚¨çŠ¶æ€ï¼Œè·³è¿‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–");
                eprintln!("âš ï¸ æœªæ‰¾åˆ°å­˜å‚¨çŠ¶æ€ï¼Œè·³è¿‡å­˜å‚¨æœåŠ¡åˆå§‹åŒ–");
            }
            
            // 2. å¼‚æ­¥åˆå§‹åŒ–å…¶ä»–éå…³é”®ç»„ä»¶ï¼ˆä¸é˜»å¡UIï¼‰
            let app_handle_clone = app_handle.clone();
            tauri::async_runtime::spawn(async move {
                log::info!("ğŸ”§ å¼€å§‹å¼‚æ­¥åˆå§‹åŒ–éå…³é”®ç»„ä»¶...");
                
                // æ‰§è¡Œå…¶ä»–éå…³é”®çš„åˆå§‹åŒ–ä»»åŠ¡
                match initialize_non_critical_components(&app_handle_clone).await {
                    Ok(_) => {
                        log::info!("âœ… éå…³é”®ç»„ä»¶å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ");
                        println!("âœ… éå…³é”®ç»„ä»¶å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ");
                    },
                    Err(e) => {
                        log::warn!("âš ï¸ éå…³é”®ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {}", e);
                        eprintln!("âš ï¸ éå…³é”®ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {}", e);
                        // éå…³é”®ç»„ä»¶å¤±è´¥ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
                    }
                }
            });
            
            log::info!("âœ… Tauri åº”ç”¨è®¾ç½®å®Œæˆ");
            println!("âœ… Tauri åº”ç”¨è®¾ç½®å®Œæˆ");
            Ok(())
        })
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}

/// åˆå§‹åŒ–éå…³é”®ç»„ä»¶ - å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡åº”ç”¨å¯åŠ¨
async fn initialize_non_critical_components(app_handle: &tauri::AppHandle) -> Result<(), String> {
    log::info!("ğŸ”§ å¼€å§‹åˆå§‹åŒ–éå…³é”®ç»„ä»¶...");
    
    // 1. æ‰§è¡Œæ•°æ®åº“ç»´æŠ¤æ£€æŸ¥ï¼ˆå¦‚å¤‡ä»½ã€å®Œæ•´æ€§æ£€æŸ¥ç­‰ï¼‰
    if let Some(lifecycle_manager) = app_handle.try_state::<app_lifecycle::AppLifecycleManager>() {
        if let Err(e) = lifecycle_manager.on_app_start().await {
            log::warn!("âš ï¸ åº”ç”¨ç”Ÿå‘½å‘¨æœŸç»„ä»¶å¯åŠ¨å¤±è´¥: {}", e);
            // ç”Ÿå‘½å‘¨æœŸç®¡ç†å¤±è´¥ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
        }
    }
    
    // 2. å…¶ä»–éå…³é”®åˆå§‹åŒ–ä»»åŠ¡å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
    // ä¾‹å¦‚ï¼šé¢„åŠ è½½é…ç½®ã€æ£€æŸ¥æ›´æ–°ç­‰
    
    log::info!("âœ… éå…³é”®ç»„ä»¶åˆå§‹åŒ–å®Œæˆ");
    Ok(())
}

// æ–‡æœ¬åå¤„ç†å‡½æ•°
fn post_process_text(text: &str, language: &str) -> String {
    let mut processed = text.to_string();
    
    // åŸºç¡€æ¸…ç†ï¼šå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    processed = processed.trim().to_string();
    processed = Regex::new(r"\s+").unwrap().replace_all(&processed, " ").to_string();
    
    match language {
        "zh" => post_process_chinese(&processed),
        "en" => post_process_english(&processed),
        _ => post_process_auto(&processed), // è‡ªåŠ¨æ£€æµ‹æˆ–å…¶ä»–è¯­è¨€
    }
}

// å»é™¤é‡å¤å­—ç¬¦çš„è¾…åŠ©å‡½æ•°
fn remove_repeated_chars(text: &str) -> String {
    let chars: Vec<char> = text.chars().collect();
    let mut result = String::new();
    let mut i = 0;
    
    while i < chars.len() {
        let current_char = chars[i];
        result.push(current_char);
        
        // è·³è¿‡è¿ç»­çš„ç›¸åŒå­—ç¬¦ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
        while i + 1 < chars.len() && chars[i + 1] == current_char {
            i += 1;
        }
        i += 1;
    }
    
    result
}

// ä¸­æ–‡æ–‡æœ¬åå¤„ç†
fn post_process_chinese(text: &str) -> String {
    let mut result = text.to_string();
    
    // 1. å»é™¤é‡å¤çš„å­—ç¬¦ (ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢æ–¹å¼)
    // æ³¨æ„: Rustçš„regexä¸æ”¯æŒåå‘å¼•ç”¨ï¼Œæ”¹ç”¨æ‰‹åŠ¨å¤„ç†
    result = remove_repeated_chars(&result);
    
    // 2. ä¿®å¤å¸¸è§çš„ä¸­æ–‡è¯†åˆ«é”™è¯¯
    let fixes = vec![
        ("çš„çš„", "çš„"),
        ("äº†äº†", "äº†"),
        ("åœ¨åœ¨", "åœ¨"),
        ("æ˜¯æ˜¯", "æ˜¯"),
        ("æœ‰æœ‰", "æœ‰"),
        ("æˆ‘æˆ‘", "æˆ‘"),
        ("ä½ ä½ ", "ä½ "),
        ("ä»–ä»–", "ä»–"),
        ("å¥¹å¥¹", "å¥¹"),
        ("å®ƒå®ƒ", "å®ƒ"),
    ];
    
    for (wrong, correct) in fixes {
        result = result.replace(wrong, correct);
    }
    
    // 3. ä¸­æ–‡æ ‡ç‚¹ç¬¦å·çŸ«æ­£
    result = result.replace("ã€‚ã€‚", "ã€‚");
    result = result.replace("ï¼Œï¼Œ", "ï¼Œ");
    result = result.replace("ï¼Ÿï¼Ÿ", "ï¼Ÿ");
    result = result.replace("ï¼ï¼", "ï¼");
    
    // 4. å¥å­é¦–å°¾å¤„ç†
    result = result.trim().to_string();
    
    // 5. æ·»åŠ é€‚å½“çš„å¥å·ï¼ˆå¦‚æœæ–‡æœ¬è¾ƒé•¿ä¸”æ²¡æœ‰ç»“å°¾æ ‡ç‚¹ï¼‰
    if result.len() > 10 && !result.ends_with(['ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!']) {
        result.push('ã€‚');
    }
    
    result
}

// è‹±æ–‡æ–‡æœ¬åå¤„ç†
fn post_process_english(text: &str) -> String {
    let mut result = text.to_string();
    
    // 1. ä¿®å¤å¸¸è§çš„è‹±æ–‡è¯†åˆ«é”™è¯¯
    let fixes = vec![
        (" a a ", " a "),
        (" the the ", " the "),
        (" and and ", " and "),
        (" to to ", " to "),
        (" of of ", " of "),
        (" in in ", " in "),
        (" is is ", " is "),
        (" it it ", " it "),
        (" that that ", " that "),
    ];
    
    for (wrong, correct) in fixes {
        result = result.replace(wrong, correct);
    }
    
    // 2. å¤§å°å†™çŸ«æ­£
    result = fix_english_capitalization(&result);
    
    // 3. æ ‡ç‚¹ç¬¦å·çŸ«æ­£
    result = result.replace(",,", ",");
    result = result.replace("..", ".");
    result = result.replace("??", "?");
    result = result.replace("!!", "!");
    
    // 4. å¥å­é—´è·çŸ«æ­£
    result = Regex::new(r"\.([A-Z])").unwrap().replace_all(&result, ". $1").to_string();
    result = Regex::new(r"\?([A-Z])").unwrap().replace_all(&result, "? $1").to_string();
    result = Regex::new(r"!([A-Z])").unwrap().replace_all(&result, "! $1").to_string();
    
    // 5. æ·»åŠ é€‚å½“çš„å¥å·
    result = result.trim().to_string();
    if result.len() > 10 && !result.ends_with(['.', '?', '!', 'ã€‚', 'ï¼Ÿ', 'ï¼']) {
        result.push('.');
    }
    
    result
}

// è‹±æ–‡å¤§å°å†™çŸ«æ­£
fn fix_english_capitalization(text: &str) -> String {
    let mut result = String::new();
    let mut capitalize_next = true;
    
    for c in text.chars() {
        if c.is_alphabetic() {
            if capitalize_next {
                result.push(c.to_uppercase().next().unwrap_or(c));
                capitalize_next = false;
            } else {
                result.push(c.to_lowercase().next().unwrap_or(c));
            }
        } else {
            result.push(c);
            if c == '.' || c == '?' || c == '!' || c == 'ã€‚' || c == 'ï¼Ÿ' || c == 'ï¼' {
                capitalize_next = true;
            }
        }
    }
    
    result
}

// è‡ªåŠ¨æ£€æµ‹è¯­è¨€çš„åå¤„ç†
fn post_process_auto(text: &str) -> String {
    // ç®€å•æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    let has_chinese = text.chars().any(|c| {
        let code = c as u32;
        (0x4E00..=0x9FFF).contains(&code) // ä¸­æ–‡å­—ç¬¦èŒƒå›´
    });
    
    if has_chinese {
        post_process_chinese(text)
    } else {
        post_process_english(text)
    }
}

// =============================================================================
// éŸ³é¢‘å¤„ç†ä¼˜åŒ–å‡½æ•°
// =============================================================================

// é«˜è´¨é‡é‡é‡‡æ ·å‡½æ•°ï¼Œä½¿ç”¨rubatoåº“è¿›è¡Œä¸“ä¸šçº§é‡é‡‡æ ·
fn high_quality_resample(samples: &[f32], from_rate: u32, to_rate: u32) -> Result<Vec<f32>, String> {
    if from_rate == to_rate {
        return Ok(samples.to_vec());
    }
    
    use rubato::{Resampler, SincFixedIn, SincInterpolationType::*};
    
    // è®¡ç®—å‚æ•°
    let chunk_size = 1024; // å¤„ç†å—å¤§å°
    let params = rubato::SincInterpolationParameters {
        sinc_len: 256,        // æ›´é•¿çš„sincé•¿åº¦æé«˜è´¨é‡
        f_cutoff: 0.95,       // æˆªæ­¢é¢‘ç‡
        interpolation: Linear, // çº¿æ€§æ’å€¼
        oversampling_factor: 256, // è¿‡é‡‡æ ·å› å­
        window: rubato::WindowFunction::BlackmanHarris2, // é«˜è´¨é‡çª—å‡½æ•°
    };
    
    let mut resampler = SincFixedIn::<f32>::new(
        to_rate as f64 / from_rate as f64,
        2.0, // æœ€å¤§å€ç‡å˜åŒ–
        params,
        chunk_size,
        1, // å•å£°é“
    ).map_err(|e| format!("åˆ›å»ºé‡é‡‡æ ·å™¨å¤±è´¥: {}", e))?;
    
    let mut input_data = vec![samples.to_vec()];
    let mut output = Vec::new();
    
    // åˆ†å—å¤„ç†
    let mut pos = 0;
    while pos < samples.len() {
        let end = (pos + chunk_size).min(samples.len());
        let chunk = &samples[pos..end];
        
        // å¦‚æœå—å¤ªå°ï¼Œå¡«å……é›¶
        let mut padded_chunk = chunk.to_vec();
        while padded_chunk.len() < chunk_size {
            padded_chunk.push(0.0);
        }
        input_data[0] = padded_chunk;
        
        match resampler.process(&input_data, None) {
            Ok(out) => {
                output.extend_from_slice(&out[0]);
            }
            Err(e) => return Err(format!("é‡é‡‡æ ·å¤„ç†å¤±è´¥: {}", e)),
        }
        
        pos += chunk_size;
    }
    
    Ok(output)
}

// å¤‡ç”¨ç®€å•é‡é‡‡æ ·æ–¹æ¡ˆ
fn fallback_resample(samples: &[f32], from_rate: u32, to_rate: u32) -> Vec<f32> {
    if from_rate == to_rate {
        return samples.to_vec();
    }
    
    let ratio = from_rate as f64 / to_rate as f64;
    let new_length = (samples.len() as f64 / ratio) as usize;
    let mut resampled = Vec::with_capacity(new_length);
    
    for i in 0..new_length {
        let source_index = (i as f64 * ratio) as usize;
        if source_index < samples.len() {
            resampled.push(samples[source_index]);
        }
    }
    
    resampled
}

// éŸ³é¢‘å¢å¼ºå¤„ç†
fn audio_enhancement(samples: &mut [f32]) {
    println!("å¼€å§‹éŸ³é¢‘å¢å¼ºå¤„ç†...");
    
    // 1. é¢„åŠ é‡æ»¤æ³¢ (æå‡é«˜é¢‘åˆ†é‡)
    apply_preemphasis(samples, 0.97);
    
    // 2. å½’ä¸€åŒ–å¤„ç†
    normalize_audio(samples);
    
    // 3. ç®€å•é™å™ª (åŸºäºèƒ½é‡é˜ˆå€¼)
    simple_noise_reduction(samples);
    
    println!("éŸ³é¢‘å¢å¼ºå¤„ç†å®Œæˆ");
}

// é¢„åŠ é‡æ»¤æ³¢å™¨ - æå‡é«˜é¢‘åˆ†é‡ï¼Œæ”¹å–„è¯­éŸ³è¯†åˆ«æ•ˆæœ
fn apply_preemphasis(samples: &mut [f32], coeff: f32) {
    if samples.is_empty() {
        return;
    }
    
    // ä»åå¾€å‰å¤„ç†ï¼Œé¿å…è¦†ç›–é—®é¢˜
    for i in (1..samples.len()).rev() {
        samples[i] -= coeff * samples[i-1];
    }
}

// éŸ³é¢‘å½’ä¸€åŒ– - é˜²æ­¢æº¢å‡ºå¹¶ä¼˜åŒ–åŠ¨æ€èŒƒå›´
fn normalize_audio(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    // æ‰¾åˆ°æœ€å¤§ç»å¯¹å€¼
    let max_abs = samples.iter()
        .map(|&x| x.abs())
        .fold(0.0f32, f32::max);
    
    if max_abs > 0.0 && max_abs != 1.0 {
        // å½’ä¸€åŒ–åˆ° [-0.95, 0.95] é¿å…å‰Šæ³¢
        let scale = 0.95 / max_abs;
        for sample in samples.iter_mut() {
            *sample *= scale;
        }
    }
}

// ç®€å•é™å™ª - åŸºäºèƒ½é‡é˜ˆå€¼çš„å™ªå£°æŠ‘åˆ¶
fn simple_noise_reduction(samples: &mut [f32]) {
    if samples.len() < 1024 {
        return;
    }
    
    // è®¡ç®—å™ªå£°é˜ˆå€¼ (å‰100msçš„å¹³å‡èƒ½é‡)
    let noise_samples = samples.len().min(1600); // 100ms at 16kHz
    let noise_energy: f32 = samples[..noise_samples]
        .iter()
        .map(|&x| x * x)
        .sum::<f32>() / noise_samples as f32;
    
    let threshold = noise_energy * 1.5; // å™ªå£°é˜ˆå€¼
    
    // åŸºäºé˜ˆå€¼çš„è½¯é™å™ª
    for sample in samples.iter_mut() {
        let energy = *sample * *sample;
        if energy < threshold {
            *sample *= 0.3; // é™ä½å™ªå£°åŒºåŸŸçš„å¢ç›Š
        }
    }
}

// VAD (è¯­éŸ³æ´»åŠ¨æ£€æµ‹) - æ£€æµ‹éŸ³é¢‘ä¸­çš„è¯­éŸ³æ®µ
pub fn detect_speech_segments(samples: &[f32]) -> Result<Vec<(usize, usize)>, String> {
    println!("å¼€å§‹VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹...");
    
    // åˆå§‹åŒ–WebRTC VAD
    let mut vad = Vad::new();
    vad.set_mode(webrtc_vad::VadMode::Quality);
    
    let frame_size = 480; // 30ms at 16kHz
    let mut speech_segments = Vec::new();
    let mut current_start: Option<usize> = None;
    let min_speech_duration = 320; // æœ€å°è¯­éŸ³æ®µé•¿åº¦ (20ms)
    let min_silence_duration = 160; // æœ€å°é™é»˜é•¿åº¦ (10ms)
    
    let mut silence_counter = 0;
    
    for (i, chunk) in samples.chunks(frame_size).enumerate() {
        if chunk.len() != frame_size {
            break; // è·³è¿‡ä¸å®Œæ•´çš„å—
        }
        
        // è½¬æ¢ä¸ºi16æ ¼å¼ä¾›VADä½¿ç”¨
        let chunk_i16: Vec<i16> = chunk.iter()
            .map(|&x| (x * 32767.0).clamp(-32767.0, 32767.0) as i16)
            .collect();
        
        match vad.is_voice_segment(&chunk_i16) {
            Ok(is_speech) => {
                let frame_start = i * frame_size;
                
                if is_speech {
                    // æ£€æµ‹åˆ°è¯­éŸ³
                    silence_counter = 0;
                    if current_start.is_none() {
                        current_start = Some(frame_start);
                    }
                } else {
                    // é™é»˜åŒºåŸŸ
                    silence_counter += 1;
                    
                    // å¦‚æœé™é»˜æ—¶é—´è¶³å¤Ÿé•¿ï¼Œç»“æŸå½“å‰è¯­éŸ³æ®µ
                    if let Some(start) = current_start {
                        if silence_counter * frame_size >= min_silence_duration {
                            let end = frame_start;
                            if end - start >= min_speech_duration {
                                speech_segments.push((start, end));
                            }
                            current_start = None;
                            silence_counter = 0;
                        }
                    }
                }
            }
            Err(e) => {
                println!("VADå¤„ç†é”™è¯¯: {:?}", e);
                // å‘ç”Ÿé”™è¯¯æ—¶è·³è¿‡è¿™ä¸€å¸§
                continue;
            }
        }
    }
    
    // å¤„ç†æœ€åä¸€ä¸ªè¯­éŸ³æ®µ
    if let Some(start) = current_start {
        let end = samples.len();
        if end - start >= min_speech_duration {
            speech_segments.push((start, end));
        }
    }
    
    // åˆå¹¶ç›¸é‚»çš„è¯­éŸ³æ®µ
    let merged_segments = merge_adjacent_segments(speech_segments, 800); // åˆå¹¶é—´éš”å°äº50msçš„æ®µ
    
    println!("VADæ£€æµ‹å®Œæˆï¼Œå‘ç° {} ä¸ªè¯­éŸ³æ®µ", merged_segments.len());
    Ok(merged_segments)
}

// åˆå¹¶ç›¸é‚»çš„è¯­éŸ³æ®µ
fn merge_adjacent_segments(segments: Vec<(usize, usize)>, max_gap: usize) -> Vec<(usize, usize)> {
    if segments.is_empty() {
        return segments;
    }
    
    let mut merged = Vec::new();
    let mut current_start = segments[0].0;
    let mut current_end = segments[0].1;
    
    for &(start, end) in &segments[1..] {
        if start - current_end <= max_gap {
            // åˆå¹¶ç›¸é‚»æ®µ
            current_end = end;
        } else {
            // ä¿å­˜å½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
            merged.push((current_start, current_end));
            current_start = start;
            current_end = end;
        }
    }
    
    // æ·»åŠ æœ€åä¸€æ®µ
    merged.push((current_start, current_end));
    
    merged
}

// åŸºäºVADçš„æ™ºèƒ½éŸ³é¢‘é¢„å¤„ç†
fn preprocess_audio_with_vad(samples: Vec<f32>) -> Result<Vec<f32>, String> {
    let mut processed_samples = samples;
    
    // 1. éŸ³é¢‘å¢å¼º
    audio_enhancement(&mut processed_samples);
    
    // 2. VADæ£€æµ‹è¯­éŸ³æ®µ
    let speech_segments = detect_speech_segments(&processed_samples)?;
    
    if speech_segments.is_empty() {
        println!("è­¦å‘Š: æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œè¿”å›åŸå§‹éŸ³é¢‘");
        return Ok(processed_samples);
    }
    
    // 3. æå–å¹¶è¿æ¥è¯­éŸ³æ®µ
    let mut vad_processed = Vec::new();
    let mut total_speech_duration = 0;
    
    for (start, end) in speech_segments {
        let segment = &processed_samples[start..end];
        vad_processed.extend_from_slice(segment);
        total_speech_duration += end - start;
    }
    
    let original_duration = processed_samples.len() as f32 / 16000.0;
    let speech_duration = total_speech_duration as f32 / 16000.0;
    
    println!("VADå¤„ç†å®Œæˆ:");
    println!("  åŸå§‹éŸ³é¢‘é•¿åº¦: {:.1}ç§’", original_duration);
    println!("  è¯­éŸ³å†…å®¹é•¿åº¦: {:.1}ç§’", speech_duration);
    println!("  è¯­éŸ³å æ¯”: {:.1}%", (speech_duration / original_duration) * 100.0);
    
    Ok(vad_processed)
}

// =============================================================================
// å®Œæ•´éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿
// =============================================================================

#[derive(Debug, Clone)]
pub struct AudioProcessingConfig {
    pub enable_preemphasis: bool,
    pub preemphasis_coeff: f32,
    pub enable_normalization: bool,
    pub enable_noise_reduction: bool,
    pub noise_threshold_multiplier: f32,
    pub enable_spectral_enhancement: bool,
    pub enable_vad: bool,
    pub vad_min_speech_duration_ms: u32,
    pub vad_min_silence_duration_ms: u32,
    pub enable_dynamic_range_compression: bool,
}

impl Default for AudioProcessingConfig {
    fn default() -> Self {
        Self {
            enable_preemphasis: true,
            preemphasis_coeff: 0.97,
            enable_normalization: true,
            enable_noise_reduction: true,
            noise_threshold_multiplier: 1.5,
            enable_spectral_enhancement: true,
            enable_vad: true,
            vad_min_speech_duration_ms: 20,
            vad_min_silence_duration_ms: 10,
            enable_dynamic_range_compression: true,
        }
    }
}

// å®Œæ•´çš„éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿
fn advanced_audio_preprocessing_pipeline(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let mut processed = samples;
    let total_steps = 7;
    let mut current_step = 0;
    
    println!("å¼€å§‹å®Œæ•´éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿ ({} ä¸ªå¤„ç†æ­¥éª¤)...", total_steps);
    
    // æ­¥éª¤1: é¢„åŠ é‡æ»¤æ³¢
    if config.enable_preemphasis {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨é¢„åŠ é‡æ»¤æ³¢ (ç³»æ•°: {})", current_step, total_steps, config.preemphasis_coeff);
        apply_preemphasis(&mut processed, config.preemphasis_coeff);
    }
    
    // æ­¥éª¤2: éŸ³é¢‘å½’ä¸€åŒ–
    if config.enable_normalization {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨éŸ³é¢‘å½’ä¸€åŒ–", current_step, total_steps);
        normalize_audio(&mut processed);
    }
    
    // æ­¥éª¤3: é¢‘è°±å¢å¼º
    if config.enable_spectral_enhancement {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨é¢‘è°±å¢å¼º", current_step, total_steps);
        match spectral_enhancement(&mut processed) {
            Ok(_) => println!("é¢‘è°±å¢å¼ºå®Œæˆ"),
            Err(e) => println!("é¢‘è°±å¢å¼ºå¤±è´¥: {}, è·³è¿‡", e),
        }
    }
    
    // æ­¥éª¤4: åŠ¨æ€èŒƒå›´å‹ç¼©
    if config.enable_dynamic_range_compression {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨åŠ¨æ€èŒƒå›´å‹ç¼©", current_step, total_steps);
        apply_dynamic_range_compression(&mut processed);
    }
    
    // æ­¥éª¤5: å™ªå£°æŠ‘åˆ¶
    if config.enable_noise_reduction {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨è‡ªé€‚åº”å™ªå£°æŠ‘åˆ¶", current_step, total_steps);
        adaptive_noise_reduction(&mut processed, config.noise_threshold_multiplier);
    }
    
    // æ­¥éª¤6: VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹
    if config.enable_vad {
        current_step += 1;
        println!("[{}/{}] åº”ç”¨VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹", current_step, total_steps);
        processed = match apply_vad_processing(processed.clone(), config) {
            Ok(vad_result) => vad_result,
            Err(e) => {
                println!("VADå¤„ç†å¤±è´¥: {}, ä½¿ç”¨åŸå§‹éŸ³é¢‘", e);
                processed
            }
        };
    }
    
    // æ­¥éª¤7: æœ€ç»ˆè´¨é‡æ£€æŸ¥å’Œä¼˜åŒ–
    current_step += 1;
    println!("[{}/{}] æœ€ç»ˆè´¨é‡æ£€æŸ¥å’Œä¼˜åŒ–", current_step, total_steps);
    final_quality_optimization(&mut processed);
    
    let duration = processed.len() as f32 / 16000.0;
    println!("éŸ³é¢‘é¢„å¤„ç†æµæ°´çº¿å®Œæˆ! å¤„ç†åéŸ³é¢‘é•¿åº¦: {:.2}ç§’", duration);
    
    Ok(processed)
}

// é¢‘è°±å¢å¼º - ä½¿ç”¨FFTè¿›è¡Œé¢‘åŸŸå¤„ç†
fn spectral_enhancement(samples: &mut [f32]) -> Result<(), String> {
    if samples.len() < 1024 {
        return Err("éŸ³é¢‘å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œé¢‘è°±å¢å¼º".to_string());
    }
    
    let mut planner = FftPlanner::<f32>::new();
    let fft_size = 1024;
    let overlap = fft_size / 2;
    let fft = planner.plan_fft_forward(fft_size);
    let ifft = planner.plan_fft_inverse(fft_size);
    
    // çª—å‡½æ•° (æ±‰å®çª—)
    let window: Vec<f32> = (0..fft_size)
        .map(|i| 0.5 * (1.0 - (2.0 * std::f32::consts::PI * i as f32 / (fft_size - 1) as f32).cos()))
        .collect();
    
    let mut enhanced = vec![0.0f32; samples.len()];
    let mut pos = 0;
    
    while pos + fft_size <= samples.len() {
        // åº”ç”¨çª—å‡½æ•°å¹¶è½¬æ¢ä¸ºå¤æ•°
        let mut buffer: Vec<Complex32> = samples[pos..pos + fft_size]
            .iter()
            .zip(window.iter())
            .map(|(&s, &w)| Complex32::new(s * w, 0.0))
            .collect();
        
        // å‰å‘FFT
        fft.process(&mut buffer);
        
        // é¢‘åŸŸå¢å¼º
        for i in 0..buffer.len() {
            let magnitude = buffer[i].norm();
            let phase = buffer[i].arg();
            
            // å¢å¼ºè¯­éŸ³é¢‘æ®µ (300Hz - 3400Hz)
            let freq = i as f32 * 16000.0 / fft_size as f32;
            let enhancement_factor = if freq >= 300.0 && freq <= 3400.0 {
                1.2 // å¢å¼ºè¯­éŸ³é¢‘æ®µ
            } else if freq < 100.0 || freq > 8000.0 {
                0.5 // æŠ‘åˆ¶ä½é¢‘å™ªéŸ³å’Œé«˜é¢‘å™ªéŸ³
            } else {
                1.0
            };
            
            let enhanced_magnitude = magnitude * enhancement_factor;
            buffer[i] = Complex32::from_polar(enhanced_magnitude, phase);
        }
        
        // é€†å‘FFT
        ifft.process(&mut buffer);
        
        // é‡å ç›¸åŠ 
        for (i, &sample) in buffer.iter().enumerate() {
            let real_part = sample.re / fft_size as f32; // å½’ä¸€åŒ–
            if pos + i < enhanced.len() {
                enhanced[pos + i] += real_part * window[i];
            }
        }
        
        pos += overlap;
    }
    
    // å¤åˆ¶å¢å¼ºç»“æœ
    samples.copy_from_slice(&enhanced[..samples.len()]);
    
    Ok(())
}

// åŠ¨æ€èŒƒå›´å‹ç¼© - å‡å°‘éŸ³é‡å·®å¼‚è¿‡å¤§çš„é—®é¢˜
fn apply_dynamic_range_compression(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    let threshold = 0.7; // å‹ç¼©é˜ˆå€¼
    let ratio = 4.0; // å‹ç¼©æ¯”
    let attack_time = 0.003; // 3msæ”»å‡»æ—¶é—´
    let release_time = 0.1; // 100msé‡Šæ”¾æ—¶é—´
    
    let sample_rate = 16000.0;
    let attack_coeff = (-1.0f32 / (attack_time * sample_rate)).exp();
    let release_coeff = (-1.0f32 / (release_time * sample_rate)).exp();
    
    let mut envelope = 0.0f32;
    let mut gain_reduction = 0.0f32;
    
    for sample in samples.iter_mut() {
        let input_level = sample.abs();
        
        // åŒ…ç»œè·Ÿè¸ª
        let target_envelope = if input_level > envelope {
            envelope + (input_level - envelope) * (1.0 - attack_coeff)
        } else {
            envelope + (input_level - envelope) * (1.0 - release_coeff)
        };
        envelope = target_envelope;
        
        // è®¡ç®—å¢ç›Šå‡å°‘
        let target_gain_reduction = if envelope > threshold {
            let over_threshold = envelope - threshold;
            over_threshold * (1.0 - 1.0 / ratio)
        } else {
            0.0
        };
        
        // å¹³æ»‘å¢ç›Šå‡å°‘
        gain_reduction = if target_gain_reduction > gain_reduction {
            gain_reduction + (target_gain_reduction - gain_reduction) * (1.0 - attack_coeff)
        } else {
            gain_reduction + (target_gain_reduction - gain_reduction) * (1.0 - release_coeff)
        };
        
        // åº”ç”¨å‹ç¼©
        let compressed_level = envelope - gain_reduction;
        let gain = if envelope > 0.0 {
            compressed_level / envelope
        } else {
            1.0
        };
        
        *sample *= gain;
    }
}

// è‡ªé€‚åº”å™ªå£°æŠ‘åˆ¶ - æ›´æ™ºèƒ½çš„å™ªå£°æ£€æµ‹å’ŒæŠ‘åˆ¶
fn adaptive_noise_reduction(samples: &mut [f32], threshold_multiplier: f32) {
    if samples.len() < 3200 { // è‡³å°‘200ms
        return;
    }
    
    let frame_size = 320; // 20ms frames
    let num_frames = samples.len() / frame_size;
    
    // è®¡ç®—æ¯å¸§çš„èƒ½é‡
    let frame_energies: Vec<f32> = (0..num_frames)
        .map(|i| {
            let start = i * frame_size;
            let end = (start + frame_size).min(samples.len());
            samples[start..end]
                .iter()
                .map(|&x| x * x)
                .sum::<f32>() / (end - start) as f32
        })
        .collect();
    
    // ä¼°è®¡å™ªå£°èƒ½é‡ (ä½¿ç”¨æœ€å°å€¼ç»Ÿè®¡)
    let mut sorted_energies = frame_energies.clone();
    sorted_energies.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let noise_energy = sorted_energies[num_frames / 10]; // ä½¿ç”¨æœ€ä½10%çš„èƒ½é‡ä½œä¸ºå™ªå£°ä¼°è®¡
    
    let adaptive_threshold = noise_energy * threshold_multiplier;
    
    // åº”ç”¨è‡ªé€‚åº”å™ªå£°æŠ‘åˆ¶
    for (i, frame_energy) in frame_energies.iter().enumerate() {
        let start = i * frame_size;
        let end = (start + frame_size).min(samples.len());
        
        if *frame_energy < adaptive_threshold {
            // å™ªå£°å¸§ - åº”ç”¨å¼ºæŠ‘åˆ¶
            let suppression_factor = 0.1;
            for sample in &mut samples[start..end] {
                *sample *= suppression_factor;
            }
        } else if *frame_energy < adaptive_threshold * 2.0 {
            // è¿‡æ¸¡åŒºåŸŸ - åº”ç”¨æ¸å˜æŠ‘åˆ¶
            let suppression_factor = 0.3 + 0.7 * ((*frame_energy - adaptive_threshold) / adaptive_threshold);
            for sample in &mut samples[start..end] {
                *sample *= suppression_factor;
            }
        }
        // è¯­éŸ³å¸§ä¸å¤„ç†
    }
}

// åº”ç”¨VADå¤„ç†
fn apply_vad_processing(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let speech_segments = detect_speech_segments_advanced(&samples, config)?;
    
    if speech_segments.is_empty() {
        return Err("æœªæ£€æµ‹åˆ°è¯­éŸ³æ®µ".to_string());
    }
    
    // æå–è¯­éŸ³æ®µ
    let mut vad_processed = Vec::new();
    for (start, end) in speech_segments {
        vad_processed.extend_from_slice(&samples[start..end]);
    }
    
    Ok(vad_processed)
}

// é«˜çº§VADæ£€æµ‹
fn detect_speech_segments_advanced(
    samples: &[f32],
    config: &AudioProcessingConfig,
) -> Result<Vec<(usize, usize)>, String> {
    let mut vad = Vad::new();
    vad.set_mode(webrtc_vad::VadMode::Quality);
    
    let frame_size = 480; // 30ms at 16kHz
    let min_speech_frames = (config.vad_min_speech_duration_ms * 16) as usize / frame_size;
    let min_silence_frames = (config.vad_min_silence_duration_ms * 16) as usize / frame_size;
    
    let mut speech_segments = Vec::new();
    let mut current_start: Option<usize> = None;
    let mut silence_counter = 0;
    let mut speech_counter = 0;
    
    for (i, chunk) in samples.chunks(frame_size).enumerate() {
        if chunk.len() != frame_size {
            break;
        }
        
        let chunk_i16: Vec<i16> = chunk.iter()
            .map(|&x| (x * 32767.0).clamp(-32767.0, 32767.0) as i16)
            .collect();
        
        let is_speech = vad.is_voice_segment(&chunk_i16).unwrap_or(false);
        let frame_start = i * frame_size;
        
        if is_speech {
            speech_counter += 1;
            silence_counter = 0;
            
            if current_start.is_none() && speech_counter >= min_speech_frames {
                current_start = Some(frame_start - (speech_counter - 1) * frame_size);
            }
        } else {
            silence_counter += 1;
            speech_counter = 0;
            
            if let Some(start) = current_start {
                if silence_counter >= min_silence_frames {
                    let end = frame_start;
                    if end > start {
                        speech_segments.push((start, end));
                    }
                    current_start = None;
                    silence_counter = 0;
                }
            }
        }
    }
    
    // å¤„ç†æœ€åä¸€ä¸ªè¯­éŸ³æ®µ
    if let Some(start) = current_start {
        speech_segments.push((start, samples.len()));
    }
    
    // åˆå¹¶ç›¸é‚»æ®µ
    let merged = merge_adjacent_segments(speech_segments, 800);
    
    Ok(merged)
}

// æœ€ç»ˆè´¨é‡ä¼˜åŒ–
fn final_quality_optimization(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    // å»é™¤ç›´æµåˆ†é‡
    let dc_offset: f32 = samples.iter().sum::<f32>() / samples.len() as f32;
    if dc_offset.abs() > 0.001 {
        for sample in samples.iter_mut() {
            *sample -= dc_offset;
        }
    }
    
    // æœ€ç»ˆå½’ä¸€åŒ–
    let max_abs = samples.iter().map(|&x| x.abs()).fold(0.0f32, f32::max);
    if max_abs > 0.95 {
        let scale = 0.95 / max_abs;
        for sample in samples.iter_mut() {
            *sample *= scale;
        }
    }
    
    // è½¯é™å¹…é˜²æ­¢å‰Šæ³¢
    for sample in samples.iter_mut() {
        if sample.abs() > 0.95 {
            *sample = sample.signum() * (0.95 + 0.05 * (1.0 - (-10.0 * (sample.abs() - 0.95)).exp()));
        }
    }
}

// =============================================================================
// æ™ºèƒ½åˆ†æ®µå¤„ç†
// =============================================================================

#[derive(Debug, Clone)]
pub struct AudioSegment {
    pub data: Vec<f32>,
    pub start_time: f32,
    pub end_time: f32,
    pub confidence: f32,
    pub is_speech: bool,
}

#[derive(Debug, Clone)]
pub struct SegmentationConfig {
    pub max_segment_duration_sec: f32,
    pub min_segment_duration_sec: f32,
    pub overlap_duration_sec: f32,
    pub enable_content_based_segmentation: bool,
    pub energy_threshold_ratio: f32,
    pub silence_threshold_sec: f32,
}

impl Default for SegmentationConfig {
    fn default() -> Self {
        Self {
            max_segment_duration_sec: 30.0,  // 30ç§’æœ€å¤§æ®µé•¿åº¦
            min_segment_duration_sec: 1.0,   // 1ç§’æœ€å°æ®µé•¿åº¦
            overlap_duration_sec: 1.0,       // 1ç§’é‡å 
            enable_content_based_segmentation: true,
            energy_threshold_ratio: 0.3,     // èƒ½é‡é˜ˆå€¼æ¯”ä¾‹
            silence_threshold_sec: 0.5,      // é™é»˜é˜ˆå€¼
        }
    }
}

// æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ
fn intelligent_audio_segmentation(
    samples: &[f32],
    config: &SegmentationConfig,
) -> Result<Vec<AudioSegment>, String> {
    println!("å¼€å§‹æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ...");
    
    let sample_rate = 16000.0;
    let total_duration = samples.len() as f32 / sample_rate;
    
    if total_duration <= config.max_segment_duration_sec {
        // éŸ³é¢‘è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†æ®µ
        return Ok(vec![AudioSegment {
            data: samples.to_vec(),
            start_time: 0.0,
            end_time: total_duration,
            confidence: 1.0,
            is_speech: true,
        }]);
    }
    
    let segments = if config.enable_content_based_segmentation {
        // åŸºäºå†…å®¹çš„æ™ºèƒ½åˆ†æ®µ
        content_based_segmentation(samples, config)?
    } else {
        // ç®€å•çš„æ—¶é—´åˆ†æ®µ
        time_based_segmentation(samples, config)
    };
    
    // æ·»åŠ é‡å åŒºåŸŸ
    let segments = add_segment_overlaps(segments, config);
    
    println!("æ™ºèƒ½åˆ†æ®µå®Œæˆï¼Œç”Ÿæˆ {} ä¸ªéŸ³é¢‘æ®µ", segments.len());
    for (i, segment) in segments.iter().enumerate() {
        println!("  æ®µ{}: {:.1}s - {:.1}s ({:.1}s, ç½®ä¿¡åº¦: {:.2})", 
                i + 1, segment.start_time, segment.end_time, 
                segment.end_time - segment.start_time, segment.confidence);
    }
    
    Ok(segments)
}

// åŸºäºå†…å®¹çš„æ™ºèƒ½åˆ†æ®µ
fn content_based_segmentation(
    samples: &[f32],
    config: &SegmentationConfig,
) -> Result<Vec<AudioSegment>, String> {
    let sample_rate = 16000.0;
    let frame_size = (sample_rate * 0.02) as usize; // 20ms frames
    let hop_size = frame_size / 2; // 50% overlap
    
    // è®¡ç®—éŸ³é¢‘ç‰¹å¾
    let features = extract_audio_features(samples, frame_size, hop_size);
    
    // æ£€æµ‹åˆ†æ®µç‚¹
    let segment_points = detect_segment_boundaries(&features, config);
    
    // ç”ŸæˆéŸ³é¢‘æ®µ
    let mut segments = Vec::new();
    let mut current_start = 0;
    
    for &point in &segment_points {
        let segment_samples = point.min(samples.len());
        if segment_samples > current_start {
            let start_time = current_start as f32 / sample_rate;
            let end_time = segment_samples as f32 / sample_rate;
            let duration = end_time - start_time;
            
            if duration >= config.min_segment_duration_sec {
                let segment_data = samples[current_start..segment_samples].to_vec();
                let confidence = calculate_segment_confidence(&segment_data);
                
                segments.push(AudioSegment {
                    data: segment_data,
                    start_time,
                    end_time,
                    confidence,
                    is_speech: confidence > 0.5,
                });
                
                current_start = segment_samples;
            }
        }
    }
    
    // å¤„ç†æœ€åä¸€æ®µ
    if current_start < samples.len() {
        let start_time = current_start as f32 / sample_rate;
        let end_time = samples.len() as f32 / sample_rate;
        let duration = end_time - start_time;
        
        if duration >= config.min_segment_duration_sec {
            let segment_data = samples[current_start..].to_vec();
            let confidence = calculate_segment_confidence(&segment_data);
            
            segments.push(AudioSegment {
                data: segment_data,
                start_time,
                end_time,
                confidence,
                is_speech: confidence > 0.5,
            });
        }
    }
    
    Ok(segments)
}

// æå–éŸ³é¢‘ç‰¹å¾
fn extract_audio_features(samples: &[f32], frame_size: usize, hop_size: usize) -> Vec<AudioFeatures> {
    let mut features = Vec::new();
    let mut pos = 0;
    
    while pos + frame_size <= samples.len() {
        let frame = &samples[pos..pos + frame_size];
        let feature = AudioFeatures::from_frame(frame);
        features.push(feature);
        pos += hop_size;
    }
    
    features
}

#[derive(Debug, Clone)]
struct AudioFeatures {
    energy: f32,
    zero_crossing_rate: f32,
    spectral_centroid: f32,
    spectral_rolloff: f32,
}

impl AudioFeatures {
    fn from_frame(frame: &[f32]) -> Self {
        let energy = frame.iter().map(|&x| x * x).sum::<f32>() / frame.len() as f32;
        
        let zero_crossings = frame.windows(2)
            .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
            .count() as f32;
        let zero_crossing_rate = zero_crossings / (frame.len() - 1) as f32;
        
        // ç®€åŒ–çš„é¢‘è°±ç‰¹å¾è®¡ç®—
        let spectral_centroid = calculate_spectral_centroid(frame);
        let spectral_rolloff = calculate_spectral_rolloff(frame);
        
        Self {
            energy,
            zero_crossing_rate,
            spectral_centroid,
            spectral_rolloff,
        }
    }
}

fn calculate_spectral_centroid(frame: &[f32]) -> f32 {
    // ç®€åŒ–çš„é¢‘è°±è´¨å¿ƒè®¡ç®—
    let mut weighted_sum = 0.0;
    let mut magnitude_sum = 0.0;
    
    for (i, &sample) in frame.iter().enumerate() {
        let magnitude = sample.abs();
        weighted_sum += i as f32 * magnitude;
        magnitude_sum += magnitude;
    }
    
    if magnitude_sum > 0.0 {
        weighted_sum / magnitude_sum
    } else {
        0.0
    }
}

fn calculate_spectral_rolloff(frame: &[f32]) -> f32 {
    // ç®€åŒ–çš„é¢‘è°±æ»šé™è®¡ç®—
    let magnitudes: Vec<f32> = frame.iter().map(|&x| x.abs()).collect();
    let total_energy: f32 = magnitudes.iter().sum();
    let threshold = total_energy * 0.85; // 85%èƒ½é‡é˜ˆå€¼
    
    let mut cumulative_energy = 0.0;
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        cumulative_energy += magnitude;
        if cumulative_energy >= threshold {
            return i as f32 / magnitudes.len() as f32;
        }
    }
    
    1.0 // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æœ€å¤§å€¼
}

// æ£€æµ‹åˆ†æ®µè¾¹ç•Œ
fn detect_segment_boundaries(
    features: &[AudioFeatures], 
    config: &SegmentationConfig
) -> Vec<usize> {
    let mut boundaries = Vec::new();
    let sample_rate = 16000.0;
    let hop_size = (sample_rate * 0.01) as usize; // 10ms hop
    
    let max_segment_samples = (config.max_segment_duration_sec * sample_rate) as usize;
    let silence_threshold_frames = (config.silence_threshold_sec * 100.0) as usize; // 10ms frames
    
    // è®¡ç®—èƒ½é‡é˜ˆå€¼
    let energies: Vec<f32> = features.iter().map(|f| f.energy).collect();
    let mut sorted_energies = energies.clone();
    sorted_energies.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let energy_threshold = sorted_energies[sorted_energies.len() / 4] * config.energy_threshold_ratio;
    
    let mut last_boundary = 0;
    let mut silence_counter = 0;
    
    for (i, feature) in features.iter().enumerate() {
        let sample_pos = i * hop_size;
        
        // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ®µé•¿åº¦
        if sample_pos - last_boundary >= max_segment_samples {
            boundaries.push(sample_pos);
            last_boundary = sample_pos;
            silence_counter = 0;
            continue;
        }
        
        // æ£€æŸ¥é™é»˜åŒºåŸŸ
        if feature.energy < energy_threshold {
            silence_counter += 1;
            
            if silence_counter >= silence_threshold_frames && 
               sample_pos - last_boundary >= (config.min_segment_duration_sec * sample_rate) as usize {
                boundaries.push(sample_pos);
                last_boundary = sample_pos;
                silence_counter = 0;
            }
        } else {
            silence_counter = 0;
        }
    }
    
    boundaries
}

// ç®€å•çš„æ—¶é—´åˆ†æ®µ
fn time_based_segmentation(samples: &[f32], config: &SegmentationConfig) -> Vec<AudioSegment> {
    let sample_rate = 16000.0;
    let segment_samples = (config.max_segment_duration_sec * sample_rate) as usize;
    let mut segments = Vec::new();
    let mut pos = 0;
    
    while pos < samples.len() {
        let end_pos = (pos + segment_samples).min(samples.len());
        let start_time = pos as f32 / sample_rate;
        let end_time = end_pos as f32 / sample_rate;
        
        if end_time - start_time >= config.min_segment_duration_sec {
            let segment_data = samples[pos..end_pos].to_vec();
            let confidence = calculate_segment_confidence(&segment_data);
            
            segments.push(AudioSegment {
                data: segment_data,
                start_time,
                end_time,
                confidence,
                is_speech: confidence > 0.5,
            });
        }
        
        pos = end_pos;
    }
    
    segments
}

// æ·»åŠ é‡å åŒºåŸŸ
fn add_segment_overlaps(
    mut segments: Vec<AudioSegment>, 
    config: &SegmentationConfig
) -> Vec<AudioSegment> {
    if segments.len() <= 1 || config.overlap_duration_sec <= 0.0 {
        return segments;
    }
    
    let sample_rate = 16000.0;
    let overlap_samples = (config.overlap_duration_sec * sample_rate) as usize;
    
    for i in 0..segments.len() - 1 {
        let next_start = 0;
        
        // ä»ä¸‹ä¸€æ®µå¼€å§‹æ·»åŠ é‡å 
        if overlap_samples < segments[i + 1].data.len() {
            let end_index = overlap_samples.min(segments[i + 1].data.len());
            let overlap_data = segments[i + 1].data[next_start..end_index].to_vec();
            segments[i].data.extend_from_slice(&overlap_data);
            segments[i].end_time += config.overlap_duration_sec;
        }
    }
    
    segments
}

// è®¡ç®—æ®µç½®ä¿¡åº¦
fn calculate_segment_confidence(data: &[f32]) -> f32 {
    if data.is_empty() {
        return 0.0;
    }
    
    // åŸºäºèƒ½é‡å’Œé›¶äº¤å‰ç‡çš„ç®€å•ç½®ä¿¡åº¦è®¡ç®—
    let energy = data.iter().map(|&x| x * x).sum::<f32>() / data.len() as f32;
    let zero_crossings = data.windows(2)
        .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
        .count() as f32;
    let zero_crossing_rate = zero_crossings / (data.len() - 1) as f32;
    
    // è¯­éŸ³é€šå¸¸æœ‰é€‚ä¸­çš„èƒ½é‡å’Œé›¶äº¤å‰ç‡
    let energy_score = if energy > 0.001 && energy < 0.1 { 1.0 } else { 0.5 };
    let zcr_score = if zero_crossing_rate > 0.01 && zero_crossing_rate < 0.5 { 1.0 } else { 0.5 };
    
    (energy_score + zcr_score) / 2.0
}

// =============================================================================
// å¹¶è¡Œå¤„ç†ä¼˜åŒ–
// =============================================================================

#[derive(Debug, Clone)]
pub struct ParallelProcessingConfig {
    pub enable_parallel_segmentation: bool,
    pub enable_parallel_recognition: bool,
    pub max_concurrent_segments: usize,
    pub segment_processing_timeout_sec: u64,
}

impl Default for ParallelProcessingConfig {
    fn default() -> Self {
        Self {
            enable_parallel_segmentation: true,
            enable_parallel_recognition: true,
            max_concurrent_segments: 4, // å¹¶è¡Œå¤„ç†4ä¸ªæ®µ
            segment_processing_timeout_sec: 300, // 5åˆ†é’Ÿè¶…æ—¶
        }
    }
}

// ç®€åŒ–çš„åˆ†æ®µè¯†åˆ«ï¼ˆä¸»è¦ç”¨äºé•¿éŸ³é¢‘ï¼‰
fn segment_based_recognition(
    segments: Vec<AudioSegment>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    window: &WebviewWindow,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    println!("å¼€å§‹åˆ†æ®µè¯†åˆ«ï¼Œå…± {} ä¸ªæ®µ", segments.len());
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }
    
    if segments.is_empty() {
        return Ok(String::new());
    }
    
    let total_segments = segments.len();
    let mut results = Vec::new();
    
    for (i, segment) in segments.into_iter().enumerate() {
        // åœ¨æ¯ä¸ªæ®µå¤„ç†å‰æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
        if recognition_state.should_cancel() {
            return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
        }
        
        let progress = ((i + 1) as f32 / total_segments as f32) * 100.0;
        let _ = window.emit("recognition_progress", RecognitionProgress {
            stage: "segment_processing".to_string(),
            progress,
            message: format!("å¤„ç†æ®µ {}/{} ({:.1}s)", i + 1, total_segments, segment.end_time - segment.start_time),
        });
        
        println!("å¤„ç†æ®µ {} ({:.1}s - {:.1}s)", i + 1, segment.start_time, segment.end_time);
        
        match recognize_segment_blocking(&segment.data, &language, &mode, &initial_prompt, whisper_state) {
            Ok(text) => {
                results.push((segment.start_time, text));
                println!("æ®µ {} å®Œæˆ: {} å­—ç¬¦", i + 1, results.last().unwrap().1.len());
            }
            Err(e) => {
                println!("æ®µ {} è¯†åˆ«å¤±è´¥: {}", i + 1, e);
                results.push((segment.start_time, String::new()));
            }
        }
    }
    
    // æŒ‰æ—¶é—´é¡ºåºæ’åºå¹¶åˆå¹¶
    results.sort_by(|a, b| a.0.partial_cmp(&b.0).unwrap());
    let combined_text = results.into_iter()
        .map(|(_, text)| text)
        .filter(|text| !text.is_empty())
        .collect::<Vec<_>>()
        .join(" ");
    
    println!("åˆ†æ®µè¯†åˆ«å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {}", combined_text.len());
    Ok(combined_text)
}

// æ™ºèƒ½åˆ†æ®µç­–ç•¥å†³ç­–
fn should_use_segmentation(samples: &[f32], config: &SegmentationConfig) -> bool {
    let duration_sec = samples.len() as f32 / 16000.0;
    
    // éŸ³é¢‘é•¿åº¦è¶…è¿‡é…ç½®çš„æœ€å¤§æ®µé•¿åº¦æ—¶æ‰è¿›è¡Œåˆ†æ®µ
    duration_sec > config.max_segment_duration_sec
}

// é˜»å¡å¼æ®µè¯†åˆ«
fn recognize_segment_blocking(
    audio_data: &[f32],
    language: &str,
    mode: &str,
    initial_prompt: &Option<String>,
    whisper_state: &WhisperContextState,
) -> Result<String, String> {
    let ctx = whisper_state.ctx.lock().unwrap();
    
    // ä½¿ç”¨ä¼˜åŒ–çš„è¯†åˆ«å‚æ•°
    let mut params = unsafe { 
        whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH) 
    };
    
    // è®¡ç®—éŸ³é¢‘é•¿åº¦ç”¨äºå‚æ•°è°ƒæ•´
    let duration = audio_data.len() as f32 / 16000.0;
    
    // åŸºç¡€å‚æ•°è®¾ç½®
    params.temperature = 0.0;
    params.suppress_blank = true;
    params.token_timestamps = true;
    params.max_len = 1;
    
    // æ ¹æ®å¤„ç†æ¨¡å¼è°ƒæ•´å‚æ•°ï¼ˆä¸ºå¹¶è¡Œå¤„ç†ä¼˜åŒ–ï¼Œä½¿ç”¨è¾ƒä½çº¿ç¨‹æ•°ï¼‰
    match mode {
        "standard" => {
            // æ ‡å‡†è´¨é‡æ¨¡å¼ - å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦
            params.n_threads = 1; // å¹¶è¡Œå¤„ç†æ—¶ä½¿ç”¨æ›´å°‘çº¿ç¨‹
            params.beam_search.beam_size = 2;
            params.greedy.best_of = 2;
        },
        "high_precision" => {
            // é«˜ç²¾åº¦æ¨¡å¼ - ä¼˜å…ˆå‡†ç¡®åº¦
            params.n_threads = 2; // é«˜ç²¾åº¦æ¨¡å¼å¯ä»¥ç”¨ç¨å¤šçº¿ç¨‹
            params.beam_search.beam_size = 4;
            params.greedy.best_of = 4;
            params.temperature = 0.05; // è½»å¾®å¢åŠ éšæœºæ€§
        },
        _ => {
            // é»˜è®¤è®¾ç½® (ä¸æ ‡å‡†è´¨é‡ç›¸åŒ)
            params.n_threads = 1;
            params.beam_search.beam_size = 2;
            params.greedy.best_of = 2;
        }
    }
    
    // è¯­è¨€è®¾ç½®
    let lang_cstring = match language {
        "zh" => Some(std::ffi::CString::new("zh").unwrap()),
        "en" => Some(std::ffi::CString::new("en").unwrap()),
        _ => None,
    };
    
    if let Some(ref lang_str) = lang_cstring {
        params.language = lang_str.as_ptr();
    } else {
        params.language = std::ptr::null();
    }

    // è®¾ç½®åˆå§‹æç¤ºè¯
    let prompt_cstring = if let Some(ref prompt) = initial_prompt {
        if !prompt.trim().is_empty() {
            Some(std::ffi::CString::new(prompt.trim()).unwrap())
        } else {
            None
        }
    } else {
        None
    };

    if let Some(ref prompt_str) = prompt_cstring {
        params.initial_prompt = prompt_str.as_ptr();
    }
    
    // æ‰§è¡Œè¯†åˆ«
    let mut audio_copy = audio_data.to_vec();
    let result = unsafe {
        whisper_full(
            *ctx,
            params,
            audio_copy.as_mut_ptr(),
            audio_copy.len() as i32,
        )
    };
    
    if result != 0 {
        return Err("Whisperæ®µè¯†åˆ«å¤±è´¥".to_string());
    }
    
    // æå–æ–‡æœ¬
    let num_segments = unsafe { whisper_full_n_segments(*ctx) };
    let mut text = String::new();
    
    for i in 0..num_segments {
        let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
        if !segment_ptr.is_null() {
            let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
            text.push_str(c_str.to_str().unwrap_or(""));
        }
    }
    
    // åº”ç”¨æ–‡æœ¬åå¤„ç†
    let processed_text = post_process_text(&text, language);
    Ok(processed_text)
}



// ä¼˜åŒ–çš„å¹¶è¡ŒéŸ³é¢‘é¢„å¤„ç†
fn parallel_audio_preprocessing(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let chunk_size = samples.len() / rayon::current_num_threads().max(1);
    
    if chunk_size < 16000 { // å°‘äº1ç§’éŸ³é¢‘ä¸å€¼å¾—å¹¶è¡Œå¤„ç†
        return advanced_audio_preprocessing_pipeline(samples, config);
    }
    
    println!("ä½¿ç”¨å¹¶è¡ŒéŸ³é¢‘é¢„å¤„ç†ï¼Œå—å¤§å°: {} æ ·æœ¬", chunk_size);
    
    // å¹¶è¡Œå¤„ç†éŸ³é¢‘å—
    let processed_chunks: Result<Vec<Vec<f32>>, String> = samples
        .par_chunks(chunk_size)
        .enumerate()
        .map(|(i, chunk)| {
            println!("å¤„ç†éŸ³é¢‘å— {}", i + 1);
            let mut chunk_data = chunk.to_vec();
            
            // å¯¹æ¯ä¸ªå—åº”ç”¨éƒ¨åˆ†é¢„å¤„ç†
            if config.enable_preemphasis {
                apply_preemphasis(&mut chunk_data, config.preemphasis_coeff);
            }
            
            if config.enable_normalization {
                normalize_audio(&mut chunk_data);
            }
            
            if config.enable_noise_reduction {
                adaptive_noise_reduction(&mut chunk_data, config.noise_threshold_multiplier);
            }
            
            Ok(chunk_data)
        })
        .collect();
    
    let mut processed_samples: Vec<f32> = processed_chunks?
        .into_iter()
        .flatten()
        .collect();
    
    // å…¨å±€åå¤„ç†æ­¥éª¤ï¼ˆä¸èƒ½å¹¶è¡Œï¼‰
    if config.enable_dynamic_range_compression {
        apply_dynamic_range_compression(&mut processed_samples);
    }
    
    if config.enable_spectral_enhancement {
        if let Err(e) = spectral_enhancement(&mut processed_samples) {
            println!("å¹¶è¡Œé¢‘è°±å¢å¼ºå¤±è´¥: {}", e);
        }
    }
    
    final_quality_optimization(&mut processed_samples);
    
    println!("å¹¶è¡ŒéŸ³é¢‘é¢„å¤„ç†å®Œæˆ");
    Ok(processed_samples)
}

// =============================================================================
// é›†æˆçš„é«˜çº§è¯†åˆ«æµç¨‹
// =============================================================================

// é«˜çº§éŸ³é¢‘è¯†åˆ«æµç¨‹ï¼Œé›†æˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
fn advanced_recognition_pipeline(
    audio_data: Vec<f32>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    window: &WebviewWindow,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    println!("å¼€å§‹é«˜çº§éŸ³é¢‘è¯†åˆ«æµç¨‹...");
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }
    
    let total_duration = audio_data.len() as f32 / 16000.0;
    println!("éŸ³é¢‘æ€»é•¿åº¦: {:.1}ç§’", total_duration);
    
    // æ­¥éª¤1: å†³å®šæ˜¯å¦éœ€è¦åˆ†æ®µå¤„ç†
    let segmentation_config = SegmentationConfig::default();
    
    if should_use_segmentation(&audio_data, &segmentation_config) {
        println!("éŸ³é¢‘è¾ƒé•¿({:.1}s)ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†æ®µå¤„ç†", total_duration);
        
        // æ™ºèƒ½åˆ†æ®µ
        let segments = match intelligent_audio_segmentation(&audio_data, &segmentation_config) {
            Ok(segs) => segs,
            Err(e) => {
                println!("æ™ºèƒ½åˆ†æ®µå¤±è´¥: {}, ä½¿ç”¨æ•´ä½“è¯†åˆ«", e);
                return recognize_whole_audio(audio_data, language, mode.clone(), initial_prompt, whisper_state, recognition_state);
            }
        };
        
        // åˆ†æ®µè¯†åˆ«
        segment_based_recognition(segments, language, mode.clone(), initial_prompt.clone(), whisper_state, window, recognition_state)
    } else {
        println!("éŸ³é¢‘è¾ƒçŸ­({:.1}s)ï¼Œä½¿ç”¨æ•´ä½“è¯†åˆ«", total_duration);
        recognize_whole_audio(audio_data, language, mode, initial_prompt, whisper_state, recognition_state)
    }
}

// æ•´ä½“éŸ³é¢‘è¯†åˆ«ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
fn recognize_whole_audio(
    mut audio_data: Vec<f32>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    // æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆ
    if recognition_state.should_cancel() {
        return Err("è½¬å½•å·²è¢«ç”¨æˆ·å–æ¶ˆ".to_string());
    }
    
    let ctx = whisper_state.ctx.lock().unwrap();
    
    // ä½¿ç”¨æœ€ä¼˜å‚æ•°
    let mut params = unsafe { 
        whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH) 
    };
    
    // æ ¹æ®éŸ³é¢‘é•¿åº¦å’Œå¤„ç†æ¨¡å¼è°ƒæ•´å‚æ•°
    let duration = audio_data.len() as f32 / 16000.0;
    
    // åŸºç¡€å‚æ•°è®¾ç½®
    params.greedy.best_of = 5;
    params.temperature = 0.0;
    params.suppress_blank = true;
    params.token_timestamps = true;
    params.max_len = 1;
    
    // æ ¹æ®å¤„ç†æ¨¡å¼å’ŒéŸ³é¢‘é•¿åº¦è°ƒæ•´å‚æ•°
    match mode.as_str() {
        "standard" => {
            // æ ‡å‡†è´¨é‡æ¨¡å¼ - å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®åº¦
            if duration > 120.0 {
                params.n_threads = 4;
                params.beam_search.beam_size = 2;
            } else {
                params.n_threads = 6;
                params.beam_search.beam_size = 3;
            }
            params.greedy.best_of = 3;
            params.temperature = 0.0;
        },
        "high_precision" => {
            // é«˜ç²¾åº¦æ¨¡å¼ - ä¼˜å…ˆå‡†ç¡®åº¦
            if duration > 120.0 {
                params.n_threads = 8;
                params.beam_search.beam_size = 5;
            } else {
                params.n_threads = 8;
                params.beam_search.beam_size = 8;
            }
            params.greedy.best_of = 8;
            params.temperature = 0.1; // è½»å¾®å¢åŠ éšæœºæ€§ä»¥è·å¾—æ›´å¥½ç»“æœ
        },
        _ => {
            // é»˜è®¤è®¾ç½® (ä¸æ ‡å‡†è´¨é‡ç›¸åŒ)
            if duration > 120.0 {
                params.n_threads = 4;
                params.beam_search.beam_size = 2;
            } else {
                params.n_threads = 6;
                params.beam_search.beam_size = 3;
            }
        }
    }
    
    // è¯­è¨€è®¾ç½®
    let lang_cstring = match language.as_str() {
        "zh" => Some(std::ffi::CString::new("zh").unwrap()),
        "en" => Some(std::ffi::CString::new("en").unwrap()),
        _ => None,
    };
    
    if let Some(ref lang_str) = lang_cstring {
        params.language = lang_str.as_ptr();
    } else {
        params.language = std::ptr::null();
    }

    // è®¾ç½®åˆå§‹æç¤ºè¯
    let prompt_cstring = if let Some(ref prompt) = initial_prompt {
        if !prompt.trim().is_empty() {
            Some(std::ffi::CString::new(prompt.trim()).unwrap())
        } else {
            None
        }
    } else {
        None
    };

    if let Some(ref prompt_str) = prompt_cstring {
        params.initial_prompt = prompt_str.as_ptr();
    }
    
    println!("ä½¿ç”¨ä¼˜åŒ–å‚æ•°: beam_size={}, threads={}, duration={:.1}s", 
             params.beam_search.beam_size, params.n_threads, duration);
    
    // æ‰§è¡Œè¯†åˆ«
    let result = unsafe {
        whisper_full(
            *ctx,
            params,
            audio_data.as_mut_ptr(),
            audio_data.len() as i32,
        )
    };
    
    if result != 0 {
        return Err("Whisperæ•´ä½“è¯†åˆ«å¤±è´¥".to_string());
    }
    
    // æå–æ–‡æœ¬
    let num_segments = unsafe { whisper_full_n_segments(*ctx) };
    let mut full_text = String::new();
    
    for i in 0..num_segments {
        let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
        if !segment_ptr.is_null() {
            let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
            full_text.push_str(c_str.to_str().unwrap_or(""));
        }
    }
    
    // è·å–å¸¦æ—¶é—´æˆ³çš„æ®µä¿¡æ¯ç”¨äºè¯´è¯äººè¯†åˆ«
    let segments = extract_timestamped_segments(*ctx);
    
    // æ–‡æœ¬åå¤„ç†
    let processed_text = post_process_text(&full_text, &language);
    
    // å¦‚æœæœ‰å¤šä¸ªæ®µï¼Œå°è¯•è¿›è¡Œè¯´è¯äººè¯†åˆ«å’Œè§’è‰²åˆ†é…
    if segments.len() > 1 {
        match perform_speaker_diarization(&audio_data, &segments) {
            Ok(dialogue_text) => Ok(dialogue_text),
            Err(e) => {
                println!("è¯´è¯äººè¯†åˆ«å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬: {}", e);
                Ok(processed_text)
            }
        }
    } else {
        Ok(processed_text)
    }
}

// =============================================================================
// è¯´è¯äººè¯†åˆ«å’Œè§’è‰²åˆ†é…ç³»ç»Ÿ
// =============================================================================

#[derive(Debug, Clone)]
pub struct TimestampedSegment {
    pub start_time: f64,  // å¼€å§‹æ—¶é—´(ç§’)
    pub end_time: f64,    // ç»“æŸæ—¶é—´(ç§’)
    pub text: String,     // è¯†åˆ«çš„æ–‡æœ¬
    pub audio_start: usize, // éŸ³é¢‘å¼€å§‹ä½ç½®(æ ·æœ¬æ•°)
    pub audio_end: usize,   // éŸ³é¢‘ç»“æŸä½ç½®(æ ·æœ¬æ•°)
}

#[derive(Debug, Clone)]
pub struct VoiceCharacteristics {
    pub fundamental_freq: f32,    // åŸºé¢‘ (Hz)
    pub formant_frequencies: Vec<f32>, // å…±æŒ¯å³°é¢‘ç‡
    pub spectral_centroid: f32,   // é¢‘è°±è´¨å¿ƒ
    pub spectral_bandwidth: f32,  // é¢‘è°±å¸¦å®½
    pub zero_crossing_rate: f32,  // è¿‡é›¶ç‡
    pub mfcc_features: Vec<f32>,  // MFCCç‰¹å¾
    pub energy: f32,              // å¹³å‡èƒ½é‡
}

#[derive(Debug, Clone)]
pub struct Speaker {
    pub id: usize,
    pub characteristics: VoiceCharacteristics,
    pub segments: Vec<TimestampedSegment>,
    pub assigned_role: String,  // åˆ†é…çš„è§’è‰²åç§°
}

// ä»Whisperä¸Šä¸‹æ–‡æå–å¸¦æ—¶é—´æˆ³çš„æ®µä¿¡æ¯
fn extract_timestamped_segments(ctx: *mut whisper_context) -> Vec<TimestampedSegment> {
    let mut segments = Vec::new();
    
    unsafe {
        let num_segments = whisper_full_n_segments(ctx);
        
        for i in 0..num_segments {
            let start_time = whisper_full_get_segment_t0(ctx, i) as f64 / 100.0; // è½¬æ¢ä¸ºç§’
            let end_time = whisper_full_get_segment_t1(ctx, i) as f64 / 100.0;
            
            let segment_ptr = whisper_full_get_segment_text(ctx, i);
            let text = if !segment_ptr.is_null() {
                let c_str = CStr::from_ptr(segment_ptr as *const c_char);
                c_str.to_str().unwrap_or("").to_string()
            } else {
                String::new()
            };
            
            // è®¡ç®—éŸ³é¢‘ä½ç½® (16kHzé‡‡æ ·ç‡)
            let audio_start = (start_time * 16000.0) as usize;
            let audio_end = (end_time * 16000.0) as usize;
            
            segments.push(TimestampedSegment {
                start_time,
                end_time,
                text,
                audio_start,
                audio_end,
            });
        }
    }
    
    segments
}

// æ‰§è¡Œè¯´è¯äººè¯†åˆ«å’Œè§’è‰²åˆ†é…
fn perform_speaker_diarization(
    audio_data: &[f32],
    segments: &[TimestampedSegment],
) -> Result<String, String> {
    println!("å¼€å§‹è¯´è¯äººè¯†åˆ«ï¼Œå…± {} ä¸ªè¯­éŸ³æ®µ", segments.len());
    
    // 1. ä¸ºæ¯ä¸ªæ®µæå–éŸ³è‰²ç‰¹å¾
    let mut segment_features = Vec::new();
    for segment in segments {
        let audio_segment = extract_audio_segment(audio_data, segment)?;
        let characteristics = extract_voice_characteristics(&audio_segment)?;
        segment_features.push((segment.clone(), characteristics));
    }
    
    // 2. èšç±»åˆ†æï¼Œè¯†åˆ«ä¸åŒè¯´è¯äºº
    let speakers = cluster_speakers(segment_features)?;
    
    // 3. åˆ†é…è§’è‰²åç§°
    let speakers_with_roles = assign_speaker_roles(speakers);
    
    // 4. æ ¼å¼åŒ–å¯¹è¯è¾“å‡º
    let dialogue_text = format_dialogue_output(&speakers_with_roles);
    
    println!("è¯´è¯äººè¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {} ä¸ªè¯´è¯äºº", speakers_with_roles.len());
    Ok(dialogue_text)
}

// æå–éŸ³é¢‘æ®µ
fn extract_audio_segment(
    audio_data: &[f32],
    segment: &TimestampedSegment,
) -> Result<Vec<f32>, String> {
    let start = segment.audio_start.min(audio_data.len());
    let end = segment.audio_end.min(audio_data.len());
    
    if start >= end {
        return Err("æ— æ•ˆçš„éŸ³é¢‘æ®µæ—¶é—´èŒƒå›´".to_string());
    }
    
    Ok(audio_data[start..end].to_vec())
}

// æå–éŸ³è‰²ç‰¹å¾
fn extract_voice_characteristics(audio_segment: &[f32]) -> Result<VoiceCharacteristics, String> {
    if audio_segment.len() < 1600 { // è‡³å°‘100ms
        return Err("éŸ³é¢‘æ®µå¤ªçŸ­ï¼Œæ— æ³•æå–ç‰¹å¾".to_string());
    }
    
    // 1. åŸºé¢‘æå– (ç®€åŒ–çš„è‡ªç›¸å…³æ–¹æ³•)
    let fundamental_freq = estimate_fundamental_frequency(audio_segment);
    
    // 2. é¢‘è°±è´¨å¿ƒå’Œå¸¦å®½
    let (spectral_centroid, spectral_bandwidth) = calculate_spectral_features(audio_segment);
    
    // 3. è¿‡é›¶ç‡
    let zero_crossing_rate = calculate_zero_crossing_rate(audio_segment);
    
    // 4. å¹³å‡èƒ½é‡
    let energy = audio_segment.iter().map(|&x| x * x).sum::<f32>() / audio_segment.len() as f32;
    
    // 5. MFCCç‰¹å¾ (ç®€åŒ–ç‰ˆæœ¬)
    let mfcc_features = extract_mfcc_features(audio_segment)?;
    
    // 6. å…±æŒ¯å³°é¢‘ç‡ (ç®€åŒ–ä¼°è®¡)
    let formant_frequencies = estimate_formant_frequencies(audio_segment);
    
    Ok(VoiceCharacteristics {
        fundamental_freq,
        formant_frequencies,
        spectral_centroid,
        spectral_bandwidth,
        zero_crossing_rate,
        mfcc_features,
        energy,
    })
}

// ç®€åŒ–çš„è¾…åŠ©å‡½æ•°ï¼Œé›†æˆåˆ°ä¸»æ–‡ä»¶ä¸­
fn estimate_fundamental_frequency(audio: &[f32]) -> f32 {
    let sample_rate = 16000.0;
    let min_period = (sample_rate / 500.0) as usize;
    let max_period = (sample_rate / 50.0) as usize;
    
    let mut max_correlation = 0.0;
    let mut best_period = min_period;
    
    for period in min_period..=max_period.min(audio.len() / 2) {
        let mut correlation = 0.0;
        let mut count = 0;
        
        for i in 0..(audio.len() - period) {
            correlation += audio[i] * audio[i + period];
            count += 1;
        }
        
        if count > 0 {
            correlation /= count as f32;
            if correlation > max_correlation {
                max_correlation = correlation;
                best_period = period;
            }
        }
    }
    
    sample_rate / best_period as f32
}

fn calculate_spectral_features(audio: &[f32]) -> (f32, f32) {
    let mut magnitudes = Vec::new();
    let chunk_size = 512;
    
    for chunk in audio.chunks(chunk_size) {
        if chunk.len() == chunk_size {
            let magnitude_sum: f32 = chunk.iter().map(|&x| x.abs()).sum();
            magnitudes.push(magnitude_sum / chunk_size as f32);
        }
    }
    
    if magnitudes.is_empty() {
        return (0.0, 0.0);
    }
    
    let mut weighted_sum = 0.0;
    let mut total_magnitude = 0.0;
    
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        let frequency = i as f32 * 16000.0 / magnitudes.len() as f32;
        weighted_sum += frequency * magnitude;
        total_magnitude += magnitude;
    }
    
    let centroid = if total_magnitude > 0.0 {
        weighted_sum / total_magnitude
    } else { 0.0 };
    
    let mut variance = 0.0;
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        let frequency = i as f32 * 16000.0 / magnitudes.len() as f32;
        variance += (frequency - centroid).powi(2) * magnitude;
    }
    
    let bandwidth = if total_magnitude > 0.0 {
        (variance / total_magnitude).sqrt()
    } else { 0.0 };
    
    (centroid, bandwidth)
}

fn calculate_zero_crossing_rate(audio: &[f32]) -> f32 {
    if audio.len() < 2 { return 0.0; }
    
    let zero_crossings = audio.windows(2)
        .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
        .count();
    
    zero_crossings as f32 / (audio.len() - 1) as f32
}

fn extract_mfcc_features(_audio: &[f32]) -> Result<Vec<f32>, String> {
    // ç®€åŒ–ç‰ˆæœ¬ï¼Œè¿”å›å›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡
    Ok(vec![0.0; 13])
}

fn estimate_formant_frequencies(_audio: &[f32]) -> Vec<f32> {
    // ç®€åŒ–ç‰ˆæœ¬ï¼Œè¿”å›é»˜è®¤å…±æŒ¯å³°é¢‘ç‡
    vec![800.0, 1200.0, 2500.0]
}

// èšç±»åˆ†æï¼Œè¯†åˆ«ä¸åŒè¯´è¯äºº
fn cluster_speakers(
    segment_features: Vec<(TimestampedSegment, VoiceCharacteristics)>
) -> Result<Vec<Speaker>, String> {
    println!("å¼€å§‹è¯´è¯äººèšç±»åˆ†æ...");
    
    if segment_features.is_empty() {
        return Ok(Vec::new());
    }
    
    // ä½¿ç”¨ç®€åŒ–çš„åŸºé¢‘èšç±»
    let max_speakers = 4;
    let actual_speakers = estimate_speaker_count(&segment_features).min(max_speakers);
    
    println!("ä¼°è®¡è¯´è¯äººæ•°é‡: {}", actual_speakers);
    
    if actual_speakers == 1 {
        // åªæœ‰ä¸€ä¸ªè¯´è¯äºº
        let avg_characteristics = calculate_average_characteristics(
            &segment_features.iter().map(|(_, chars)| chars).collect::<Vec<_>>()
        );
        
        let segments = segment_features.into_iter().map(|(seg, _)| seg).collect();
        
        return Ok(vec![Speaker {
            id: 0,
            characteristics: avg_characteristics,
            segments,
            assigned_role: String::new(),
        }]);
    }
    
    // ç®€åŒ–çš„K-meansèšç±»åŸºäºåŸºé¢‘
    let mut speakers = Vec::new();
    let mut freq_sorted: Vec<(usize, f32)> = segment_features.iter()
        .enumerate()
        .map(|(i, (_, chars))| (i, chars.fundamental_freq))
        .filter(|(_, freq)| *freq > 50.0 && *freq < 500.0)
        .collect();
    
    freq_sorted.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
    
    // å°†é¢‘ç‡åˆ†ç»„
    let chunk_size = freq_sorted.len() / actual_speakers;
    for speaker_id in 0..actual_speakers {
        let start = speaker_id * chunk_size;
        let end = if speaker_id == actual_speakers - 1 {
            freq_sorted.len()
        } else {
            (speaker_id + 1) * chunk_size
        };
        
        if start < freq_sorted.len() {
            let indices: Vec<usize> = freq_sorted[start..end.min(freq_sorted.len())]
                .iter().map(|(i, _)| *i).collect();
            
            let characteristics_list: Vec<&VoiceCharacteristics> = indices.iter()
                .map(|&i| &segment_features[i].1).collect();
            
            let avg_characteristics = calculate_average_characteristics(&characteristics_list);
            let segments: Vec<TimestampedSegment> = indices.iter()
                .map(|&i| segment_features[i].0.clone()).collect();
            
            speakers.push(Speaker {
                id: speaker_id,
                characteristics: avg_characteristics,
                segments,
                assigned_role: String::new(),
            });
        }
    }
    
    // æŒ‰å‡ºç°æ—¶é—´æ’åº
    speakers.sort_by(|a, b| {
        let a_first_time = a.segments.iter().map(|s| s.start_time).fold(f64::INFINITY, f64::min);
        let b_first_time = b.segments.iter().map(|s| s.start_time).fold(f64::INFINITY, f64::min);
        a_first_time.partial_cmp(&b_first_time).unwrap()
    });
    
    Ok(speakers)
}

fn estimate_speaker_count(segment_features: &[(TimestampedSegment, VoiceCharacteristics)]) -> usize {
    if segment_features.len() <= 2 {
        return segment_features.len();
    }
    
    let mut fundamental_freqs: Vec<f32> = segment_features.iter()
        .map(|(_, chars)| chars.fundamental_freq)
        .filter(|&f| f > 50.0 && f < 500.0)
        .collect();
    
    if fundamental_freqs.is_empty() {
        return 1;
    }
    
    fundamental_freqs.sort_by(|a, b| a.partial_cmp(b).unwrap());
    
    // ç®€å•çš„é—´éš”åˆ†æ
    let mut gaps = Vec::new();
    for i in 1..fundamental_freqs.len() {
        gaps.push(fundamental_freqs[i] - fundamental_freqs[i-1]);
    }
    
    if gaps.is_empty() {
        return 1;
    }
    
    gaps.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let median_gap = gaps[gaps.len() / 2];
    
    let significant_gaps = gaps.iter()
        .filter(|&&gap| gap > median_gap * 2.0)
        .count();
    
    (significant_gaps + 1).min(4).max(1)
}

fn calculate_average_characteristics(
    characteristics_list: &[&VoiceCharacteristics]
) -> VoiceCharacteristics {
    if characteristics_list.is_empty() {
        return VoiceCharacteristics {
            fundamental_freq: 0.0,
            formant_frequencies: vec![0.0, 0.0, 0.0],
            spectral_centroid: 0.0,
            spectral_bandwidth: 0.0,
            zero_crossing_rate: 0.0,
            mfcc_features: vec![0.0; 13],
            energy: 0.0,
        };
    }
    
    let count = characteristics_list.len() as f32;
    let mut avg = VoiceCharacteristics {
        fundamental_freq: 0.0,
        formant_frequencies: vec![0.0; 3],
        spectral_centroid: 0.0,
        spectral_bandwidth: 0.0,
        zero_crossing_rate: 0.0,
        mfcc_features: vec![0.0; 13],
        energy: 0.0,
    };
    
    for chars in characteristics_list {
        avg.fundamental_freq += chars.fundamental_freq;
        avg.spectral_centroid += chars.spectral_centroid;
        avg.spectral_bandwidth += chars.spectral_bandwidth;
        avg.zero_crossing_rate += chars.zero_crossing_rate;
        avg.energy += chars.energy;
        
        for i in 0..chars.formant_frequencies.len().min(3) {
            avg.formant_frequencies[i] += chars.formant_frequencies[i];
        }
        
        for i in 0..chars.mfcc_features.len().min(13) {
            avg.mfcc_features[i] += chars.mfcc_features[i];
        }
    }
    
    avg.fundamental_freq /= count;
    avg.spectral_centroid /= count;
    avg.spectral_bandwidth /= count;
    avg.zero_crossing_rate /= count;
    avg.energy /= count;
    
    for i in 0..3 {
        avg.formant_frequencies[i] /= count;
    }
    
    for i in 0..13 {
        avg.mfcc_features[i] /= count;
    }
    
    avg
}

fn assign_speaker_roles(mut speakers: Vec<Speaker>) -> Vec<Speaker> {
    let role_names = ["è¯´è¯äººA", "è¯´è¯äººB", "è¯´è¯äººC", "è¯´è¯äººD"];
    
    for (i, speaker) in speakers.iter_mut().enumerate() {
        if i < role_names.len() {
            speaker.assigned_role = role_names[i].to_string();
        } else {
            speaker.assigned_role = format!("è¯´è¯äºº{}", i + 1);
        }
    }
    
    println!("è§’è‰²åˆ†é…å®Œæˆ:");
    for speaker in &speakers {
        println!("  {} - åŸºé¢‘: {:.1}Hz, æ®µæ•°: {}", 
                speaker.assigned_role, 
                speaker.characteristics.fundamental_freq,
                speaker.segments.len());
    }
    
    speakers
}

fn format_dialogue_output(speakers: &[Speaker]) -> String {
    println!("å¼€å§‹æ ¼å¼åŒ–å¯¹è¯è¾“å‡º...");
    
    // æ”¶é›†æ‰€æœ‰æ®µå¹¶æŒ‰æ—¶é—´æ’åº
    let mut all_segments = Vec::new();
    for speaker in speakers {
        for segment in &speaker.segments {
            all_segments.push((segment, &speaker.assigned_role));
        }
    }
    
    all_segments.sort_by(|a, b| a.0.start_time.partial_cmp(&b.0.start_time).unwrap());
    
    // æ ¼å¼åŒ–è¾“å‡º
    let mut formatted_text = String::new();
    let mut current_speaker = "";
    
    for (segment, role) in all_segments {
        let text = segment.text.trim();
        if text.is_empty() {
            continue;
        }
        
        if current_speaker != *role {
            if !formatted_text.is_empty() {
                formatted_text.push('\n');
            }
            formatted_text.push_str(&format!("{}ï¼š", role));
            current_speaker = role;
        } else {
            if !formatted_text.ends_with('ï¼š') {
                formatted_text.push(' ');
            }
        }
        
        formatted_text.push_str(text);
    }
    
    println!("å¯¹è¯æ ¼å¼åŒ–å®Œæˆï¼Œæ€»é•¿åº¦: {} å­—ç¬¦", formatted_text.len());
    formatted_text
}
