// These are required for bindgen generated code
#![allow(non_upper_case_globals)]
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]
#![allow(dead_code)]

// Include FFI bindings generated by build.rs
include!(concat!(env!("OUT_DIR"), "/bindings.rs"));

// 模块导入
mod storage;
mod storage_commands;
mod database_manager;
mod database_commands;
mod app_lifecycle;
mod long_audio;
mod long_audio_commands;
mod realtime_audio_full;
mod realtime_speaker_diarization;
mod audio_devices;
mod realtime_whisper;
// 新的优化模块
mod audio_processing;
mod layered_processor;
mod context_processor;
mod result_manager;
mod optimal_realtime_processor;
mod model_management;

use std::ffi::{CStr, CString};
use std::os::raw::c_char;
use std::sync::{Arc, Mutex};
use tauri::{Emitter, Manager, WebviewWindow};
use serde::Serialize;
use regex::Regex;
use webrtc_vad::Vad;
use rustfft::{FftPlanner, num_complex::Complex32};
use rayon::prelude::*;

// 存储相关导入
use storage_commands::StorageState;

// 音频转换相关导入
use symphonia::core::audio::SampleBuffer;
use symphonia::core::codecs::DecoderOptions;
use symphonia::core::formats::FormatOptions;
use symphonia::core::io::MediaSourceStream;
use symphonia::core::meta::MetadataOptions;
use symphonia::core::probe::Hint;
// use rubato::{FftFixedInOut, Resampler}; // 暂时不使用复杂的重采样

struct WhisperContextState {
    ctx: Mutex<*mut whisper_context>,
}

// 进度回调数据结构
#[derive(Clone, Serialize)]
pub struct RecognitionProgress {
    pub stage: String,
    pub progress: f32,
    pub message: String,
}

// 识别结果数据结构
#[derive(Clone, Serialize)]
pub struct RecognitionResult {
    pub success: bool,
    pub text: Option<String>,
    pub error: Option<String>,
    pub processing_time: f64,
}

// 全局状态管理器
pub struct RecognitionState {
    pub is_processing: Arc<Mutex<bool>>,
    pub should_cancel: Arc<Mutex<bool>>,
}

unsafe impl Send for WhisperContextState {}
unsafe impl Sync for WhisperContextState {}

impl RecognitionState {
    fn new() -> Self {
        Self {
            is_processing: Arc::new(Mutex::new(false)),
            should_cancel: Arc::new(Mutex::new(false)),
        }
    }

    fn start_processing(&self) {
        *self.is_processing.lock().unwrap() = true;
        *self.should_cancel.lock().unwrap() = false;
    }

    fn stop_processing(&self) {
        *self.is_processing.lock().unwrap() = false;
        *self.should_cancel.lock().unwrap() = false;
    }

    fn request_cancel(&self) {
        *self.should_cancel.lock().unwrap() = true;
    }

    fn should_cancel(&self) -> bool {
        *self.should_cancel.lock().unwrap()
    }

    fn is_processing(&self) -> bool {
        *self.is_processing.lock().unwrap()
    }
}

impl WhisperContextState {
    fn new(model_path: &str) -> Result<Self, String> {
        let c_model_path = CString::new(model_path).map_err(|e| e.to_string())?;
        
        // Use new recommended API
        // 1. Get default context parameters
        let mut cparams = unsafe { whisper_context_default_params() };
        // 2. Enable GPU
        cparams.use_gpu = true;

        unsafe {
            // 3. Use initialization function with parameters
            let ctx = whisper_init_from_file_with_params(c_model_path.as_ptr(), cparams);
            if ctx.is_null() {
                Err("Failed to initialize whisper context".to_string())
            } else {
                Ok(Self {
                    ctx: Mutex::new(ctx),
                })
            }
        }
    }

    pub fn get_context_ptr(&self) -> *mut whisper_context {
        *self.ctx.lock().unwrap()
    }

    pub fn reinitialize(&self, model_path: &str) -> Result<(), String> {
        let c_model_path = CString::new(model_path).map_err(|e| e.to_string())?;
        
        let mut cparams = unsafe { whisper_context_default_params() };
        cparams.use_gpu = true;

        unsafe {
            let new_ctx = whisper_init_from_file_with_params(c_model_path.as_ptr(), cparams);
            if new_ctx.is_null() {
                return Err("Failed to initialize new whisper context".to_string());
            }

            let mut ctx_lock = self.ctx.lock().unwrap();
            let old_ctx = *ctx_lock;
            *ctx_lock = new_ctx;
            
            // 释放旧的上下文
            if !old_ctx.is_null() {
                whisper_free(old_ctx);
            }
        }

        Ok(())
    }

    // 创建空的上下文，用于模型不存在的情况
    fn new_empty() -> Self {
        Self {
            ctx: Mutex::new(std::ptr::null_mut()),
        }
    }
}

impl Drop for WhisperContextState {
    fn drop(&mut self) {
        unsafe {
            // 安全地获取锁，避免在清理时panic
            if let Ok(ctx) = self.ctx.lock() {
                whisper_free(*ctx);
            }
        }
    }
}

// 音频格式转换函数 - 支持多种格式包括MP3, M4A, AAC等
pub fn load_and_convert_audio(file_path: &str) -> Result<(Vec<f32>, u32, f64), String> {
    println!("开始处理音频文件: {}", file_path);
    
    // 读取音频文件
    let file = std::fs::File::open(file_path).map_err(|e| format!("无法打开文件: {}", e))?;
    let mss = MediaSourceStream::new(Box::new(file), Default::default());

    // 探测音频格式
    let mut hint = Hint::new();
    if let Some(extension) = std::path::Path::new(file_path).extension() {
        if let Some(ext_str) = extension.to_str() {
            hint.with_extension(ext_str);
            println!("检测到文件扩展名: {}", ext_str);
        }
    }

    let meta_opts: MetadataOptions = Default::default();
    let fmt_opts: FormatOptions = Default::default();

    let probed = symphonia::default::get_probe()
        .format(&hint, mss, &fmt_opts, &meta_opts)
        .map_err(|e| format!("无法探测音频格式 (支持MP3, M4A, AAC, FLAC, OGG, WAV等): {}", e))?;

    let mut format = probed.format;
    println!("成功探测音频格式");
    
    let track = format
        .tracks()
        .iter()
        .find(|t| t.codec_params.codec != symphonia::core::codecs::CODEC_TYPE_NULL)
        .ok_or("找不到音频轨道")?;
    
    println!("找到音频轨道，编解码器: {:?}", track.codec_params.codec);

    let track_id = track.id;
    let mut decoder = symphonia::default::get_codecs()
        .make(&track.codec_params, &DecoderOptions { verify: false })
        .map_err(|e| format!("无法创建解码器: {}", e))?;

    let codec_params = &track.codec_params;
    let sample_rate = codec_params.sample_rate.ok_or("无法获取采样率")?;
    let channels = if let Some(channel_layout) = codec_params.channels {
        channel_layout.count()
    } else {
        // 对于某些M4A/AAC文件，可能需要从第一个包中推断声道数
        println!("无法直接获取声道数，将在解码过程中确定");
        1 // 先假设单声道，稍后从实际解码数据中获取
    };
    
    println!("音频信息: {}Hz, {}声道(预估)", sample_rate, channels);

    // 解码音频数据
    let mut audio_samples = Vec::new();
    let mut sample_buf = None;
    let mut actual_channels = channels; // 从音频数据中获取的实际声道数

    loop {
        let packet = match format.next_packet() {
            Ok(packet) => packet,
            Err(_) => break,
        };

        if packet.track_id() != track_id {
            continue;
        }

        match decoder.decode(&packet) {
            Ok(decoded) => {
                if sample_buf.is_none() {
                    let spec = *decoded.spec();
                    actual_channels = spec.channels.count(); // 从实际解码数据获取声道数
                    println!("从解码数据确定实际声道数: {}", actual_channels);
                    let duration = decoded.capacity() as u64;
                    sample_buf = Some(SampleBuffer::<f32>::new(duration, spec));
                }

                if let Some(ref mut buf) = sample_buf {
                    buf.copy_interleaved_ref(decoded);
                    audio_samples.extend_from_slice(buf.samples());
                }
            }
            Err(e) => {
                println!("解码包时出错: {}, 跳过", e);
                continue;
            }
        }
    }

    if audio_samples.is_empty() {
        return Err("无法解码音频数据".to_string());
    }

    // 转换为单声道 (如果是多声道)
    let mono_samples = if actual_channels > 1 {
        println!("转换{}声道音频为单声道", actual_channels);
        audio_samples
            .chunks(actual_channels)
            .map(|chunk| chunk.iter().sum::<f32>() / actual_channels as f32)
            .collect()
    } else {
        println!("音频已经是单声道");
        audio_samples
    };

    // 重采样到 16kHz (如果需要)
    let final_samples = if sample_rate != 16000 {
        println!("需要重采样: {}Hz -> 16000Hz, 样本数: {}", sample_rate, mono_samples.len());
        
        // 使用高质量重采样
        match high_quality_resample(&mono_samples, sample_rate, 16000) {
            Ok(resampled) => {
                println!("高质量重采样完成: {} -> {} 个采样点", mono_samples.len(), resampled.len());
                resampled
            }
            Err(e) => {
                println!("高质量重采样失败，使用备用方案: {}", e);
                // 回退到简单重采样
                fallback_resample(&mono_samples, sample_rate, 16000)
            }
        }
    } else {
        mono_samples
    };

    // 应用完整音频预处理流水线
    let config = AudioProcessingConfig::default();
    let optimized_samples = match advanced_audio_preprocessing_pipeline(final_samples.clone(), &config) {
        Ok(processed) => {
            println!("完整音频预处理流水线成功");
            processed
        }
        Err(e) => {
            println!("完整预处理失败，尝试简单优化: {}", e);
            // 回退到简单处理
            match preprocess_audio_with_vad(final_samples.clone()) {
                Ok(processed) => processed,
                Err(_) => {
                    println!("简单处理也失败，使用原始音频");
                    final_samples
                }
            }
        }
    };

    let duration = optimized_samples.len() as f64 / 16000.0;
    Ok((optimized_samples, 16000, duration))
}

// Learn more about Tauri commands at https://tauri.app/develop/calling-rust/
#[tauri::command]
fn greet(name: &str) -> String {
    format!("Hello, {}! You've been greeted from Rust!", name)
}

// 改进的语音识别命令，在独立线程中运行避免阻塞前端
#[tauri::command]
fn recognize_file_async(
    path: String,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    app_handle: tauri::AppHandle,
) -> Result<String, String> {
    // 获取状态管理器
    let recognition_state = app_handle.state::<RecognitionState>();
    
    // 检查是否已在处理中
    if recognition_state.is_processing() {
        return Err("已有识别任务在进行中".to_string());
    }
    
    // 启动处理状态
    recognition_state.start_processing();
    
    // 获取主窗口
    let window = app_handle.get_webview_window("main").unwrap();
    
    // 立即发送开始事件
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "initializing".to_string(),
        progress: 0.0,
        message: "初始化语音识别引擎...".to_string(),
    });
    
    // 在新线程中执行识别，避免阻塞前端
    let path_clone = path.clone();
    let language_clone = language.clone();
    let mode_clone = mode.clone();
    let initial_prompt_clone = initial_prompt.clone();
    let app_handle_clone = app_handle.clone();
    
    std::thread::spawn(move || {
        let whisper_state = app_handle_clone.state::<WhisperContextState>();
        let recognition_state = app_handle_clone.state::<RecognitionState>();
        let window = app_handle_clone.get_webview_window("main").unwrap();
        
        let result = recognize_file_blocking_inner(
            path_clone, 
            language_clone,
            mode_clone,
            initial_prompt_clone,
            window, 
            &*whisper_state, 
            &*recognition_state
        );
        
        // 无论成功失败都停止处理状态
        recognition_state.stop_processing();
        
        match result {
            Ok(text) => println!("识别成功完成: {} 字符", text.len()),
            Err(e) => println!("识别失败: {}", e),
        }
    });
    
    Ok("识别任务已启动".to_string())
}

#[tauri::command]
fn cancel_file_transcription(app_handle: tauri::AppHandle) -> Result<String, String> {
    let recognition_state = app_handle.state::<RecognitionState>();
    
    // 总是设置取消标志，即使当前状态显示没有处理中的任务
    // 这可以处理竞态条件问题，确保取消请求不会被拒绝
    recognition_state.request_cancel();
    
    // 发送取消事件到前端
    if let Some(window) = app_handle.get_webview_window("main") {
        let _ = window.emit("recognition_progress", RecognitionProgress {
            stage: "cancelling".to_string(),
            progress: 0.0,
            message: "正在取消转录...".to_string(),
        });
    }
    
    // 根据处理状态返回相应的消息
    if recognition_state.is_processing() {
        Ok("转录取消请求已发送".to_string())
    } else {
        Ok("取消请求已处理".to_string())
    }
}

// 实际的阻塞式识别函数
fn recognize_file_blocking_inner(
    path: String,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    window: WebviewWindow,
    whisper_state: &WhisperContextState,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    let start_time = std::time::Instant::now();
    
    // 检查是否需要取消
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("转录已被用户取消".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("转录已被用户取消".to_string());
    }

    // 步骤1: 音频转换
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "converting".to_string(),
        progress: 20.0,
        message: "正在转换音频格式...".to_string(),
    });

    let (audio_data, _, _) = match load_and_convert_audio(&path) {
        Ok(data) => {
            println!("音频转换成功: {} 个采样点", data.0.len());
            data
        }
        Err(e) => {
            // 如果通用转换失败，尝试作为 WAV 文件处理
            println!("通用音频转换失败: {}, 尝试 WAV 格式", e);
            
            let _ = window.emit("recognition_progress", RecognitionProgress {
                stage: "converting_wav".to_string(),
                progress: 25.0,
                message: "尝试WAV格式转换...".to_string(),
            });
            
            let mut reader = hound::WavReader::open(&path).map_err(|e| {
                let error_msg = format!("无法打开文件 (尝试了通用格式和 WAV 格式): {}", e);
                // 停止处理状态并发送错误事件
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                error_msg
            })?;
            let spec = reader.spec();
            
            if spec.channels != 1 || spec.sample_rate != 16000 {
                let error_msg = format!(
                    "检测到 WAV 文件，但格式不符合要求:\\n当前: {}声道, {}Hz\\n需要: 1声道, 16000Hz\\n\\n建议使用 FFmpeg 转换:\\nffmpeg -i \\\"{}\\\" -ar 16000 -ac 1 output.wav", 
                    spec.channels, spec.sample_rate, path
                );
                // 停止处理状态并发送错误事件
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                return Err(error_msg);
            }
            
            let samples: Result<Vec<i16>, _> = reader.samples().collect();
            let samples = samples.map_err(|e| {
                let error_msg = e.to_string();
                // 停止处理状态并发送错误事件
                recognition_state.stop_processing();
                let _ = window.emit("recognition_complete", RecognitionResult {
                    success: false,
                    text: None,
                    error: Some(error_msg.clone()),
                    processing_time: 0.0,
                });
                error_msg
            })?;
            
            (samples.iter().map(|&s| s as f32 / 32768.0).collect(), 16000, samples.len() as f64 / 16000.0)
        }
    };

    // 检查是否需要取消
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("转录已被用户取消".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("转录已被用户取消".to_string());
    }

    // 步骤2: 准备识别
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "preparing".to_string(),
        progress: 40.0,
        message: "准备语音识别...".to_string(),
    });

    // 检查是否需要取消
    if recognition_state.should_cancel() {
        let _ = window.emit("recognition_complete", RecognitionResult {
            success: false,
            text: None,
            error: Some("转录已被用户取消".to_string()),
            processing_time: start_time.elapsed().as_secs_f64(),
        });
        return Err("转录已被用户取消".to_string());
    }

    // 步骤3: 使用高级音频识别流程
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "processing".to_string(),
        progress: 50.0,
        message: "开始高级音频预处理...".to_string(),
    });

    // 使用高级识别流水线，包含音频预处理、VAD分段、增强等功能
    let full_text = match advanced_recognition_pipeline(audio_data, language.clone(), mode.clone(), initial_prompt.clone(), whisper_state, &window, recognition_state) {
        Ok(text) => text,
        Err(e) => {
            let error_msg = format!("高级识别流程失败: {}", e);
            // 停止处理状态并发送错误事件
            recognition_state.stop_processing();
            let _ = window.emit("recognition_complete", RecognitionResult {
                success: false,
                text: None,
                error: Some(error_msg.clone()),
                processing_time: start_time.elapsed().as_secs_f64(),
            });
            return Err(error_msg);
        }
    };

    // 步骤4: 文本后处理
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "post_processing".to_string(),
        progress: 95.0,
        message: "后处理识别结果...".to_string(),
    });

    let processed_text = post_process_text(&full_text, &language);

    let processing_time = start_time.elapsed().as_secs_f64();

    // 步骤5: 完成
    let _ = window.emit("recognition_progress", RecognitionProgress {
        stage: "completing".to_string(),
        progress: 100.0,
        message: "识别完成!".to_string(),
    });

    // 发送结果事件
    let final_result = RecognitionResult {
        success: true,
        text: Some(processed_text.clone()),
        error: None,
        processing_time,
    };

    let _ = window.emit("recognition_complete", final_result);
    
    // 停止处理状态
    recognition_state.stop_processing();

    Ok(processed_text)
}


// #[tauri::command]
// fn recognize_file(
//     path: String,
//     whisper_state: State<'_, WhisperContextState>,
// ) -> Result<String, String> {
//     let mut reader = hound::WavReader::open(&path).map_err(|e| e.to_string())?;
//     let samples: Vec<i16> = reader.samples().map(|s| s.unwrap()).collect();

//     // whisper.cpp needs f32 format sample data
//     let mut audio_data: Vec<f32> = samples
//         .iter()
//         .map(|&s| s as f32 / 32768.0)
//         .collect();

//     let ctx = whisper_state.ctx.lock().unwrap();

//     let mut params = unsafe { whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_GREEDY) };
//     params.n_threads = 8; // M1 has 8 performance cores

//     let result = unsafe {
//         whisper_full(
//             *ctx,
//             params,
//             audio_data.as_mut_ptr(),
//             audio_data.len() as i32,
//         )
//     };

//     if result != 0 {
//         return Err("Whisper recognition failed".to_string());
//     }

//     let num_segments = unsafe { whisper_full_n_segments(*ctx) };
//     let mut full_text = String::new();

//     for i in 0..num_segments {
//         let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
//         if !segment_ptr.is_null() {
//             let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
//             full_text.push_str(c_str.to_str().unwrap_or(""));
//         }
//     }

//     Ok(full_text)
// }

#[cfg_attr(mobile, tauri::mobile_entry_point)]
// 窗口控制命令
#[tauri::command]
async fn minimize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.minimize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn maximize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.maximize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn unmaximize_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.unmaximize().map_err(|e| e.to_string())
}

#[tauri::command]
async fn close_window(window: tauri::WebviewWindow) -> Result<(), String> {
    window.close().map_err(|e| e.to_string())
}

#[tauri::command]
async fn is_maximized(window: tauri::WebviewWindow) -> Result<bool, String> {
    window.is_maximized().map_err(|e| e.to_string())
}

pub fn run() {
    // 先创建ModelManager来读取持久化配置
    let model_manager = Arc::new(Mutex::new(model_management::ModelManager::new()));
    
    // 从持久化配置获取当前模型路径
    let model_path = {
        let manager = model_manager.lock().unwrap();
        manager.get_current_model_path()
    };
    
    // 检查模型文件是否存在，如果不存在则使用默认模型
    let final_model_path = if model_path.exists() {
        model_path
    } else {
        // 如果配置的模型不存在，使用默认路径
        let default_path = std::env::current_dir()
            .unwrap()
            .parent() // 从 src-tauri 目录回到项目根目录
            .unwrap()
            .join("models")
            .join("ggml-large-v3.bin");
        
        // 如果默认模型也不存在，创建一个临时的空上下文
        if !default_path.exists() {
            eprintln!("Warning: No model file found. Please download a model first.");
            // 创建空的模型路径，稍后需要用户手动切换
            std::env::current_dir()
                .unwrap()
                .parent()
                .unwrap()
                .join("models")
                .join("dummy.bin")
        } else {
            default_path
        }
    };
    
    let whisper_context = if final_model_path.exists() {
        WhisperContextState::new(&final_model_path.to_string_lossy())
            .expect("Failed to create whisper context")
    } else {
        // 创建一个占位符上下文，稍后需要重新初始化
        WhisperContextState::new_empty()
    };
    
    let recognition_state = RecognitionState::new();

    tauri::Builder::default()
        .manage(whisper_context)
        .manage(recognition_state)
        .manage(StorageState::new())
        .manage(realtime_audio_full::AudioCaptureState::default())
        // 新的优化处理器状态
        .manage(optimal_realtime_processor::OptimalRealtimeState::default())
        // 模型管理状态
        .manage(model_manager)
        .plugin(tauri_plugin_opener::init())
        .plugin(tauri_plugin_dialog::init())
        .plugin(tauri_plugin_fs::init())
        .plugin(tauri_plugin_window_state::Builder::default().build())
        .plugin(tauri_plugin_os::init())
        .invoke_handler(tauri::generate_handler![
            greet, 
            recognize_file_async,
            cancel_file_transcription,
            storage_commands::init_storage,
            storage_commands::save_transcription_record,
            storage_commands::get_transcription_record,
            storage_commands::get_all_transcription_records,
            storage_commands::update_transcription_status,
            storage_commands::update_transcription_result,
            storage_commands::delete_transcription_record,
            storage_commands::toggle_transcription_star,
            storage_commands::update_transcription_name,
            storage_commands::search_transcription_records,
            // 提示词管理相关命令
            storage_commands::get_prompt_templates,
            storage_commands::get_prompts_by_filter,
            storage_commands::save_prompt_template,
            storage_commands::get_prompt_template,
            storage_commands::delete_prompt_template,
            storage_commands::search_prompt_templates,
            storage_commands::increment_prompt_usage,
            // 数据库管理命令
            database_commands::get_database_info,
            database_commands::create_database_backup,
            database_commands::list_database_backups,
            database_commands::restore_database_backup,
            database_commands::vacuum_database,
            database_commands::check_database_integrity,
            database_commands::delete_database_backup,
            long_audio_commands::create_long_audio_task,
            long_audio_commands::start_long_audio_task,
            long_audio_commands::pause_long_audio_task,
            long_audio_commands::resume_long_audio_task,
            long_audio_commands::cancel_long_audio_task,
            long_audio_commands::get_long_audio_task,
            long_audio_commands::get_all_long_audio_tasks,
            realtime_audio_full::start_realtime_recording,
            realtime_audio_full::pause_realtime_recording,
            realtime_audio_full::resume_realtime_recording,
            realtime_audio_full::stop_realtime_recording,
            realtime_audio_full::get_recording_duration,
            audio_devices::get_audio_devices,
            audio_devices::test_audio_device,
            audio_devices::stop_audio_test,
            audio_devices::start_mic_test,
            audio_devices::get_mic_test_state,
            audio_devices::play_recorded_audio,
            audio_devices::set_global_audio_device,
            audio_devices::get_global_audio_device,
            // 新的优化实时处理命令
            optimal_realtime_processor::start_optimal_realtime_recording,
            optimal_realtime_processor::pause_optimal_realtime_recording,
            optimal_realtime_processor::resume_optimal_realtime_recording,
            optimal_realtime_processor::stop_optimal_realtime_recording,
            optimal_realtime_processor::get_optimal_current_transcript,
            optimal_realtime_processor::get_optimal_segments,
            optimal_realtime_processor::update_optimal_segment,
            optimal_realtime_processor::get_optimal_recording_duration,
            // 窗口控制命令
            minimize_window,
            maximize_window,
            unmaximize_window,
            close_window,
            is_maximized,
            // 模型管理命令
            model_management::list_installed_models,
            model_management::get_storage_info,
            model_management::download_model,
            model_management::switch_model,
            model_management::delete_model,
            model_management::scan_local_models,
            model_management::import_local_model,
            model_management::get_current_model
        ])
        .setup(|app| {
            // 同步初始化关键组件，确保应用就绪前完成初始化
            let app_handle = app.handle().clone();
            
            // 使用 block_on 确保初始化在应用启动前完成
            match tauri::async_runtime::block_on(app_lifecycle::initialize_app(&app_handle)) {
                Ok(_) => {
                    println!("✅ 应用初始化成功完成");
                    Ok(())
                },
                Err(e) => {
                    eprintln!("❗ 应用初始化失败: {}", e);
                    // 应用启动失败，返回错误
                    Err(Box::new(std::io::Error::new(
                        std::io::ErrorKind::Other,
                        format!("应用初始化失败: {}", e)
                    )))
                }
            }
        })
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}

// 文本后处理函数
fn post_process_text(text: &str, language: &str) -> String {
    let mut processed = text.to_string();
    
    // 基础清理：去除多余空格和换行
    processed = processed.trim().to_string();
    processed = Regex::new(r"\s+").unwrap().replace_all(&processed, " ").to_string();
    
    match language {
        "zh" => post_process_chinese(&processed),
        "en" => post_process_english(&processed),
        _ => post_process_auto(&processed), // 自动检测或其他语言
    }
}

// 去除重复字符的辅助函数
fn remove_repeated_chars(text: &str) -> String {
    let chars: Vec<char> = text.chars().collect();
    let mut result = String::new();
    let mut i = 0;
    
    while i < chars.len() {
        let current_char = chars[i];
        result.push(current_char);
        
        // 跳过连续的相同字符（保留第一个）
        while i + 1 < chars.len() && chars[i + 1] == current_char {
            i += 1;
        }
        i += 1;
    }
    
    result
}

// 中文文本后处理
fn post_process_chinese(text: &str) -> String {
    let mut result = text.to_string();
    
    // 1. 去除重复的字符 (使用简单的字符串替换方式)
    // 注意: Rust的regex不支持反向引用，改用手动处理
    result = remove_repeated_chars(&result);
    
    // 2. 修复常见的中文识别错误
    let fixes = vec![
        ("的的", "的"),
        ("了了", "了"),
        ("在在", "在"),
        ("是是", "是"),
        ("有有", "有"),
        ("我我", "我"),
        ("你你", "你"),
        ("他他", "他"),
        ("她她", "她"),
        ("它它", "它"),
    ];
    
    for (wrong, correct) in fixes {
        result = result.replace(wrong, correct);
    }
    
    // 3. 中文标点符号矫正
    result = result.replace("。。", "。");
    result = result.replace("，，", "，");
    result = result.replace("？？", "？");
    result = result.replace("！！", "！");
    
    // 4. 句子首尾处理
    result = result.trim().to_string();
    
    // 5. 添加适当的句号（如果文本较长且没有结尾标点）
    if result.len() > 10 && !result.ends_with(['。', '？', '！', '.', '?', '!']) {
        result.push('。');
    }
    
    result
}

// 英文文本后处理
fn post_process_english(text: &str) -> String {
    let mut result = text.to_string();
    
    // 1. 修复常见的英文识别错误
    let fixes = vec![
        (" a a ", " a "),
        (" the the ", " the "),
        (" and and ", " and "),
        (" to to ", " to "),
        (" of of ", " of "),
        (" in in ", " in "),
        (" is is ", " is "),
        (" it it ", " it "),
        (" that that ", " that "),
    ];
    
    for (wrong, correct) in fixes {
        result = result.replace(wrong, correct);
    }
    
    // 2. 大小写矫正
    result = fix_english_capitalization(&result);
    
    // 3. 标点符号矫正
    result = result.replace(",,", ",");
    result = result.replace("..", ".");
    result = result.replace("??", "?");
    result = result.replace("!!", "!");
    
    // 4. 句子间距矫正
    result = Regex::new(r"\.([A-Z])").unwrap().replace_all(&result, ". $1").to_string();
    result = Regex::new(r"\?([A-Z])").unwrap().replace_all(&result, "? $1").to_string();
    result = Regex::new(r"!([A-Z])").unwrap().replace_all(&result, "! $1").to_string();
    
    // 5. 添加适当的句号
    result = result.trim().to_string();
    if result.len() > 10 && !result.ends_with(['.', '?', '!', '。', '？', '！']) {
        result.push('.');
    }
    
    result
}

// 英文大小写矫正
fn fix_english_capitalization(text: &str) -> String {
    let mut result = String::new();
    let mut capitalize_next = true;
    
    for c in text.chars() {
        if c.is_alphabetic() {
            if capitalize_next {
                result.push(c.to_uppercase().next().unwrap_or(c));
                capitalize_next = false;
            } else {
                result.push(c.to_lowercase().next().unwrap_or(c));
            }
        } else {
            result.push(c);
            if c == '.' || c == '?' || c == '!' || c == '。' || c == '？' || c == '！' {
                capitalize_next = true;
            }
        }
    }
    
    result
}

// 自动检测语言的后处理
fn post_process_auto(text: &str) -> String {
    // 简单检测是否包含中文字符
    let has_chinese = text.chars().any(|c| {
        let code = c as u32;
        (0x4E00..=0x9FFF).contains(&code) // 中文字符范围
    });
    
    if has_chinese {
        post_process_chinese(text)
    } else {
        post_process_english(text)
    }
}

// =============================================================================
// 音频处理优化函数
// =============================================================================

// 高质量重采样函数，使用rubato库进行专业级重采样
fn high_quality_resample(samples: &[f32], from_rate: u32, to_rate: u32) -> Result<Vec<f32>, String> {
    if from_rate == to_rate {
        return Ok(samples.to_vec());
    }
    
    use rubato::{Resampler, SincFixedIn, SincInterpolationType::*};
    
    // 计算参数
    let chunk_size = 1024; // 处理块大小
    let params = rubato::SincInterpolationParameters {
        sinc_len: 256,        // 更长的sinc长度提高质量
        f_cutoff: 0.95,       // 截止频率
        interpolation: Linear, // 线性插值
        oversampling_factor: 256, // 过采样因子
        window: rubato::WindowFunction::BlackmanHarris2, // 高质量窗函数
    };
    
    let mut resampler = SincFixedIn::<f32>::new(
        to_rate as f64 / from_rate as f64,
        2.0, // 最大倍率变化
        params,
        chunk_size,
        1, // 单声道
    ).map_err(|e| format!("创建重采样器失败: {}", e))?;
    
    let mut input_data = vec![samples.to_vec()];
    let mut output = Vec::new();
    
    // 分块处理
    let mut pos = 0;
    while pos < samples.len() {
        let end = (pos + chunk_size).min(samples.len());
        let chunk = &samples[pos..end];
        
        // 如果块太小，填充零
        let mut padded_chunk = chunk.to_vec();
        while padded_chunk.len() < chunk_size {
            padded_chunk.push(0.0);
        }
        input_data[0] = padded_chunk;
        
        match resampler.process(&input_data, None) {
            Ok(out) => {
                output.extend_from_slice(&out[0]);
            }
            Err(e) => return Err(format!("重采样处理失败: {}", e)),
        }
        
        pos += chunk_size;
    }
    
    Ok(output)
}

// 备用简单重采样方案
fn fallback_resample(samples: &[f32], from_rate: u32, to_rate: u32) -> Vec<f32> {
    if from_rate == to_rate {
        return samples.to_vec();
    }
    
    let ratio = from_rate as f64 / to_rate as f64;
    let new_length = (samples.len() as f64 / ratio) as usize;
    let mut resampled = Vec::with_capacity(new_length);
    
    for i in 0..new_length {
        let source_index = (i as f64 * ratio) as usize;
        if source_index < samples.len() {
            resampled.push(samples[source_index]);
        }
    }
    
    resampled
}

// 音频增强处理
fn audio_enhancement(samples: &mut [f32]) {
    println!("开始音频增强处理...");
    
    // 1. 预加重滤波 (提升高频分量)
    apply_preemphasis(samples, 0.97);
    
    // 2. 归一化处理
    normalize_audio(samples);
    
    // 3. 简单降噪 (基于能量阈值)
    simple_noise_reduction(samples);
    
    println!("音频增强处理完成");
}

// 预加重滤波器 - 提升高频分量，改善语音识别效果
fn apply_preemphasis(samples: &mut [f32], coeff: f32) {
    if samples.is_empty() {
        return;
    }
    
    // 从后往前处理，避免覆盖问题
    for i in (1..samples.len()).rev() {
        samples[i] -= coeff * samples[i-1];
    }
}

// 音频归一化 - 防止溢出并优化动态范围
fn normalize_audio(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    // 找到最大绝对值
    let max_abs = samples.iter()
        .map(|&x| x.abs())
        .fold(0.0f32, f32::max);
    
    if max_abs > 0.0 && max_abs != 1.0 {
        // 归一化到 [-0.95, 0.95] 避免削波
        let scale = 0.95 / max_abs;
        for sample in samples.iter_mut() {
            *sample *= scale;
        }
    }
}

// 简单降噪 - 基于能量阈值的噪声抑制
fn simple_noise_reduction(samples: &mut [f32]) {
    if samples.len() < 1024 {
        return;
    }
    
    // 计算噪声阈值 (前100ms的平均能量)
    let noise_samples = samples.len().min(1600); // 100ms at 16kHz
    let noise_energy: f32 = samples[..noise_samples]
        .iter()
        .map(|&x| x * x)
        .sum::<f32>() / noise_samples as f32;
    
    let threshold = noise_energy * 1.5; // 噪声阈值
    
    // 基于阈值的软降噪
    for sample in samples.iter_mut() {
        let energy = *sample * *sample;
        if energy < threshold {
            *sample *= 0.3; // 降低噪声区域的增益
        }
    }
}

// VAD (语音活动检测) - 检测音频中的语音段
pub fn detect_speech_segments(samples: &[f32]) -> Result<Vec<(usize, usize)>, String> {
    println!("开始VAD语音活动检测...");
    
    // 初始化WebRTC VAD
    let mut vad = Vad::new();
    vad.set_mode(webrtc_vad::VadMode::Quality);
    
    let frame_size = 480; // 30ms at 16kHz
    let mut speech_segments = Vec::new();
    let mut current_start: Option<usize> = None;
    let min_speech_duration = 320; // 最小语音段长度 (20ms)
    let min_silence_duration = 160; // 最小静默长度 (10ms)
    
    let mut silence_counter = 0;
    
    for (i, chunk) in samples.chunks(frame_size).enumerate() {
        if chunk.len() != frame_size {
            break; // 跳过不完整的块
        }
        
        // 转换为i16格式供VAD使用
        let chunk_i16: Vec<i16> = chunk.iter()
            .map(|&x| (x * 32767.0).clamp(-32767.0, 32767.0) as i16)
            .collect();
        
        match vad.is_voice_segment(&chunk_i16) {
            Ok(is_speech) => {
                let frame_start = i * frame_size;
                
                if is_speech {
                    // 检测到语音
                    silence_counter = 0;
                    if current_start.is_none() {
                        current_start = Some(frame_start);
                    }
                } else {
                    // 静默区域
                    silence_counter += 1;
                    
                    // 如果静默时间足够长，结束当前语音段
                    if let Some(start) = current_start {
                        if silence_counter * frame_size >= min_silence_duration {
                            let end = frame_start;
                            if end - start >= min_speech_duration {
                                speech_segments.push((start, end));
                            }
                            current_start = None;
                            silence_counter = 0;
                        }
                    }
                }
            }
            Err(e) => {
                println!("VAD处理错误: {:?}", e);
                // 发生错误时跳过这一帧
                continue;
            }
        }
    }
    
    // 处理最后一个语音段
    if let Some(start) = current_start {
        let end = samples.len();
        if end - start >= min_speech_duration {
            speech_segments.push((start, end));
        }
    }
    
    // 合并相邻的语音段
    let merged_segments = merge_adjacent_segments(speech_segments, 800); // 合并间隔小于50ms的段
    
    println!("VAD检测完成，发现 {} 个语音段", merged_segments.len());
    Ok(merged_segments)
}

// 合并相邻的语音段
fn merge_adjacent_segments(segments: Vec<(usize, usize)>, max_gap: usize) -> Vec<(usize, usize)> {
    if segments.is_empty() {
        return segments;
    }
    
    let mut merged = Vec::new();
    let mut current_start = segments[0].0;
    let mut current_end = segments[0].1;
    
    for &(start, end) in &segments[1..] {
        if start - current_end <= max_gap {
            // 合并相邻段
            current_end = end;
        } else {
            // 保存当前段，开始新段
            merged.push((current_start, current_end));
            current_start = start;
            current_end = end;
        }
    }
    
    // 添加最后一段
    merged.push((current_start, current_end));
    
    merged
}

// 基于VAD的智能音频预处理
fn preprocess_audio_with_vad(samples: Vec<f32>) -> Result<Vec<f32>, String> {
    let mut processed_samples = samples;
    
    // 1. 音频增强
    audio_enhancement(&mut processed_samples);
    
    // 2. VAD检测语音段
    let speech_segments = detect_speech_segments(&processed_samples)?;
    
    if speech_segments.is_empty() {
        println!("警告: 未检测到语音段，返回原始音频");
        return Ok(processed_samples);
    }
    
    // 3. 提取并连接语音段
    let mut vad_processed = Vec::new();
    let mut total_speech_duration = 0;
    
    for (start, end) in speech_segments {
        let segment = &processed_samples[start..end];
        vad_processed.extend_from_slice(segment);
        total_speech_duration += end - start;
    }
    
    let original_duration = processed_samples.len() as f32 / 16000.0;
    let speech_duration = total_speech_duration as f32 / 16000.0;
    
    println!("VAD处理完成:");
    println!("  原始音频长度: {:.1}秒", original_duration);
    println!("  语音内容长度: {:.1}秒", speech_duration);
    println!("  语音占比: {:.1}%", (speech_duration / original_duration) * 100.0);
    
    Ok(vad_processed)
}

// =============================================================================
// 完整音频预处理流水线
// =============================================================================

#[derive(Debug, Clone)]
pub struct AudioProcessingConfig {
    pub enable_preemphasis: bool,
    pub preemphasis_coeff: f32,
    pub enable_normalization: bool,
    pub enable_noise_reduction: bool,
    pub noise_threshold_multiplier: f32,
    pub enable_spectral_enhancement: bool,
    pub enable_vad: bool,
    pub vad_min_speech_duration_ms: u32,
    pub vad_min_silence_duration_ms: u32,
    pub enable_dynamic_range_compression: bool,
}

impl Default for AudioProcessingConfig {
    fn default() -> Self {
        Self {
            enable_preemphasis: true,
            preemphasis_coeff: 0.97,
            enable_normalization: true,
            enable_noise_reduction: true,
            noise_threshold_multiplier: 1.5,
            enable_spectral_enhancement: true,
            enable_vad: true,
            vad_min_speech_duration_ms: 20,
            vad_min_silence_duration_ms: 10,
            enable_dynamic_range_compression: true,
        }
    }
}

// 完整的音频预处理流水线
fn advanced_audio_preprocessing_pipeline(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let mut processed = samples;
    let total_steps = 7;
    let mut current_step = 0;
    
    println!("开始完整音频预处理流水线 ({} 个处理步骤)...", total_steps);
    
    // 步骤1: 预加重滤波
    if config.enable_preemphasis {
        current_step += 1;
        println!("[{}/{}] 应用预加重滤波 (系数: {})", current_step, total_steps, config.preemphasis_coeff);
        apply_preemphasis(&mut processed, config.preemphasis_coeff);
    }
    
    // 步骤2: 音频归一化
    if config.enable_normalization {
        current_step += 1;
        println!("[{}/{}] 应用音频归一化", current_step, total_steps);
        normalize_audio(&mut processed);
    }
    
    // 步骤3: 频谱增强
    if config.enable_spectral_enhancement {
        current_step += 1;
        println!("[{}/{}] 应用频谱增强", current_step, total_steps);
        match spectral_enhancement(&mut processed) {
            Ok(_) => println!("频谱增强完成"),
            Err(e) => println!("频谱增强失败: {}, 跳过", e),
        }
    }
    
    // 步骤4: 动态范围压缩
    if config.enable_dynamic_range_compression {
        current_step += 1;
        println!("[{}/{}] 应用动态范围压缩", current_step, total_steps);
        apply_dynamic_range_compression(&mut processed);
    }
    
    // 步骤5: 噪声抑制
    if config.enable_noise_reduction {
        current_step += 1;
        println!("[{}/{}] 应用自适应噪声抑制", current_step, total_steps);
        adaptive_noise_reduction(&mut processed, config.noise_threshold_multiplier);
    }
    
    // 步骤6: VAD语音活动检测
    if config.enable_vad {
        current_step += 1;
        println!("[{}/{}] 应用VAD语音活动检测", current_step, total_steps);
        processed = match apply_vad_processing(processed.clone(), config) {
            Ok(vad_result) => vad_result,
            Err(e) => {
                println!("VAD处理失败: {}, 使用原始音频", e);
                processed
            }
        };
    }
    
    // 步骤7: 最终质量检查和优化
    current_step += 1;
    println!("[{}/{}] 最终质量检查和优化", current_step, total_steps);
    final_quality_optimization(&mut processed);
    
    let duration = processed.len() as f32 / 16000.0;
    println!("音频预处理流水线完成! 处理后音频长度: {:.2}秒", duration);
    
    Ok(processed)
}

// 频谱增强 - 使用FFT进行频域处理
fn spectral_enhancement(samples: &mut [f32]) -> Result<(), String> {
    if samples.len() < 1024 {
        return Err("音频太短，无法进行频谱增强".to_string());
    }
    
    let mut planner = FftPlanner::<f32>::new();
    let fft_size = 1024;
    let overlap = fft_size / 2;
    let fft = planner.plan_fft_forward(fft_size);
    let ifft = planner.plan_fft_inverse(fft_size);
    
    // 窗函数 (汉宁窗)
    let window: Vec<f32> = (0..fft_size)
        .map(|i| 0.5 * (1.0 - (2.0 * std::f32::consts::PI * i as f32 / (fft_size - 1) as f32).cos()))
        .collect();
    
    let mut enhanced = vec![0.0f32; samples.len()];
    let mut pos = 0;
    
    while pos + fft_size <= samples.len() {
        // 应用窗函数并转换为复数
        let mut buffer: Vec<Complex32> = samples[pos..pos + fft_size]
            .iter()
            .zip(window.iter())
            .map(|(&s, &w)| Complex32::new(s * w, 0.0))
            .collect();
        
        // 前向FFT
        fft.process(&mut buffer);
        
        // 频域增强
        for i in 0..buffer.len() {
            let magnitude = buffer[i].norm();
            let phase = buffer[i].arg();
            
            // 增强语音频段 (300Hz - 3400Hz)
            let freq = i as f32 * 16000.0 / fft_size as f32;
            let enhancement_factor = if freq >= 300.0 && freq <= 3400.0 {
                1.2 // 增强语音频段
            } else if freq < 100.0 || freq > 8000.0 {
                0.5 // 抑制低频噪音和高频噪音
            } else {
                1.0
            };
            
            let enhanced_magnitude = magnitude * enhancement_factor;
            buffer[i] = Complex32::from_polar(enhanced_magnitude, phase);
        }
        
        // 逆向FFT
        ifft.process(&mut buffer);
        
        // 重叠相加
        for (i, &sample) in buffer.iter().enumerate() {
            let real_part = sample.re / fft_size as f32; // 归一化
            if pos + i < enhanced.len() {
                enhanced[pos + i] += real_part * window[i];
            }
        }
        
        pos += overlap;
    }
    
    // 复制增强结果
    samples.copy_from_slice(&enhanced[..samples.len()]);
    
    Ok(())
}

// 动态范围压缩 - 减少音量差异过大的问题
fn apply_dynamic_range_compression(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    let threshold = 0.7; // 压缩阈值
    let ratio = 4.0; // 压缩比
    let attack_time = 0.003; // 3ms攻击时间
    let release_time = 0.1; // 100ms释放时间
    
    let sample_rate = 16000.0;
    let attack_coeff = (-1.0f32 / (attack_time * sample_rate)).exp();
    let release_coeff = (-1.0f32 / (release_time * sample_rate)).exp();
    
    let mut envelope = 0.0f32;
    let mut gain_reduction = 0.0f32;
    
    for sample in samples.iter_mut() {
        let input_level = sample.abs();
        
        // 包络跟踪
        let target_envelope = if input_level > envelope {
            envelope + (input_level - envelope) * (1.0 - attack_coeff)
        } else {
            envelope + (input_level - envelope) * (1.0 - release_coeff)
        };
        envelope = target_envelope;
        
        // 计算增益减少
        let target_gain_reduction = if envelope > threshold {
            let over_threshold = envelope - threshold;
            over_threshold * (1.0 - 1.0 / ratio)
        } else {
            0.0
        };
        
        // 平滑增益减少
        gain_reduction = if target_gain_reduction > gain_reduction {
            gain_reduction + (target_gain_reduction - gain_reduction) * (1.0 - attack_coeff)
        } else {
            gain_reduction + (target_gain_reduction - gain_reduction) * (1.0 - release_coeff)
        };
        
        // 应用压缩
        let compressed_level = envelope - gain_reduction;
        let gain = if envelope > 0.0 {
            compressed_level / envelope
        } else {
            1.0
        };
        
        *sample *= gain;
    }
}

// 自适应噪声抑制 - 更智能的噪声检测和抑制
fn adaptive_noise_reduction(samples: &mut [f32], threshold_multiplier: f32) {
    if samples.len() < 3200 { // 至少200ms
        return;
    }
    
    let frame_size = 320; // 20ms frames
    let num_frames = samples.len() / frame_size;
    
    // 计算每帧的能量
    let frame_energies: Vec<f32> = (0..num_frames)
        .map(|i| {
            let start = i * frame_size;
            let end = (start + frame_size).min(samples.len());
            samples[start..end]
                .iter()
                .map(|&x| x * x)
                .sum::<f32>() / (end - start) as f32
        })
        .collect();
    
    // 估计噪声能量 (使用最小值统计)
    let mut sorted_energies = frame_energies.clone();
    sorted_energies.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let noise_energy = sorted_energies[num_frames / 10]; // 使用最低10%的能量作为噪声估计
    
    let adaptive_threshold = noise_energy * threshold_multiplier;
    
    // 应用自适应噪声抑制
    for (i, frame_energy) in frame_energies.iter().enumerate() {
        let start = i * frame_size;
        let end = (start + frame_size).min(samples.len());
        
        if *frame_energy < adaptive_threshold {
            // 噪声帧 - 应用强抑制
            let suppression_factor = 0.1;
            for sample in &mut samples[start..end] {
                *sample *= suppression_factor;
            }
        } else if *frame_energy < adaptive_threshold * 2.0 {
            // 过渡区域 - 应用渐变抑制
            let suppression_factor = 0.3 + 0.7 * ((*frame_energy - adaptive_threshold) / adaptive_threshold);
            for sample in &mut samples[start..end] {
                *sample *= suppression_factor;
            }
        }
        // 语音帧不处理
    }
}

// 应用VAD处理
fn apply_vad_processing(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let speech_segments = detect_speech_segments_advanced(&samples, config)?;
    
    if speech_segments.is_empty() {
        return Err("未检测到语音段".to_string());
    }
    
    // 提取语音段
    let mut vad_processed = Vec::new();
    for (start, end) in speech_segments {
        vad_processed.extend_from_slice(&samples[start..end]);
    }
    
    Ok(vad_processed)
}

// 高级VAD检测
fn detect_speech_segments_advanced(
    samples: &[f32],
    config: &AudioProcessingConfig,
) -> Result<Vec<(usize, usize)>, String> {
    let mut vad = Vad::new();
    vad.set_mode(webrtc_vad::VadMode::Quality);
    
    let frame_size = 480; // 30ms at 16kHz
    let min_speech_frames = (config.vad_min_speech_duration_ms * 16) as usize / frame_size;
    let min_silence_frames = (config.vad_min_silence_duration_ms * 16) as usize / frame_size;
    
    let mut speech_segments = Vec::new();
    let mut current_start: Option<usize> = None;
    let mut silence_counter = 0;
    let mut speech_counter = 0;
    
    for (i, chunk) in samples.chunks(frame_size).enumerate() {
        if chunk.len() != frame_size {
            break;
        }
        
        let chunk_i16: Vec<i16> = chunk.iter()
            .map(|&x| (x * 32767.0).clamp(-32767.0, 32767.0) as i16)
            .collect();
        
        let is_speech = vad.is_voice_segment(&chunk_i16).unwrap_or(false);
        let frame_start = i * frame_size;
        
        if is_speech {
            speech_counter += 1;
            silence_counter = 0;
            
            if current_start.is_none() && speech_counter >= min_speech_frames {
                current_start = Some(frame_start - (speech_counter - 1) * frame_size);
            }
        } else {
            silence_counter += 1;
            speech_counter = 0;
            
            if let Some(start) = current_start {
                if silence_counter >= min_silence_frames {
                    let end = frame_start;
                    if end > start {
                        speech_segments.push((start, end));
                    }
                    current_start = None;
                    silence_counter = 0;
                }
            }
        }
    }
    
    // 处理最后一个语音段
    if let Some(start) = current_start {
        speech_segments.push((start, samples.len()));
    }
    
    // 合并相邻段
    let merged = merge_adjacent_segments(speech_segments, 800);
    
    Ok(merged)
}

// 最终质量优化
fn final_quality_optimization(samples: &mut [f32]) {
    if samples.is_empty() {
        return;
    }
    
    // 去除直流分量
    let dc_offset: f32 = samples.iter().sum::<f32>() / samples.len() as f32;
    if dc_offset.abs() > 0.001 {
        for sample in samples.iter_mut() {
            *sample -= dc_offset;
        }
    }
    
    // 最终归一化
    let max_abs = samples.iter().map(|&x| x.abs()).fold(0.0f32, f32::max);
    if max_abs > 0.95 {
        let scale = 0.95 / max_abs;
        for sample in samples.iter_mut() {
            *sample *= scale;
        }
    }
    
    // 软限幅防止削波
    for sample in samples.iter_mut() {
        if sample.abs() > 0.95 {
            *sample = sample.signum() * (0.95 + 0.05 * (1.0 - (-10.0 * (sample.abs() - 0.95)).exp()));
        }
    }
}

// =============================================================================
// 智能分段处理
// =============================================================================

#[derive(Debug, Clone)]
pub struct AudioSegment {
    pub data: Vec<f32>,
    pub start_time: f32,
    pub end_time: f32,
    pub confidence: f32,
    pub is_speech: bool,
}

#[derive(Debug, Clone)]
pub struct SegmentationConfig {
    pub max_segment_duration_sec: f32,
    pub min_segment_duration_sec: f32,
    pub overlap_duration_sec: f32,
    pub enable_content_based_segmentation: bool,
    pub energy_threshold_ratio: f32,
    pub silence_threshold_sec: f32,
}

impl Default for SegmentationConfig {
    fn default() -> Self {
        Self {
            max_segment_duration_sec: 30.0,  // 30秒最大段长度
            min_segment_duration_sec: 1.0,   // 1秒最小段长度
            overlap_duration_sec: 1.0,       // 1秒重叠
            enable_content_based_segmentation: true,
            energy_threshold_ratio: 0.3,     // 能量阈值比例
            silence_threshold_sec: 0.5,      // 静默阈值
        }
    }
}

// 智能音频分段
fn intelligent_audio_segmentation(
    samples: &[f32],
    config: &SegmentationConfig,
) -> Result<Vec<AudioSegment>, String> {
    println!("开始智能音频分段...");
    
    let sample_rate = 16000.0;
    let total_duration = samples.len() as f32 / sample_rate;
    
    if total_duration <= config.max_segment_duration_sec {
        // 音频较短，不需要分段
        return Ok(vec![AudioSegment {
            data: samples.to_vec(),
            start_time: 0.0,
            end_time: total_duration,
            confidence: 1.0,
            is_speech: true,
        }]);
    }
    
    let segments = if config.enable_content_based_segmentation {
        // 基于内容的智能分段
        content_based_segmentation(samples, config)?
    } else {
        // 简单的时间分段
        time_based_segmentation(samples, config)
    };
    
    // 添加重叠区域
    let segments = add_segment_overlaps(segments, config);
    
    println!("智能分段完成，生成 {} 个音频段", segments.len());
    for (i, segment) in segments.iter().enumerate() {
        println!("  段{}: {:.1}s - {:.1}s ({:.1}s, 置信度: {:.2})", 
                i + 1, segment.start_time, segment.end_time, 
                segment.end_time - segment.start_time, segment.confidence);
    }
    
    Ok(segments)
}

// 基于内容的智能分段
fn content_based_segmentation(
    samples: &[f32],
    config: &SegmentationConfig,
) -> Result<Vec<AudioSegment>, String> {
    let sample_rate = 16000.0;
    let frame_size = (sample_rate * 0.02) as usize; // 20ms frames
    let hop_size = frame_size / 2; // 50% overlap
    
    // 计算音频特征
    let features = extract_audio_features(samples, frame_size, hop_size);
    
    // 检测分段点
    let segment_points = detect_segment_boundaries(&features, config);
    
    // 生成音频段
    let mut segments = Vec::new();
    let mut current_start = 0;
    
    for &point in &segment_points {
        let segment_samples = point.min(samples.len());
        if segment_samples > current_start {
            let start_time = current_start as f32 / sample_rate;
            let end_time = segment_samples as f32 / sample_rate;
            let duration = end_time - start_time;
            
            if duration >= config.min_segment_duration_sec {
                let segment_data = samples[current_start..segment_samples].to_vec();
                let confidence = calculate_segment_confidence(&segment_data);
                
                segments.push(AudioSegment {
                    data: segment_data,
                    start_time,
                    end_time,
                    confidence,
                    is_speech: confidence > 0.5,
                });
                
                current_start = segment_samples;
            }
        }
    }
    
    // 处理最后一段
    if current_start < samples.len() {
        let start_time = current_start as f32 / sample_rate;
        let end_time = samples.len() as f32 / sample_rate;
        let duration = end_time - start_time;
        
        if duration >= config.min_segment_duration_sec {
            let segment_data = samples[current_start..].to_vec();
            let confidence = calculate_segment_confidence(&segment_data);
            
            segments.push(AudioSegment {
                data: segment_data,
                start_time,
                end_time,
                confidence,
                is_speech: confidence > 0.5,
            });
        }
    }
    
    Ok(segments)
}

// 提取音频特征
fn extract_audio_features(samples: &[f32], frame_size: usize, hop_size: usize) -> Vec<AudioFeatures> {
    let mut features = Vec::new();
    let mut pos = 0;
    
    while pos + frame_size <= samples.len() {
        let frame = &samples[pos..pos + frame_size];
        let feature = AudioFeatures::from_frame(frame);
        features.push(feature);
        pos += hop_size;
    }
    
    features
}

#[derive(Debug, Clone)]
struct AudioFeatures {
    energy: f32,
    zero_crossing_rate: f32,
    spectral_centroid: f32,
    spectral_rolloff: f32,
}

impl AudioFeatures {
    fn from_frame(frame: &[f32]) -> Self {
        let energy = frame.iter().map(|&x| x * x).sum::<f32>() / frame.len() as f32;
        
        let zero_crossings = frame.windows(2)
            .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
            .count() as f32;
        let zero_crossing_rate = zero_crossings / (frame.len() - 1) as f32;
        
        // 简化的频谱特征计算
        let spectral_centroid = calculate_spectral_centroid(frame);
        let spectral_rolloff = calculate_spectral_rolloff(frame);
        
        Self {
            energy,
            zero_crossing_rate,
            spectral_centroid,
            spectral_rolloff,
        }
    }
}

fn calculate_spectral_centroid(frame: &[f32]) -> f32 {
    // 简化的频谱质心计算
    let mut weighted_sum = 0.0;
    let mut magnitude_sum = 0.0;
    
    for (i, &sample) in frame.iter().enumerate() {
        let magnitude = sample.abs();
        weighted_sum += i as f32 * magnitude;
        magnitude_sum += magnitude;
    }
    
    if magnitude_sum > 0.0 {
        weighted_sum / magnitude_sum
    } else {
        0.0
    }
}

fn calculate_spectral_rolloff(frame: &[f32]) -> f32 {
    // 简化的频谱滚降计算
    let magnitudes: Vec<f32> = frame.iter().map(|&x| x.abs()).collect();
    let total_energy: f32 = magnitudes.iter().sum();
    let threshold = total_energy * 0.85; // 85%能量阈值
    
    let mut cumulative_energy = 0.0;
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        cumulative_energy += magnitude;
        if cumulative_energy >= threshold {
            return i as f32 / magnitudes.len() as f32;
        }
    }
    
    1.0 // 如果没找到，返回最大值
}

// 检测分段边界
fn detect_segment_boundaries(
    features: &[AudioFeatures], 
    config: &SegmentationConfig
) -> Vec<usize> {
    let mut boundaries = Vec::new();
    let sample_rate = 16000.0;
    let hop_size = (sample_rate * 0.01) as usize; // 10ms hop
    
    let max_segment_samples = (config.max_segment_duration_sec * sample_rate) as usize;
    let silence_threshold_frames = (config.silence_threshold_sec * 100.0) as usize; // 10ms frames
    
    // 计算能量阈值
    let energies: Vec<f32> = features.iter().map(|f| f.energy).collect();
    let mut sorted_energies = energies.clone();
    sorted_energies.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let energy_threshold = sorted_energies[sorted_energies.len() / 4] * config.energy_threshold_ratio;
    
    let mut last_boundary = 0;
    let mut silence_counter = 0;
    
    for (i, feature) in features.iter().enumerate() {
        let sample_pos = i * hop_size;
        
        // 检查是否达到最大段长度
        if sample_pos - last_boundary >= max_segment_samples {
            boundaries.push(sample_pos);
            last_boundary = sample_pos;
            silence_counter = 0;
            continue;
        }
        
        // 检查静默区域
        if feature.energy < energy_threshold {
            silence_counter += 1;
            
            if silence_counter >= silence_threshold_frames && 
               sample_pos - last_boundary >= (config.min_segment_duration_sec * sample_rate) as usize {
                boundaries.push(sample_pos);
                last_boundary = sample_pos;
                silence_counter = 0;
            }
        } else {
            silence_counter = 0;
        }
    }
    
    boundaries
}

// 简单的时间分段
fn time_based_segmentation(samples: &[f32], config: &SegmentationConfig) -> Vec<AudioSegment> {
    let sample_rate = 16000.0;
    let segment_samples = (config.max_segment_duration_sec * sample_rate) as usize;
    let mut segments = Vec::new();
    let mut pos = 0;
    
    while pos < samples.len() {
        let end_pos = (pos + segment_samples).min(samples.len());
        let start_time = pos as f32 / sample_rate;
        let end_time = end_pos as f32 / sample_rate;
        
        if end_time - start_time >= config.min_segment_duration_sec {
            let segment_data = samples[pos..end_pos].to_vec();
            let confidence = calculate_segment_confidence(&segment_data);
            
            segments.push(AudioSegment {
                data: segment_data,
                start_time,
                end_time,
                confidence,
                is_speech: confidence > 0.5,
            });
        }
        
        pos = end_pos;
    }
    
    segments
}

// 添加重叠区域
fn add_segment_overlaps(
    mut segments: Vec<AudioSegment>, 
    config: &SegmentationConfig
) -> Vec<AudioSegment> {
    if segments.len() <= 1 || config.overlap_duration_sec <= 0.0 {
        return segments;
    }
    
    let sample_rate = 16000.0;
    let overlap_samples = (config.overlap_duration_sec * sample_rate) as usize;
    
    for i in 0..segments.len() - 1 {
        let next_start = 0;
        
        // 从下一段开始添加重叠
        if overlap_samples < segments[i + 1].data.len() {
            let end_index = overlap_samples.min(segments[i + 1].data.len());
            let overlap_data = segments[i + 1].data[next_start..end_index].to_vec();
            segments[i].data.extend_from_slice(&overlap_data);
            segments[i].end_time += config.overlap_duration_sec;
        }
    }
    
    segments
}

// 计算段置信度
fn calculate_segment_confidence(data: &[f32]) -> f32 {
    if data.is_empty() {
        return 0.0;
    }
    
    // 基于能量和零交叉率的简单置信度计算
    let energy = data.iter().map(|&x| x * x).sum::<f32>() / data.len() as f32;
    let zero_crossings = data.windows(2)
        .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
        .count() as f32;
    let zero_crossing_rate = zero_crossings / (data.len() - 1) as f32;
    
    // 语音通常有适中的能量和零交叉率
    let energy_score = if energy > 0.001 && energy < 0.1 { 1.0 } else { 0.5 };
    let zcr_score = if zero_crossing_rate > 0.01 && zero_crossing_rate < 0.5 { 1.0 } else { 0.5 };
    
    (energy_score + zcr_score) / 2.0
}

// =============================================================================
// 并行处理优化
// =============================================================================

#[derive(Debug, Clone)]
pub struct ParallelProcessingConfig {
    pub enable_parallel_segmentation: bool,
    pub enable_parallel_recognition: bool,
    pub max_concurrent_segments: usize,
    pub segment_processing_timeout_sec: u64,
}

impl Default for ParallelProcessingConfig {
    fn default() -> Self {
        Self {
            enable_parallel_segmentation: true,
            enable_parallel_recognition: true,
            max_concurrent_segments: 4, // 并行处理4个段
            segment_processing_timeout_sec: 300, // 5分钟超时
        }
    }
}

// 简化的分段识别（主要用于长音频）
fn segment_based_recognition(
    segments: Vec<AudioSegment>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    window: &WebviewWindow,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    println!("开始分段识别，共 {} 个段", segments.len());
    
    // 检查是否需要取消
    if recognition_state.should_cancel() {
        return Err("转录已被用户取消".to_string());
    }
    
    if segments.is_empty() {
        return Ok(String::new());
    }
    
    let total_segments = segments.len();
    let mut results = Vec::new();
    
    for (i, segment) in segments.into_iter().enumerate() {
        // 在每个段处理前检查是否需要取消
        if recognition_state.should_cancel() {
            return Err("转录已被用户取消".to_string());
        }
        
        let progress = ((i + 1) as f32 / total_segments as f32) * 100.0;
        let _ = window.emit("recognition_progress", RecognitionProgress {
            stage: "segment_processing".to_string(),
            progress,
            message: format!("处理段 {}/{} ({:.1}s)", i + 1, total_segments, segment.end_time - segment.start_time),
        });
        
        println!("处理段 {} ({:.1}s - {:.1}s)", i + 1, segment.start_time, segment.end_time);
        
        match recognize_segment_blocking(&segment.data, &language, &mode, &initial_prompt, whisper_state) {
            Ok(text) => {
                results.push((segment.start_time, text));
                println!("段 {} 完成: {} 字符", i + 1, results.last().unwrap().1.len());
            }
            Err(e) => {
                println!("段 {} 识别失败: {}", i + 1, e);
                results.push((segment.start_time, String::new()));
            }
        }
    }
    
    // 按时间顺序排序并合并
    results.sort_by(|a, b| a.0.partial_cmp(&b.0).unwrap());
    let combined_text = results.into_iter()
        .map(|(_, text)| text)
        .filter(|text| !text.is_empty())
        .collect::<Vec<_>>()
        .join(" ");
    
    println!("分段识别完成，总字符数: {}", combined_text.len());
    Ok(combined_text)
}

// 智能分段策略决策
fn should_use_segmentation(samples: &[f32], config: &SegmentationConfig) -> bool {
    let duration_sec = samples.len() as f32 / 16000.0;
    
    // 音频长度超过配置的最大段长度时才进行分段
    duration_sec > config.max_segment_duration_sec
}

// 阻塞式段识别
fn recognize_segment_blocking(
    audio_data: &[f32],
    language: &str,
    mode: &str,
    initial_prompt: &Option<String>,
    whisper_state: &WhisperContextState,
) -> Result<String, String> {
    let ctx = whisper_state.ctx.lock().unwrap();
    
    // 使用优化的识别参数
    let mut params = unsafe { 
        whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH) 
    };
    
    // 计算音频长度用于参数调整
    let duration = audio_data.len() as f32 / 16000.0;
    
    // 基础参数设置
    params.temperature = 0.0;
    params.suppress_blank = true;
    params.token_timestamps = true;
    params.max_len = 1;
    
    // 根据处理模式调整参数（为并行处理优化，使用较低线程数）
    match mode {
        "standard" => {
            // 标准质量模式 - 平衡速度和准确度
            params.n_threads = 1; // 并行处理时使用更少线程
            params.beam_search.beam_size = 2;
            params.greedy.best_of = 2;
        },
        "high_precision" => {
            // 高精度模式 - 优先准确度
            params.n_threads = 2; // 高精度模式可以用稍多线程
            params.beam_search.beam_size = 4;
            params.greedy.best_of = 4;
            params.temperature = 0.05; // 轻微增加随机性
        },
        _ => {
            // 默认设置 (与标准质量相同)
            params.n_threads = 1;
            params.beam_search.beam_size = 2;
            params.greedy.best_of = 2;
        }
    }
    
    // 语言设置
    let lang_cstring = match language {
        "zh" => Some(std::ffi::CString::new("zh").unwrap()),
        "en" => Some(std::ffi::CString::new("en").unwrap()),
        _ => None,
    };
    
    if let Some(ref lang_str) = lang_cstring {
        params.language = lang_str.as_ptr();
    } else {
        params.language = std::ptr::null();
    }

    // 设置初始提示词
    let prompt_cstring = if let Some(ref prompt) = initial_prompt {
        if !prompt.trim().is_empty() {
            Some(std::ffi::CString::new(prompt.trim()).unwrap())
        } else {
            None
        }
    } else {
        None
    };

    if let Some(ref prompt_str) = prompt_cstring {
        params.initial_prompt = prompt_str.as_ptr();
    }
    
    // 执行识别
    let mut audio_copy = audio_data.to_vec();
    let result = unsafe {
        whisper_full(
            *ctx,
            params,
            audio_copy.as_mut_ptr(),
            audio_copy.len() as i32,
        )
    };
    
    if result != 0 {
        return Err("Whisper段识别失败".to_string());
    }
    
    // 提取文本
    let num_segments = unsafe { whisper_full_n_segments(*ctx) };
    let mut text = String::new();
    
    for i in 0..num_segments {
        let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
        if !segment_ptr.is_null() {
            let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
            text.push_str(c_str.to_str().unwrap_or(""));
        }
    }
    
    // 应用文本后处理
    let processed_text = post_process_text(&text, language);
    Ok(processed_text)
}



// 优化的并行音频预处理
fn parallel_audio_preprocessing(
    samples: Vec<f32>,
    config: &AudioProcessingConfig,
) -> Result<Vec<f32>, String> {
    let chunk_size = samples.len() / rayon::current_num_threads().max(1);
    
    if chunk_size < 16000 { // 少于1秒音频不值得并行处理
        return advanced_audio_preprocessing_pipeline(samples, config);
    }
    
    println!("使用并行音频预处理，块大小: {} 样本", chunk_size);
    
    // 并行处理音频块
    let processed_chunks: Result<Vec<Vec<f32>>, String> = samples
        .par_chunks(chunk_size)
        .enumerate()
        .map(|(i, chunk)| {
            println!("处理音频块 {}", i + 1);
            let mut chunk_data = chunk.to_vec();
            
            // 对每个块应用部分预处理
            if config.enable_preemphasis {
                apply_preemphasis(&mut chunk_data, config.preemphasis_coeff);
            }
            
            if config.enable_normalization {
                normalize_audio(&mut chunk_data);
            }
            
            if config.enable_noise_reduction {
                adaptive_noise_reduction(&mut chunk_data, config.noise_threshold_multiplier);
            }
            
            Ok(chunk_data)
        })
        .collect();
    
    let mut processed_samples: Vec<f32> = processed_chunks?
        .into_iter()
        .flatten()
        .collect();
    
    // 全局后处理步骤（不能并行）
    if config.enable_dynamic_range_compression {
        apply_dynamic_range_compression(&mut processed_samples);
    }
    
    if config.enable_spectral_enhancement {
        if let Err(e) = spectral_enhancement(&mut processed_samples) {
            println!("并行频谱增强失败: {}", e);
        }
    }
    
    final_quality_optimization(&mut processed_samples);
    
    println!("并行音频预处理完成");
    Ok(processed_samples)
}

// =============================================================================
// 集成的高级识别流程
// =============================================================================

// 高级音频识别流程，集成所有优化功能
fn advanced_recognition_pipeline(
    audio_data: Vec<f32>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    window: &WebviewWindow,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    println!("开始高级音频识别流程...");
    
    // 检查是否需要取消
    if recognition_state.should_cancel() {
        return Err("转录已被用户取消".to_string());
    }
    
    let total_duration = audio_data.len() as f32 / 16000.0;
    println!("音频总长度: {:.1}秒", total_duration);
    
    // 步骤1: 决定是否需要分段处理
    let segmentation_config = SegmentationConfig::default();
    
    if should_use_segmentation(&audio_data, &segmentation_config) {
        println!("音频较长({:.1}s)，使用智能分段处理", total_duration);
        
        // 智能分段
        let segments = match intelligent_audio_segmentation(&audio_data, &segmentation_config) {
            Ok(segs) => segs,
            Err(e) => {
                println!("智能分段失败: {}, 使用整体识别", e);
                return recognize_whole_audio(audio_data, language, mode.clone(), initial_prompt, whisper_state, recognition_state);
            }
        };
        
        // 分段识别
        segment_based_recognition(segments, language, mode.clone(), initial_prompt.clone(), whisper_state, window, recognition_state)
    } else {
        println!("音频较短({:.1}s)，使用整体识别", total_duration);
        recognize_whole_audio(audio_data, language, mode, initial_prompt, whisper_state, recognition_state)
    }
}

// 整体音频识别（优化版本）
fn recognize_whole_audio(
    mut audio_data: Vec<f32>,
    language: String,
    mode: String,
    initial_prompt: Option<String>,
    whisper_state: &WhisperContextState,
    recognition_state: &RecognitionState,
) -> Result<String, String> {
    // 检查是否需要取消
    if recognition_state.should_cancel() {
        return Err("转录已被用户取消".to_string());
    }
    
    let ctx = whisper_state.ctx.lock().unwrap();
    
    // 使用最优参数
    let mut params = unsafe { 
        whisper_full_default_params(whisper_sampling_strategy_WHISPER_SAMPLING_BEAM_SEARCH) 
    };
    
    // 根据音频长度和处理模式调整参数
    let duration = audio_data.len() as f32 / 16000.0;
    
    // 基础参数设置
    params.greedy.best_of = 5;
    params.temperature = 0.0;
    params.suppress_blank = true;
    params.token_timestamps = true;
    params.max_len = 1;
    
    // 根据处理模式和音频长度调整参数
    match mode.as_str() {
        "standard" => {
            // 标准质量模式 - 平衡速度和准确度
            if duration > 120.0 {
                params.n_threads = 4;
                params.beam_search.beam_size = 2;
            } else {
                params.n_threads = 6;
                params.beam_search.beam_size = 3;
            }
            params.greedy.best_of = 3;
            params.temperature = 0.0;
        },
        "high_precision" => {
            // 高精度模式 - 优先准确度
            if duration > 120.0 {
                params.n_threads = 8;
                params.beam_search.beam_size = 5;
            } else {
                params.n_threads = 8;
                params.beam_search.beam_size = 8;
            }
            params.greedy.best_of = 8;
            params.temperature = 0.1; // 轻微增加随机性以获得更好结果
        },
        _ => {
            // 默认设置 (与标准质量相同)
            if duration > 120.0 {
                params.n_threads = 4;
                params.beam_search.beam_size = 2;
            } else {
                params.n_threads = 6;
                params.beam_search.beam_size = 3;
            }
        }
    }
    
    // 语言设置
    let lang_cstring = match language.as_str() {
        "zh" => Some(std::ffi::CString::new("zh").unwrap()),
        "en" => Some(std::ffi::CString::new("en").unwrap()),
        _ => None,
    };
    
    if let Some(ref lang_str) = lang_cstring {
        params.language = lang_str.as_ptr();
    } else {
        params.language = std::ptr::null();
    }

    // 设置初始提示词
    let prompt_cstring = if let Some(ref prompt) = initial_prompt {
        if !prompt.trim().is_empty() {
            Some(std::ffi::CString::new(prompt.trim()).unwrap())
        } else {
            None
        }
    } else {
        None
    };

    if let Some(ref prompt_str) = prompt_cstring {
        params.initial_prompt = prompt_str.as_ptr();
    }
    
    println!("使用优化参数: beam_size={}, threads={}, duration={:.1}s", 
             params.beam_search.beam_size, params.n_threads, duration);
    
    // 执行识别
    let result = unsafe {
        whisper_full(
            *ctx,
            params,
            audio_data.as_mut_ptr(),
            audio_data.len() as i32,
        )
    };
    
    if result != 0 {
        return Err("Whisper整体识别失败".to_string());
    }
    
    // 提取文本
    let num_segments = unsafe { whisper_full_n_segments(*ctx) };
    let mut full_text = String::new();
    
    for i in 0..num_segments {
        let segment_ptr = unsafe { whisper_full_get_segment_text(*ctx, i) };
        if !segment_ptr.is_null() {
            let c_str = unsafe { CStr::from_ptr(segment_ptr as *const c_char) };
            full_text.push_str(c_str.to_str().unwrap_or(""));
        }
    }
    
    // 获取带时间戳的段信息用于说话人识别
    let segments = extract_timestamped_segments(*ctx);
    
    // 文本后处理
    let processed_text = post_process_text(&full_text, &language);
    
    // 如果有多个段，尝试进行说话人识别和角色分配
    if segments.len() > 1 {
        match perform_speaker_diarization(&audio_data, &segments) {
            Ok(dialogue_text) => Ok(dialogue_text),
            Err(e) => {
                println!("说话人识别失败，返回原始文本: {}", e);
                Ok(processed_text)
            }
        }
    } else {
        Ok(processed_text)
    }
}

// =============================================================================
// 说话人识别和角色分配系统
// =============================================================================

#[derive(Debug, Clone)]
pub struct TimestampedSegment {
    pub start_time: f64,  // 开始时间(秒)
    pub end_time: f64,    // 结束时间(秒)
    pub text: String,     // 识别的文本
    pub audio_start: usize, // 音频开始位置(样本数)
    pub audio_end: usize,   // 音频结束位置(样本数)
}

#[derive(Debug, Clone)]
pub struct VoiceCharacteristics {
    pub fundamental_freq: f32,    // 基频 (Hz)
    pub formant_frequencies: Vec<f32>, // 共振峰频率
    pub spectral_centroid: f32,   // 频谱质心
    pub spectral_bandwidth: f32,  // 频谱带宽
    pub zero_crossing_rate: f32,  // 过零率
    pub mfcc_features: Vec<f32>,  // MFCC特征
    pub energy: f32,              // 平均能量
}

#[derive(Debug, Clone)]
pub struct Speaker {
    pub id: usize,
    pub characteristics: VoiceCharacteristics,
    pub segments: Vec<TimestampedSegment>,
    pub assigned_role: String,  // 分配的角色名称
}

// 从Whisper上下文提取带时间戳的段信息
fn extract_timestamped_segments(ctx: *mut whisper_context) -> Vec<TimestampedSegment> {
    let mut segments = Vec::new();
    
    unsafe {
        let num_segments = whisper_full_n_segments(ctx);
        
        for i in 0..num_segments {
            let start_time = whisper_full_get_segment_t0(ctx, i) as f64 / 100.0; // 转换为秒
            let end_time = whisper_full_get_segment_t1(ctx, i) as f64 / 100.0;
            
            let segment_ptr = whisper_full_get_segment_text(ctx, i);
            let text = if !segment_ptr.is_null() {
                let c_str = CStr::from_ptr(segment_ptr as *const c_char);
                c_str.to_str().unwrap_or("").to_string()
            } else {
                String::new()
            };
            
            // 计算音频位置 (16kHz采样率)
            let audio_start = (start_time * 16000.0) as usize;
            let audio_end = (end_time * 16000.0) as usize;
            
            segments.push(TimestampedSegment {
                start_time,
                end_time,
                text,
                audio_start,
                audio_end,
            });
        }
    }
    
    segments
}

// 执行说话人识别和角色分配
fn perform_speaker_diarization(
    audio_data: &[f32],
    segments: &[TimestampedSegment],
) -> Result<String, String> {
    println!("开始说话人识别，共 {} 个语音段", segments.len());
    
    // 1. 为每个段提取音色特征
    let mut segment_features = Vec::new();
    for segment in segments {
        let audio_segment = extract_audio_segment(audio_data, segment)?;
        let characteristics = extract_voice_characteristics(&audio_segment)?;
        segment_features.push((segment.clone(), characteristics));
    }
    
    // 2. 聚类分析，识别不同说话人
    let speakers = cluster_speakers(segment_features)?;
    
    // 3. 分配角色名称
    let speakers_with_roles = assign_speaker_roles(speakers);
    
    // 4. 格式化对话输出
    let dialogue_text = format_dialogue_output(&speakers_with_roles);
    
    println!("说话人识别完成，识别出 {} 个说话人", speakers_with_roles.len());
    Ok(dialogue_text)
}

// 提取音频段
fn extract_audio_segment(
    audio_data: &[f32],
    segment: &TimestampedSegment,
) -> Result<Vec<f32>, String> {
    let start = segment.audio_start.min(audio_data.len());
    let end = segment.audio_end.min(audio_data.len());
    
    if start >= end {
        return Err("无效的音频段时间范围".to_string());
    }
    
    Ok(audio_data[start..end].to_vec())
}

// 提取音色特征
fn extract_voice_characteristics(audio_segment: &[f32]) -> Result<VoiceCharacteristics, String> {
    if audio_segment.len() < 1600 { // 至少100ms
        return Err("音频段太短，无法提取特征".to_string());
    }
    
    // 1. 基频提取 (简化的自相关方法)
    let fundamental_freq = estimate_fundamental_frequency(audio_segment);
    
    // 2. 频谱质心和带宽
    let (spectral_centroid, spectral_bandwidth) = calculate_spectral_features(audio_segment);
    
    // 3. 过零率
    let zero_crossing_rate = calculate_zero_crossing_rate(audio_segment);
    
    // 4. 平均能量
    let energy = audio_segment.iter().map(|&x| x * x).sum::<f32>() / audio_segment.len() as f32;
    
    // 5. MFCC特征 (简化版本)
    let mfcc_features = extract_mfcc_features(audio_segment)?;
    
    // 6. 共振峰频率 (简化估计)
    let formant_frequencies = estimate_formant_frequencies(audio_segment);
    
    Ok(VoiceCharacteristics {
        fundamental_freq,
        formant_frequencies,
        spectral_centroid,
        spectral_bandwidth,
        zero_crossing_rate,
        mfcc_features,
        energy,
    })
}

// 简化的辅助函数，集成到主文件中
fn estimate_fundamental_frequency(audio: &[f32]) -> f32 {
    let sample_rate = 16000.0;
    let min_period = (sample_rate / 500.0) as usize;
    let max_period = (sample_rate / 50.0) as usize;
    
    let mut max_correlation = 0.0;
    let mut best_period = min_period;
    
    for period in min_period..=max_period.min(audio.len() / 2) {
        let mut correlation = 0.0;
        let mut count = 0;
        
        for i in 0..(audio.len() - period) {
            correlation += audio[i] * audio[i + period];
            count += 1;
        }
        
        if count > 0 {
            correlation /= count as f32;
            if correlation > max_correlation {
                max_correlation = correlation;
                best_period = period;
            }
        }
    }
    
    sample_rate / best_period as f32
}

fn calculate_spectral_features(audio: &[f32]) -> (f32, f32) {
    let mut magnitudes = Vec::new();
    let chunk_size = 512;
    
    for chunk in audio.chunks(chunk_size) {
        if chunk.len() == chunk_size {
            let magnitude_sum: f32 = chunk.iter().map(|&x| x.abs()).sum();
            magnitudes.push(magnitude_sum / chunk_size as f32);
        }
    }
    
    if magnitudes.is_empty() {
        return (0.0, 0.0);
    }
    
    let mut weighted_sum = 0.0;
    let mut total_magnitude = 0.0;
    
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        let frequency = i as f32 * 16000.0 / magnitudes.len() as f32;
        weighted_sum += frequency * magnitude;
        total_magnitude += magnitude;
    }
    
    let centroid = if total_magnitude > 0.0 {
        weighted_sum / total_magnitude
    } else { 0.0 };
    
    let mut variance = 0.0;
    for (i, &magnitude) in magnitudes.iter().enumerate() {
        let frequency = i as f32 * 16000.0 / magnitudes.len() as f32;
        variance += (frequency - centroid).powi(2) * magnitude;
    }
    
    let bandwidth = if total_magnitude > 0.0 {
        (variance / total_magnitude).sqrt()
    } else { 0.0 };
    
    (centroid, bandwidth)
}

fn calculate_zero_crossing_rate(audio: &[f32]) -> f32 {
    if audio.len() < 2 { return 0.0; }
    
    let zero_crossings = audio.windows(2)
        .filter(|window| (window[0] >= 0.0) != (window[1] >= 0.0))
        .count();
    
    zero_crossings as f32 / (audio.len() - 1) as f32
}

fn extract_mfcc_features(_audio: &[f32]) -> Result<Vec<f32>, String> {
    // 简化版本，返回固定长度的特征向量
    Ok(vec![0.0; 13])
}

fn estimate_formant_frequencies(_audio: &[f32]) -> Vec<f32> {
    // 简化版本，返回默认共振峰频率
    vec![800.0, 1200.0, 2500.0]
}

// 聚类分析，识别不同说话人
fn cluster_speakers(
    segment_features: Vec<(TimestampedSegment, VoiceCharacteristics)>
) -> Result<Vec<Speaker>, String> {
    println!("开始说话人聚类分析...");
    
    if segment_features.is_empty() {
        return Ok(Vec::new());
    }
    
    // 使用简化的基频聚类
    let max_speakers = 4;
    let actual_speakers = estimate_speaker_count(&segment_features).min(max_speakers);
    
    println!("估计说话人数量: {}", actual_speakers);
    
    if actual_speakers == 1 {
        // 只有一个说话人
        let avg_characteristics = calculate_average_characteristics(
            &segment_features.iter().map(|(_, chars)| chars).collect::<Vec<_>>()
        );
        
        let segments = segment_features.into_iter().map(|(seg, _)| seg).collect();
        
        return Ok(vec![Speaker {
            id: 0,
            characteristics: avg_characteristics,
            segments,
            assigned_role: String::new(),
        }]);
    }
    
    // 简化的K-means聚类基于基频
    let mut speakers = Vec::new();
    let mut freq_sorted: Vec<(usize, f32)> = segment_features.iter()
        .enumerate()
        .map(|(i, (_, chars))| (i, chars.fundamental_freq))
        .filter(|(_, freq)| *freq > 50.0 && *freq < 500.0)
        .collect();
    
    freq_sorted.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
    
    // 将频率分组
    let chunk_size = freq_sorted.len() / actual_speakers;
    for speaker_id in 0..actual_speakers {
        let start = speaker_id * chunk_size;
        let end = if speaker_id == actual_speakers - 1 {
            freq_sorted.len()
        } else {
            (speaker_id + 1) * chunk_size
        };
        
        if start < freq_sorted.len() {
            let indices: Vec<usize> = freq_sorted[start..end.min(freq_sorted.len())]
                .iter().map(|(i, _)| *i).collect();
            
            let characteristics_list: Vec<&VoiceCharacteristics> = indices.iter()
                .map(|&i| &segment_features[i].1).collect();
            
            let avg_characteristics = calculate_average_characteristics(&characteristics_list);
            let segments: Vec<TimestampedSegment> = indices.iter()
                .map(|&i| segment_features[i].0.clone()).collect();
            
            speakers.push(Speaker {
                id: speaker_id,
                characteristics: avg_characteristics,
                segments,
                assigned_role: String::new(),
            });
        }
    }
    
    // 按出现时间排序
    speakers.sort_by(|a, b| {
        let a_first_time = a.segments.iter().map(|s| s.start_time).fold(f64::INFINITY, f64::min);
        let b_first_time = b.segments.iter().map(|s| s.start_time).fold(f64::INFINITY, f64::min);
        a_first_time.partial_cmp(&b_first_time).unwrap()
    });
    
    Ok(speakers)
}

fn estimate_speaker_count(segment_features: &[(TimestampedSegment, VoiceCharacteristics)]) -> usize {
    if segment_features.len() <= 2 {
        return segment_features.len();
    }
    
    let mut fundamental_freqs: Vec<f32> = segment_features.iter()
        .map(|(_, chars)| chars.fundamental_freq)
        .filter(|&f| f > 50.0 && f < 500.0)
        .collect();
    
    if fundamental_freqs.is_empty() {
        return 1;
    }
    
    fundamental_freqs.sort_by(|a, b| a.partial_cmp(b).unwrap());
    
    // 简单的间隔分析
    let mut gaps = Vec::new();
    for i in 1..fundamental_freqs.len() {
        gaps.push(fundamental_freqs[i] - fundamental_freqs[i-1]);
    }
    
    if gaps.is_empty() {
        return 1;
    }
    
    gaps.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let median_gap = gaps[gaps.len() / 2];
    
    let significant_gaps = gaps.iter()
        .filter(|&&gap| gap > median_gap * 2.0)
        .count();
    
    (significant_gaps + 1).min(4).max(1)
}

fn calculate_average_characteristics(
    characteristics_list: &[&VoiceCharacteristics]
) -> VoiceCharacteristics {
    if characteristics_list.is_empty() {
        return VoiceCharacteristics {
            fundamental_freq: 0.0,
            formant_frequencies: vec![0.0, 0.0, 0.0],
            spectral_centroid: 0.0,
            spectral_bandwidth: 0.0,
            zero_crossing_rate: 0.0,
            mfcc_features: vec![0.0; 13],
            energy: 0.0,
        };
    }
    
    let count = characteristics_list.len() as f32;
    let mut avg = VoiceCharacteristics {
        fundamental_freq: 0.0,
        formant_frequencies: vec![0.0; 3],
        spectral_centroid: 0.0,
        spectral_bandwidth: 0.0,
        zero_crossing_rate: 0.0,
        mfcc_features: vec![0.0; 13],
        energy: 0.0,
    };
    
    for chars in characteristics_list {
        avg.fundamental_freq += chars.fundamental_freq;
        avg.spectral_centroid += chars.spectral_centroid;
        avg.spectral_bandwidth += chars.spectral_bandwidth;
        avg.zero_crossing_rate += chars.zero_crossing_rate;
        avg.energy += chars.energy;
        
        for i in 0..chars.formant_frequencies.len().min(3) {
            avg.formant_frequencies[i] += chars.formant_frequencies[i];
        }
        
        for i in 0..chars.mfcc_features.len().min(13) {
            avg.mfcc_features[i] += chars.mfcc_features[i];
        }
    }
    
    avg.fundamental_freq /= count;
    avg.spectral_centroid /= count;
    avg.spectral_bandwidth /= count;
    avg.zero_crossing_rate /= count;
    avg.energy /= count;
    
    for i in 0..3 {
        avg.formant_frequencies[i] /= count;
    }
    
    for i in 0..13 {
        avg.mfcc_features[i] /= count;
    }
    
    avg
}

fn assign_speaker_roles(mut speakers: Vec<Speaker>) -> Vec<Speaker> {
    let role_names = ["说话人A", "说话人B", "说话人C", "说话人D"];
    
    for (i, speaker) in speakers.iter_mut().enumerate() {
        if i < role_names.len() {
            speaker.assigned_role = role_names[i].to_string();
        } else {
            speaker.assigned_role = format!("说话人{}", i + 1);
        }
    }
    
    println!("角色分配完成:");
    for speaker in &speakers {
        println!("  {} - 基频: {:.1}Hz, 段数: {}", 
                speaker.assigned_role, 
                speaker.characteristics.fundamental_freq,
                speaker.segments.len());
    }
    
    speakers
}

fn format_dialogue_output(speakers: &[Speaker]) -> String {
    println!("开始格式化对话输出...");
    
    // 收集所有段并按时间排序
    let mut all_segments = Vec::new();
    for speaker in speakers {
        for segment in &speaker.segments {
            all_segments.push((segment, &speaker.assigned_role));
        }
    }
    
    all_segments.sort_by(|a, b| a.0.start_time.partial_cmp(&b.0.start_time).unwrap());
    
    // 格式化输出
    let mut formatted_text = String::new();
    let mut current_speaker = "";
    
    for (segment, role) in all_segments {
        let text = segment.text.trim();
        if text.is_empty() {
            continue;
        }
        
        if current_speaker != *role {
            if !formatted_text.is_empty() {
                formatted_text.push('\n');
            }
            formatted_text.push_str(&format!("{}：", role));
            current_speaker = role;
        } else {
            if !formatted_text.ends_with('：') {
                formatted_text.push(' ');
            }
        }
        
        formatted_text.push_str(text);
    }
    
    println!("对话格式化完成，总长度: {} 字符", formatted_text.len());
    formatted_text
}
