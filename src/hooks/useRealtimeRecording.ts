import { useState, useEffect, useCallback, useRef } from 'react';
import { invoke } from '@tauri-apps/api/core';
import { listen, UnlistenFn } from '@tauri-apps/api/event';

export interface RealtimeConfig {
  language: 'zh' | 'en' | 'auto';
  mode: 'streaming' | 'buffered' | 'hybrid';
  speakerDiarization: boolean;
  noiseReduction: boolean;
  autoSave: boolean;
  saveInterval: number; // minutes
}

export interface RecognitionResult {
  text: string;
  confidence: number;
  isTemporary: boolean;
  speaker?: string;
  timestamp: number;
}

export interface AudioLevelUpdate {
  level: number;
  timestamp: number;
}

export interface RecordingStats {
  duration: number; // seconds
  segmentsCount: number;
  speakerCount: number;
  averageConfidence: number;
}

export interface TranscriptSegment {
  id: string;
  text: string;
  speaker?: string;
  timestamp: number;
  confidence: number;
  isTemporary: boolean;
}

export type RecordingStatus = 'idle' | 'recording' | 'paused' | 'processing';

export interface RecordingState {
  status: RecordingStatus;
  duration: number;
  segments: TranscriptSegment[];
  currentText: string;
  confidence: number;
  speakerCount: number;
  audioFilePath?: string;
}

export interface UseRealtimeRecordingReturn {
  // çŠ¶æ€
  recordingState: RecordingState;
  audioLevel: number;
  isSupported: boolean;
  error: string | null;
  
  // æ“ä½œ
  startRecording: (config: RealtimeConfig) => Promise<void>;
  pauseRecording: () => Promise<void>;
  resumeRecording: () => Promise<void>;
  stopRecording: () => Promise<void>;
  clearTranscript: () => void;
  
  // é…ç½®
  updateConfig: (config: Partial<RealtimeConfig>) => void;
  config: RealtimeConfig;
}

const DEFAULT_CONFIG: RealtimeConfig = {
  language: 'zh',
  mode: 'hybrid',
  speakerDiarization: true,
  noiseReduction: true,
  autoSave: true,
  saveInterval: 5,
};

export const useRealtimeRecording = (): UseRealtimeRecordingReturn => {
  const [config, setConfig] = useState<RealtimeConfig>(DEFAULT_CONFIG);
  const [recordingState, setRecordingState] = useState<RecordingState>({
    status: 'idle',
    duration: 0,
    segments: [],
    currentText: '',
    confidence: 0.95,
    speakerCount: 1,
  });
  const [audioLevel, setAudioLevel] = useState(0);
  const [isSupported, setIsSupported] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // ç”¨äºç®¡ç†äº‹ä»¶ç›‘å¬å™¨çš„å¼•ç”¨
  const unlistenersRef = useRef<UnlistenFn[]>([]);
  const durationTimerRef = useRef<number>();
  const segmentIdCounterRef = useRef(0);

  // æ¸…ç†å‡½æ•°
  const cleanup = useCallback(() => {
    // æ¸…ç†äº‹ä»¶ç›‘å¬å™¨
    unlistenersRef.current.forEach(unlisten => unlisten());
    unlistenersRef.current = [];
    
    // æ¸…ç†è®¡æ—¶å™¨
    if (durationTimerRef.current) {
      clearInterval(durationTimerRef.current);
      durationTimerRef.current = undefined;
    }
  }, []);

  // è®¾ç½®äº‹ä»¶ç›‘å¬å™¨
  const setupEventListeners = useCallback(async () => {
    try {
      // éŸ³é¢‘çº§åˆ«æ›´æ–°ç›‘å¬å™¨
      const audioLevelUnlisten = await listen<AudioLevelUpdate>('audio_level_update', (event) => {
        setAudioLevel(event.payload.level);
      });
      unlistenersRef.current.push(audioLevelUnlisten);

      // è¯†åˆ«ç»“æœç›‘å¬å™¨
      const recognitionUnlisten = await listen<RecognitionResult>('recognition_result', (event) => {
        const result = event.payload;
        const segmentId = `segment_${segmentIdCounterRef.current++}`;
        
        const newSegment: TranscriptSegment = {
          id: segmentId,
          text: result.text,
          speaker: result.speaker,
          timestamp: result.timestamp,
          confidence: result.confidence,
          isTemporary: result.isTemporary,
        };

        setRecordingState(prev => {
          if (result.isTemporary) {
            // ä¸´æ—¶ç»“æœ - æ›´æ–°å½“å‰æ–‡æœ¬
            return {
              ...prev,
              currentText: result.text,
              confidence: result.confidence,
            };
          } else {
            // æœ€ç»ˆç»“æœ - æ·»åŠ åˆ°æ®µè½åˆ—è¡¨
            const updatedSegments = [...prev.segments, newSegment];
            const avgConfidence = updatedSegments.reduce((sum, seg) => sum + seg.confidence, 0) / updatedSegments.length;
            
            // æ¸…é™¤å½“å‰ä¸´æ—¶æ–‡æœ¬
            return {
              ...prev,
              segments: updatedSegments,
              currentText: '',
              confidence: avgConfidence,
            };
          }
        });
      });
      unlistenersRef.current.push(recognitionUnlisten);

      // å½•éŸ³ç»Ÿè®¡ç›‘å¬å™¨
      const statsUnlisten = await listen<RecordingStats>('recording_stats', (event) => {
        const stats = event.payload;
        setRecordingState(prev => ({
          ...prev,
          speakerCount: stats.speakerCount,
          confidence: stats.averageConfidence,
        }));
      });
      unlistenersRef.current.push(statsUnlisten);

      // å½•éŸ³å®Œæˆç›‘å¬å™¨
      const completedUnlisten = await listen('recording_completed', () => {
        console.log('ğŸ“ å½•éŸ³å®Œæˆäº‹ä»¶æ¥æ”¶åˆ°');
        setRecordingState(prev => ({
          ...prev,
          status: 'idle',
        }));
        setAudioLevel(0);
        cleanup();
      });
      unlistenersRef.current.push(completedUnlisten);

      // å½•éŸ³åœæ­¢ç›‘å¬å™¨
      const stoppedUnlisten = await listen('recording_stopped', () => {
        console.log('ğŸ›‘ å½•éŸ³åœæ­¢äº‹ä»¶æ¥æ”¶åˆ°');
        setRecordingState(prev => ({
          ...prev,
          status: 'idle',
        }));
        setAudioLevel(0);
      });
      unlistenersRef.current.push(stoppedUnlisten);

      // å½•éŸ³æ–‡ä»¶ä¿å­˜ç›‘å¬å™¨
      const fileSavedUnlisten = await listen<string>('recording_file_saved', (event) => {
        console.log('ğŸ’¾ å½•éŸ³æ–‡ä»¶å·²ä¿å­˜:', event.payload);
        // å°†æ–‡ä»¶è·¯å¾„å­˜å‚¨åˆ°çŠ¶æ€ä¸­ï¼Œä¾›å¤–éƒ¨ç»„ä»¶ä½¿ç”¨
        setRecordingState(prev => ({
          ...prev,
          audioFilePath: event.payload,
        }));
      });
      unlistenersRef.current.push(fileSavedUnlisten);

      // å½•éŸ³é”™è¯¯ç›‘å¬å™¨
      const errorUnlisten = await listen<string>('recording_error', (event) => {
        const errorMessage = event.payload;
        console.error('å½•éŸ³é”™è¯¯è¯¦æƒ…:', errorMessage);
        console.error('å®Œæ•´é”™è¯¯äº‹ä»¶:', event);
        setError(`å½•éŸ³å¤±è´¥: ${errorMessage}`);
        setRecordingState(prev => ({
          ...prev,
          status: 'idle',
        }));
        cleanup();
      });
      unlistenersRef.current.push(errorUnlisten);

    } catch (error) {
      console.error('è®¾ç½®äº‹ä»¶ç›‘å¬å™¨å¤±è´¥:', error);
      setError('è®¾ç½®å®æ—¶è¯†åˆ«ç›‘å¬å™¨å¤±è´¥');
    }
  }, [cleanup]);

  // å¼€å§‹å½•éŸ³æ—¶é•¿è®¡æ—¶å™¨
  const startDurationTimer = useCallback(() => {
    if (durationTimerRef.current) {
      clearInterval(durationTimerRef.current);
    }
    
    const startTime = Date.now();
    durationTimerRef.current = setInterval(() => {
      const elapsed = Math.floor((Date.now() - startTime) / 1000);
      setRecordingState(prev => ({
        ...prev,
        duration: elapsed,
      }));
    }, 1000);
  }, []);

  // åœæ­¢è®¡æ—¶å™¨
  const stopDurationTimer = useCallback(() => {
    if (durationTimerRef.current) {
      clearInterval(durationTimerRef.current);
      durationTimerRef.current = undefined;
    }
  }, []);

  // å¼€å§‹å½•éŸ³
  const startRecording = useCallback(async (newConfig: RealtimeConfig) => {
    try {
      setError(null);
      setConfig(newConfig);
      
      // é‡ç½®çŠ¶æ€
      setRecordingState({
        status: 'recording',
        duration: 0,
        segments: [],
        currentText: '',
        confidence: 0.95,
        speakerCount: 1,
      });
      setAudioLevel(0);
      segmentIdCounterRef.current = 0;

      // è®¾ç½®äº‹ä»¶ç›‘å¬å™¨
      await setupEventListeners();

      // å¯åŠ¨åç«¯å½•éŸ³ - è½¬æ¢é…ç½®æ ¼å¼ä»¥åŒ¹é…åç«¯æœŸæœ›
      const backendConfig = {
        language: newConfig.language,
        mode: newConfig.mode,
        speaker_diarization: newConfig.speakerDiarization,
        noise_reduction: newConfig.noiseReduction,
        auto_save: newConfig.autoSave,
        save_interval: newConfig.saveInterval,
      };
      
      console.log('å‘é€ç»™åç«¯çš„é…ç½®:', backendConfig);
      await invoke('start_realtime_recording', { config: backendConfig });
      
      // å¼€å§‹è®¡æ—¶
      startDurationTimer();
      
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error);
      console.error('é”™è¯¯è¯¦æƒ…:', JSON.stringify(error, null, 2));
      const errorMessage = error instanceof Error ? error.message : String(error);
      setError(`å¯åŠ¨å½•éŸ³å¤±è´¥: ${errorMessage}`);
      setRecordingState(prev => ({ ...prev, status: 'idle' }));
      cleanup();
    }
  }, [setupEventListeners, startDurationTimer, cleanup]);

  // æš‚åœå½•éŸ³
  const pauseRecording = useCallback(async () => {
    try {
      await invoke('pause_realtime_recording');
      setRecordingState(prev => ({ ...prev, status: 'paused' }));
      stopDurationTimer();
    } catch (error) {
      console.error('æš‚åœå½•éŸ³å¤±è´¥:', error);
      setError(`æš‚åœå½•éŸ³å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [stopDurationTimer]);

  // æ¢å¤å½•éŸ³
  const resumeRecording = useCallback(async () => {
    try {
      await invoke('resume_realtime_recording');
      setRecordingState(prev => ({ ...prev, status: 'recording' }));
      startDurationTimer();
    } catch (error) {
      console.error('æ¢å¤å½•éŸ³å¤±è´¥:', error);
      setError(`æ¢å¤å½•éŸ³å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
    }
  }, [startDurationTimer]);

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(async () => {
    try {
      setRecordingState(prev => ({ ...prev, status: 'processing' }));
      await invoke('stop_realtime_recording');
      stopDurationTimer();
      // æ³¨æ„ï¼šçŠ¶æ€ä¼šåœ¨ recording_completed äº‹ä»¶ä¸­æ›´æ–°ä¸º idle
    } catch (error) {
      console.error('åœæ­¢å½•éŸ³å¤±è´¥:', error);
      setError(`åœæ­¢å½•éŸ³å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
      setRecordingState(prev => ({ ...prev, status: 'idle' }));
      cleanup();
    }
  }, [stopDurationTimer, cleanup]);

  // æ¸…é™¤è½¬å½•ç»“æœ
  const clearTranscript = useCallback(() => {
    setRecordingState(prev => ({
      ...prev,
      segments: [],
      currentText: '',
    }));
    segmentIdCounterRef.current = 0;
  }, []);

  // æ›´æ–°é…ç½®
  const updateConfig = useCallback((newConfig: Partial<RealtimeConfig>) => {
    setConfig(prev => ({ ...prev, ...newConfig }));
  }, []);

  // æ£€æŸ¥æ˜¯å¦æ”¯æŒå®æ—¶å½•éŸ³
  useEffect(() => {
    const checkSupport = async () => {
      try {
        // è¿™é‡Œå¯ä»¥æ·»åŠ æ£€æŸ¥éº¦å…‹é£æƒé™ç­‰é€»è¾‘
        setIsSupported(true);
      } catch (error) {
        console.error('æ£€æŸ¥å®æ—¶å½•éŸ³æ”¯æŒå¤±è´¥:', error);
        setIsSupported(false);
        setError('å½“å‰ç¯å¢ƒä¸æ”¯æŒå®æ—¶å½•éŸ³åŠŸèƒ½');
      }
    };

    checkSupport();
  }, []);

  // æ¸…ç†å‡½æ•°
  useEffect(() => {
    return () => {
      cleanup();
    };
  }, [cleanup]);

  // å½“çŠ¶æ€å˜ä¸ºéå½•éŸ³çŠ¶æ€æ—¶åœæ­¢éŸ³é¢‘çº§åˆ«æ›´æ–°
  useEffect(() => {
    if (recordingState.status !== 'recording') {
      const timer = setTimeout(() => {
        setAudioLevel(0);
      }, 1000);
      return () => clearTimeout(timer);
    }
  }, [recordingState.status]);

  return {
    // çŠ¶æ€
    recordingState,
    audioLevel,
    isSupported,
    error,
    
    // æ“ä½œ
    startRecording,
    pauseRecording,
    resumeRecording,
    stopRecording,
    clearTranscript,
    
    // é…ç½®
    updateConfig,
    config,
  };
};